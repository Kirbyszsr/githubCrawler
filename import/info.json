[
  {
    "github_url": "https://github.com/spacewalkproject/spacewalk",
    "github_info": {
      "name": "spacewalkproject/spacewalk",
      "language": "Java"
    },
    "github_branches": {
      "branch_datas": [
        {
          "branch_version": "608355_pwd_link",
          "branch_url": "https://github.com/spacewalkproject/spacewalk/tree/608355_pwd_link",
          "branch_download_url": "https://github.com/spacewalkproject/spacewalk/archive/608355_pwd_link.zip"
        },
        {
          "branch_version": "1226329",
          "branch_url": "https://github.com/spacewalkproject/spacewalk/tree/1226329",
          "branch_download_url": "https://github.com/spacewalkproject/spacewalk/archive/1226329.zip"
        },
        {
          "branch_version": "Hibernate-5",
          "branch_url": "https://github.com/spacewalkproject/spacewalk/tree/Hibernate-5",
          "branch_download_url": "https://github.com/spacewalkproject/spacewalk/archive/Hibernate-5.zip"
        },
        {
          "branch_version": "RELEASE-0.2",
          "branch_url": "https://github.com/spacewalkproject/spacewalk/tree/RELEASE-0.2",
          "branch_download_url": "https://github.com/spacewalkproject/spacewalk/archive/RELEASE-0.2.zip"
        },
        {
          "branch_version": "RELEASE-0.3",
          "branch_url": "https://github.com/spacewalkproject/spacewalk/tree/RELEASE-0.3",
          "branch_download_url": "https://github.com/spacewalkproject/spacewalk/archive/RELEASE-0.3.zip"
        },
        {
          "branch_version": "SPACEWALK-0.4",
          "branch_url": "https://github.com/spacewalkproject/spacewalk/tree/SPACEWALK-0.4",
          "branch_download_url": "https://github.com/spacewalkproject/spacewalk/archive/SPACEWALK-0.4.zip"
        },
        {
          "branch_version": "SPACEWALK-0.5",
          "branch_url": "https://github.com/spacewalkproject/spacewalk/tree/SPACEWALK-0.5",
          "branch_download_url": "https://github.com/spacewalkproject/spacewalk/archive/SPACEWALK-0.5.zip"
        },
        {
          "branch_version": "SPACEWALK-0.6",
          "branch_url": "https://github.com/spacewalkproject/spacewalk/tree/SPACEWALK-0.6",
          "branch_download_url": "https://github.com/spacewalkproject/spacewalk/archive/SPACEWALK-0.6.zip"
        },
        {
          "branch_version": "SPACEWALK-0.7",
          "branch_url": "https://github.com/spacewalkproject/spacewalk/tree/SPACEWALK-0.7",
          "branch_download_url": "https://github.com/spacewalkproject/spacewalk/archive/SPACEWALK-0.7.zip"
        },
        {
          "branch_version": "SPACEWALK-0.8",
          "branch_url": "https://github.com/spacewalkproject/spacewalk/tree/SPACEWALK-0.8",
          "branch_download_url": "https://github.com/spacewalkproject/spacewalk/archive/SPACEWALK-0.8.zip"
        },
        {
          "branch_version": "SPACEWALK-1.0",
          "branch_url": "https://github.com/spacewalkproject/spacewalk/tree/SPACEWALK-1.0",
          "branch_download_url": "https://github.com/spacewalkproject/spacewalk/archive/SPACEWALK-1.0.zip"
        },
        {
          "branch_version": "SPACEWALK-1.1",
          "branch_url": "https://github.com/spacewalkproject/spacewalk/tree/SPACEWALK-1.1",
          "branch_download_url": "https://github.com/spacewalkproject/spacewalk/archive/SPACEWALK-1.1.zip"
        },
        {
          "branch_version": "SPACEWALK-1.2",
          "branch_url": "https://github.com/spacewalkproject/spacewalk/tree/SPACEWALK-1.2",
          "branch_download_url": "https://github.com/spacewalkproject/spacewalk/archive/SPACEWALK-1.2.zip"
        },
        {
          "branch_version": "SPACEWALK-1.3",
          "branch_url": "https://github.com/spacewalkproject/spacewalk/tree/SPACEWALK-1.3",
          "branch_download_url": "https://github.com/spacewalkproject/spacewalk/archive/SPACEWALK-1.3.zip"
        },
        {
          "branch_version": "SPACEWALK-1.4",
          "branch_url": "https://github.com/spacewalkproject/spacewalk/tree/SPACEWALK-1.4",
          "branch_download_url": "https://github.com/spacewalkproject/spacewalk/archive/SPACEWALK-1.4.zip"
        },
        {
          "branch_version": "SPACEWALK-1.5",
          "branch_url": "https://github.com/spacewalkproject/spacewalk/tree/SPACEWALK-1.5",
          "branch_download_url": "https://github.com/spacewalkproject/spacewalk/archive/SPACEWALK-1.5.zip"
        },
        {
          "branch_version": "SPACEWALK-1.6",
          "branch_url": "https://github.com/spacewalkproject/spacewalk/tree/SPACEWALK-1.6",
          "branch_download_url": "https://github.com/spacewalkproject/spacewalk/archive/SPACEWALK-1.6.zip"
        },
        {
          "branch_version": "SPACEWALK-1.7",
          "branch_url": "https://github.com/spacewalkproject/spacewalk/tree/SPACEWALK-1.7",
          "branch_download_url": "https://github.com/spacewalkproject/spacewalk/archive/SPACEWALK-1.7.zip"
        },
        {
          "branch_version": "SPACEWALK-1.8",
          "branch_url": "https://github.com/spacewalkproject/spacewalk/tree/SPACEWALK-1.8",
          "branch_download_url": "https://github.com/spacewalkproject/spacewalk/archive/SPACEWALK-1.8.zip"
        },
        {
          "branch_version": "SPACEWALK-1.9",
          "branch_url": "https://github.com/spacewalkproject/spacewalk/tree/SPACEWALK-1.9",
          "branch_download_url": "https://github.com/spacewalkproject/spacewalk/archive/SPACEWALK-1.9.zip"
        },
        {
          "branch_version": "SPACEWALK-2.0",
          "branch_url": "https://github.com/spacewalkproject/spacewalk/tree/SPACEWALK-2.0",
          "branch_download_url": "https://github.com/spacewalkproject/spacewalk/archive/SPACEWALK-2.0.zip"
        },
        {
          "branch_version": "SPACEWALK-2.1",
          "branch_url": "https://github.com/spacewalkproject/spacewalk/tree/SPACEWALK-2.1",
          "branch_download_url": "https://github.com/spacewalkproject/spacewalk/archive/SPACEWALK-2.1.zip"
        },
        {
          "branch_version": "SPACEWALK-2.2",
          "branch_url": "https://github.com/spacewalkproject/spacewalk/tree/SPACEWALK-2.2",
          "branch_download_url": "https://github.com/spacewalkproject/spacewalk/archive/SPACEWALK-2.2.zip"
        },
        {
          "branch_version": "SPACEWALK-2.3",
          "branch_url": "https://github.com/spacewalkproject/spacewalk/tree/SPACEWALK-2.3",
          "branch_download_url": "https://github.com/spacewalkproject/spacewalk/archive/SPACEWALK-2.3.zip"
        },
        {
          "branch_version": "SPACEWALK-2.4",
          "branch_url": "https://github.com/spacewalkproject/spacewalk/tree/SPACEWALK-2.4",
          "branch_download_url": "https://github.com/spacewalkproject/spacewalk/archive/SPACEWALK-2.4.zip"
        },
        {
          "branch_version": "SPACEWALK-2.5",
          "branch_url": "https://github.com/spacewalkproject/spacewalk/tree/SPACEWALK-2.5",
          "branch_download_url": "https://github.com/spacewalkproject/spacewalk/archive/SPACEWALK-2.5.zip"
        },
        {
          "branch_version": "SPACEWALK-2.6",
          "branch_url": "https://github.com/spacewalkproject/spacewalk/tree/SPACEWALK-2.6",
          "branch_download_url": "https://github.com/spacewalkproject/spacewalk/archive/SPACEWALK-2.6.zip"
        },
        {
          "branch_version": "SPACEWALK-2.7",
          "branch_url": "https://github.com/spacewalkproject/spacewalk/tree/SPACEWALK-2.7",
          "branch_download_url": "https://github.com/spacewalkproject/spacewalk/archive/SPACEWALK-2.7.zip"
        },
        {
          "branch_version": "SPACEWALK-2.8",
          "branch_url": "https://github.com/spacewalkproject/spacewalk/tree/SPACEWALK-2.8",
          "branch_download_url": "https://github.com/spacewalkproject/spacewalk/archive/SPACEWALK-2.8.zip"
        },
        {
          "branch_version": "SPACEWALK-2.9",
          "branch_url": "https://github.com/spacewalkproject/spacewalk/tree/SPACEWALK-2.9",
          "branch_download_url": "https://github.com/spacewalkproject/spacewalk/archive/SPACEWALK-2.9.zip"
        }
      ]
    },
    "github_pull_requests": { "pull_datas": [] },
    "github_issues": { "issue_datas": [] }
  },
  {
    "github_url": "https://github.com/apereo/cas",
    "github_info": {
      "name": "apereo/cas",
      "language": "Java"
    },
    "github_branches": {
      "branch_datas": [
        {
          "branch_version": "3.6.x",
          "branch_url": "https://github.com/Jasig/cas/tree/3.6.x",
          "branch_download_url": "https://github.com/Jasig/cas/archive/3.6.x.zip"
        },
        {
          "branch_version": "4.0.x",
          "branch_url": "https://github.com/Jasig/cas/tree/4.0.x",
          "branch_download_url": "https://github.com/Jasig/cas/archive/4.0.x.zip"
        },
        {
          "branch_version": "4.1.x",
          "branch_url": "https://github.com/Jasig/cas/tree/4.1.x",
          "branch_download_url": "https://github.com/Jasig/cas/archive/4.1.x.zip"
        },
        {
          "branch_version": "4.2.x",
          "branch_url": "https://github.com/Jasig/cas/tree/4.2.x",
          "branch_download_url": "https://github.com/Jasig/cas/archive/4.2.x.zip"
        },
        {
          "branch_version": "5.0.x",
          "branch_url": "https://github.com/Jasig/cas/tree/5.0.x",
          "branch_download_url": "https://github.com/Jasig/cas/archive/5.0.x.zip"
        },
        {
          "branch_version": "5.1.x",
          "branch_url": "https://github.com/Jasig/cas/tree/5.1.x",
          "branch_download_url": "https://github.com/Jasig/cas/archive/5.1.x.zip"
        },
        {
          "branch_version": "5.2.x",
          "branch_url": "https://github.com/Jasig/cas/tree/5.2.x",
          "branch_download_url": "https://github.com/Jasig/cas/archive/5.2.x.zip"
        },
        {
          "branch_version": "5.3.x",
          "branch_url": "https://github.com/Jasig/cas/tree/5.3.x",
          "branch_download_url": "https://github.com/Jasig/cas/archive/5.3.x.zip"
        },
        {
          "branch_version": "6.0.x",
          "branch_url": "https://github.com/Jasig/cas/tree/6.0.x",
          "branch_download_url": "https://github.com/Jasig/cas/archive/6.0.x.zip"
        },
        {
          "branch_version": "6.1.x",
          "branch_url": "https://github.com/Jasig/cas/tree/6.1.x",
          "branch_download_url": "https://github.com/Jasig/cas/archive/6.1.x.zip"
        },
        {
          "branch_version": "6.2.x",
          "branch_url": "https://github.com/Jasig/cas/tree/6.2.x",
          "branch_download_url": "https://github.com/Jasig/cas/archive/6.2.x.zip"
        },
        {
          "branch_version": "gh-pages",
          "branch_url": "https://github.com/Jasig/cas/tree/gh-pages",
          "branch_download_url": "https://github.com/Jasig/cas/archive/gh-pages.zip"
        },
        {
          "branch_version": "heroku-bootadminserver",
          "branch_url": "https://github.com/Jasig/cas/tree/heroku-bootadminserver",
          "branch_download_url": "https://github.com/Jasig/cas/archive/heroku-bootadminserver.zip"
        },
        {
          "branch_version": "heroku-casconfigserver",
          "branch_url": "https://github.com/Jasig/cas/tree/heroku-casconfigserver",
          "branch_download_url": "https://github.com/Jasig/cas/archive/heroku-casconfigserver.zip"
        },
        {
          "branch_version": "heroku-casinitializr",
          "branch_url": "https://github.com/Jasig/cas/tree/heroku-casinitializr",
          "branch_download_url": "https://github.com/Jasig/cas/archive/heroku-casinitializr.zip"
        },
        {
          "branch_version": "heroku-caswebapp-oidccert",
          "branch_url": "https://github.com/Jasig/cas/tree/heroku-caswebapp-oidccert",
          "branch_download_url": "https://github.com/Jasig/cas/archive/heroku-caswebapp-oidccert.zip"
        },
        {
          "branch_version": "heroku-caswebapp",
          "branch_url": "https://github.com/Jasig/cas/tree/heroku-caswebapp",
          "branch_download_url": "https://github.com/Jasig/cas/archive/heroku-caswebapp.zip"
        },
        {
          "branch_version": "heroku-discoveryserver",
          "branch_url": "https://github.com/Jasig/cas/tree/heroku-discoveryserver",
          "branch_download_url": "https://github.com/Jasig/cas/archive/heroku-discoveryserver.zip"
        },
        {
          "branch_version": "heroku-githubbot",
          "branch_url": "https://github.com/Jasig/cas/tree/heroku-githubbot",
          "branch_download_url": "https://github.com/Jasig/cas/archive/heroku-githubbot.zip"
        },
        {
          "branch_version": "heroku-gradle-buildcache",
          "branch_url": "https://github.com/Jasig/cas/tree/heroku-gradle-buildcache",
          "branch_download_url": "https://github.com/Jasig/cas/archive/heroku-gradle-buildcache.zip"
        },
        {
          "branch_version": "heroku-mgmtwebapp",
          "branch_url": "https://github.com/Jasig/cas/tree/heroku-mgmtwebapp",
          "branch_download_url": "https://github.com/Jasig/cas/archive/heroku-mgmtwebapp.zip"
        },
        {
          "branch_version": "heroku-zipkinserver",
          "branch_url": "https://github.com/Jasig/cas/tree/heroku-zipkinserver",
          "branch_download_url": "https://github.com/Jasig/cas/archive/heroku-zipkinserver.zip"
        },
        {
          "branch_version": "master",
          "branch_url": "https://github.com/Jasig/cas/tree/master",
          "branch_download_url": "https://github.com/Jasig/cas/archive/master.zip"
        },
        {
          "branch_version": "stacktraces",
          "branch_url": "https://github.com/Jasig/cas/tree/stacktraces",
          "branch_download_url": "https://github.com/Jasig/cas/archive/stacktraces.zip"
        }
      ]
    },
    "github_pull_requests": {
      "pull_datas": [
        {
          "pull_number": 4947,
          "pull_title": "MFA:Gauth: Fix potential TOTP secret key leakage due to HTTP request logging",
          "pull_version": "apereo:6.2.x",
          "pull_version_url": "https://github.com/Jasig/cas/tree/6.2.x"
        },
        {
          "pull_number": 4942,
          "pull_title": "Support disable Pkce",
          "pull_version": "apereo:6.2.x",
          "pull_version_url": "https://github.com/Jasig/cas/tree/6.2.x"
        },
        {
          "pull_number": 4937,
          "pull_title": "GitServiceRepository:  settings branchesToClone fix",
          "pull_version": "apereo:6.2.x",
          "pull_version_url": "https://github.com/Jasig/cas/tree/6.2.x"
        },
        {
          "pull_number": 4928,
          "pull_title": "Log4j config update",
          "pull_version": "apereo:master",
          "pull_version_url": "https://github.com/Jasig/cas/tree/master"
        },
        {
          "pull_number": 4906,
          "pull_title": "WIP - DelegatedClientID being lost",
          "pull_version": "apereo:master",
          "pull_version_url": "https://github.com/Jasig/cas/tree/master"
        }
      ]
    },
    "github_issues": {
      "issue_datas": [
        {
          "issue_url": "https://github.com/Jasig/cas/issues/4947",
          "issue_title": "MFA:Gauth: Fix potential TOTP secret key leakage due to HTTP request logging",
          "issue_number": 4947,
          "issue_text": "# Details\r\nWhile deploying CAS for my [employer](https://www.skroutz.gr/), I skimmed through the load balancer's\r\nlogs and that's how I stumbled upon the issue described below. In short, the user's\r\nTOTP secret key is logged as it's sent as a GET parameter during the registration\r\nprocess. Most of the times every middleware that terminates TLS (such as reverse proxies,\r\nload balancers and so on) tend to also log GET parameters. CAS users could unknowingly\r\nwriting TOTP secret keys to disk, and probably ship those logs to other machines (ELK stacks and so on).\r\nAn adversary could potentially steal those just by having access to logs somewhere\r\nother than the CAS hosts themselves.\r\n \r\n## Brief description of changes applied\r\nAn HTTP endpoint [0] currently serves the purpose of grabbing the user's\r\nTOTP secret key and produce the required QR code that in turn gets\r\nscanned by the user's phone, as shown below:\r\nhttps://github.com/apereo/cas/blob/acdc6cfd9e919e7feb54ad1ad0cbea44db88733d/support/cas-server-support-otp-mfa-core/src/main/java/org/apereo/cas/otp/web/flow/rest/OneTimeTokenQRGeneratorController.java#L28-L33\r\n\r\nA security issue arises in the Google Authenticator registration\r\ntemplate (below) because the aforementioned HTTP endpoint is sourced in an\r\n`img` tag, passing the user's secret key as a GET parameter. This poses\r\nan immediate threat to the confidentiality of the key.\r\nGET parameters get logged in proxies (e.g. Load balancers, reverse\r\nproxies etc), hence CAS users might be unknowingly logging these.\r\nhttps://github.com/apereo/cas/blob/c839b62772078bf347c45e67bb2b51d0aa622ff0/support/cas-server-support-thymeleaf/src/main/resources/templates/casGoogleAuthenticatorRegistrationView.html#L108\r\n\r\nThis PR attempts to provide the registration template with the QR code image, base64 encode\r\nit and embed it using the `base64:image/png` scheme.\r\n\r\nNOTE: This is a first draft to discuss the matter, see if you're interested in me merging this one.\r\nNevertheless, this PR is working as-is, despite lacking the two points below.\r\nThere are certainly two things missing here:\r\n- Cleaning up the existing `/otp/qrgen` endpoint\r\n- Potentially add more tests\r\n\r\n##  Any possible limitations, side effects, etc\r\nI've verified that this endpoint is only used there, so the shouldn't be any side-effects.\r\n\r\n\r\n\r\n",
          "issue_comments": [
            {
              "comment_username": "CLAassistant",
              "comment_create_time": "2020-09-25T15:27:53Z",
              "comment_edit_time": "2020-09-28T12:24:13Z",
              "comment_text": "[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/apereo/cas?pullRequest=4947) <br/>All committers have signed the CLA."
            }
          ]
        },
        {
          "issue_url": "https://github.com/Jasig/cas/issues/4942",
          "issue_title": "Support disable Pkce",
          "issue_number": 4942,
          "issue_text": "Support disable Pkce",
          "issue_comments": [
            {
              "comment_username": "codecov[bot]",
              "comment_create_time": "2020-09-17T12:43:15Z",
              "comment_edit_time": "2020-09-17T12:45:58Z",
              "comment_text": "# [Codecov](https://codecov.io/gh/apereo/cas/pull/4942?src=pr&el=h1) Report\n> Merging [#4942](https://codecov.io/gh/apereo/cas/pull/4942?src=pr&el=desc) into [6.2.x](https://codecov.io/gh/apereo/cas/commit/bb27600f0129dc598d8b8b6b0dc7741ba197b577?el=desc) will **decrease** coverage by `63.54003%`.\n> The diff coverage is `0.00000%`.\n\n[![Impacted file tree graph](https://codecov.io/gh/apereo/cas/pull/4942/graphs/tree.svg?width=650&height=150&src=pr&token=T0wbvlaNTv)](https://codecov.io/gh/apereo/cas/pull/4942?src=pr&el=tree)\n\n```diff\n@@                  Coverage Diff                  @@\n##                 6.2.x      #4942          +/-   ##\n=====================================================\n- Coverage     65.46082%   1.92079%   -63.54003%     \n+ Complexity       11397        419       -10978     \n=====================================================\n  Files             2667       2667                  \n  Lines            56226      56227           +1     \n  Branches          4687       4687                  \n=====================================================\n- Hits             36806       1080       -35726     \n- Misses           16562      55032       +38470     \n+ Partials          2858        115        -2743     \n```\n\n| Flag | Coverage Δ | Complexity Δ | |\n|---|---|---|---|\n| #aws | `0.00000% <0.00000%> (ø)` | `0.00000 <0.00000> (ø)` | |\n| #cassandra | `?` | `?` | |\n| #cosmosdb | `0.00000% <0.00000%> (?)` | `0.00000 <0.00000> (?)` | |\n| #couchbase | `?` | `?` | |\n| #couchdb | `?` | `?` | |\n| #dynamodb | `?` | `?` | |\n| #ehcache | `?` | `?` | |\n| #filesystem | `?` | `?` | |\n| #groovy | `?` | `?` | |\n| #hazelcast | `?` | `?` | |\n| #ignite | `1.58643% <0.00000%> (-0.00180%)` | `342.00000 <0.00000> (-1.00000)` | |\n| #influxdb | `0.54956% <0.00000%> (+0.00534%)` | `123.00000 <0.00000> (+1.00000)` | |\n| #jdbc | `?` | `?` | |\n| #jms | `?` | `?` | |\n| #jmx | `?` | `?` | |\n| #ldap | `?` | `?` | |\n| #mail | `0.00711% <0.00000%> (ø)` | `1.00000 <0.00000> (ø)` | |\n| #mariadb | `?` | `?` | |\n| #memcached | `?` | `?` | |\n| #mfa | `?` | `?` | |\n| #mongo | `?` | `?` | |\n| #mysql | `?` | `?` | |\n| #oauth | `?` | `?` | |\n| #oidc | `?` | `?` | |\n| #oracle | `?` | `?` | |\n| #postgres | `?` | `?` | |\n| #radius | `?` | `?` | |\n| #redis | `?` | `?` | |\n| #rest | `?` | `?` | |\n| #saml | `?` | `?` | |\n| #shell | `?` | `?` | |\n| #simple | `?` | `?` | |\n| #uma | `?` | `?` | |\n| #webflow | `?` | `?` | |\n| #x509 | `?` | `?` | |\n| #zookeeper | `0.00711% <0.00000%> (ø)` | `1.00000 <0.00000> (ø)` | |\n\nFlags with carried forward coverage won't be shown. [Click here](https://docs.codecov.io/docs/carryforward-flags#carryforward-flags-in-the-pull-request-comment) to find out more.\n\n| [Impacted Files](https://codecov.io/gh/apereo/cas/pull/4942?src=pr&el=tree) | Coverage Δ | Complexity Δ | |\n|---|---|---|---|\n| [...port/pac4j/oidc/BasePac4jOidcClientProperties.java](https://codecov.io/gh/apereo/cas/pull/4942/diff?src=pr&el=tree#diff-YXBpL2Nhcy1zZXJ2ZXItY29yZS1hcGktY29uZmlndXJhdGlvbi1tb2RlbC9zcmMvbWFpbi9qYXZhL29yZy9hcGVyZW8vY2FzL2NvbmZpZ3VyYXRpb24vbW9kZWwvc3VwcG9ydC9wYWM0ai9vaWRjL0Jhc2VQYWM0ak9pZGNDbGllbnRQcm9wZXJ0aWVzLmphdmE=) | `0.00000% <ø> (ø)` | `0.00000 <0.00000> (ø)` | |\n| [.../authentication/DefaultDelegatedClientFactory.java](https://codecov.io/gh/apereo/cas/pull/4942/diff?src=pr&el=tree#diff-c3VwcG9ydC9jYXMtc2VydmVyLXN1cHBvcnQtcGFjNGotY29yZS1jbGllbnRzL3NyYy9tYWluL2phdmEvb3JnL2FwZXJlby9jYXMvc3VwcG9ydC9wYWM0ai9hdXRoZW50aWNhdGlvbi9EZWZhdWx0RGVsZWdhdGVkQ2xpZW50RmFjdG9yeS5qYXZh) | `0.00000% <0.00000%> (-74.30168%)` | `0.00000 <0.00000> (-50.00000)` | |\n| [.../src/main/java/org/apereo/cas/CasTomcatBanner.java](https://codecov.io/gh/apereo/cas/pull/4942/diff?src=pr&el=tree#diff-d2ViYXBwL2Nhcy1zZXJ2ZXItd2ViYXBwLXRvbWNhdC9zcmMvbWFpbi9qYXZhL29yZy9hcGVyZW8vY2FzL0Nhc1RvbWNhdEJhbm5lci5qYXZh) | `0.00000% <0.00000%> (-100.00000%)` | `0.00000% <0.00000%> (-2.00000%)` | |\n| [...in/java/org/apereo/cas/audit/AuditableContext.java](https://codecov.io/gh/apereo/cas/pull/4942/diff?src=pr&el=tree#diff-YXBpL2Nhcy1zZXJ2ZXItY29yZS1hcGktYXVkaXQvc3JjL21haW4vamF2YS9vcmcvYXBlcmVvL2Nhcy9hdWRpdC9BdWRpdGFibGVDb250ZXh0LmphdmE=) | `0.00000% <0.00000%> (-100.00000%)` | `0.00000% <0.00000%> (-10.00000%)` | |\n| [...in/java/org/apereo/cas/util/io/WatcherService.java](https://codecov.io/gh/apereo/cas/pull/4942/diff?src=pr&el=tree#diff-Y29yZS9jYXMtc2VydmVyLWNvcmUtdXRpbC1hcGkvc3JjL21haW4vamF2YS9vcmcvYXBlcmVvL2Nhcy91dGlsL2lvL1dhdGNoZXJTZXJ2aWNlLmphdmE=) | `0.00000% <0.00000%> (-100.00000%)` | `0.00000% <0.00000%> (-3.00000%)` | |\n| [.../java/org/apereo/cas/web/view/DynamicHtmlView.java](https://codecov.io/gh/apereo/cas/pull/4942/diff?src=pr&el=tree#diff-Y29yZS9jYXMtc2VydmVyLWNvcmUtd2ViLWFwaS9zcmMvbWFpbi9qYXZhL29yZy9hcGVyZW8vY2FzL3dlYi92aWV3L0R5bmFtaWNIdG1sVmlldy5qYXZh) | `0.00000% <0.00000%> (-100.00000%)` | `0.00000% <0.00000%> (-2.00000%)` | |\n| [...n/java/org/apereo/cas/ticket/ExpirationPolicy.java](https://codecov.io/gh/apereo/cas/pull/4942/diff?src=pr&el=tree#diff-YXBpL2Nhcy1zZXJ2ZXItY29yZS1hcGktdGlja2V0L3NyYy9tYWluL2phdmEvb3JnL2FwZXJlby9jYXMvdGlja2V0L0V4cGlyYXRpb25Qb2xpY3kuamF2YQ==) | `0.00000% <0.00000%> (-100.00000%)` | `0.00000% <0.00000%> (-1.00000%)` | |\n| [.../java/org/apereo/cas/DefaultMessageDescriptor.java](https://codecov.io/gh/apereo/cas/pull/4942/diff?src=pr&el=tree#diff-Y29yZS9jYXMtc2VydmVyLWNvcmUtdXRpbC1hcGkvc3JjL21haW4vamF2YS9vcmcvYXBlcmVvL2Nhcy9EZWZhdWx0TWVzc2FnZURlc2NyaXB0b3IuamF2YQ==) | `0.00000% <0.00000%> (-100.00000%)` | `0.00000% <0.00000%> (-1.00000%)` | |\n| [...c/main/java/org/apereo/cas/jpa/JpaBeanFactory.java](https://codecov.io/gh/apereo/cas/pull/4942/diff?src=pr&el=tree#diff-c3VwcG9ydC9jYXMtc2VydmVyLXN1cHBvcnQtanBhLXV0aWwvc3JjL21haW4vamF2YS9vcmcvYXBlcmVvL2Nhcy9qcGEvSnBhQmVhbkZhY3RvcnkuamF2YQ==) | `0.00000% <0.00000%> (-100.00000%)` | `0.00000% <0.00000%> (-1.00000%)` | |\n| [...in/java/org/apereo/cas/ticket/ProxyTicketImpl.java](https://codecov.io/gh/apereo/cas/pull/4942/diff?src=pr&el=tree#diff-Y29yZS9jYXMtc2VydmVyLWNvcmUtdGlja2V0cy1hcGkvc3JjL21haW4vamF2YS9vcmcvYXBlcmVvL2Nhcy90aWNrZXQvUHJveHlUaWNrZXRJbXBsLmphdmE=) | `0.00000% <0.00000%> (-100.00000%)` | `0.00000% <0.00000%> (-2.00000%)` | |\n| ... and [1904 more](https://codecov.io/gh/apereo/cas/pull/4942/diff?src=pr&el=tree-more) | |\n\n------\n\n[Continue to review full report at Codecov](https://codecov.io/gh/apereo/cas/pull/4942?src=pr&el=continue).\n> **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta)\n> `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`\n> Powered by [Codecov](https://codecov.io/gh/apereo/cas/pull/4942?src=pr&el=footer). Last update [bb27600...8d34290](https://codecov.io/gh/apereo/cas/pull/4942?src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).\n"
            }
          ]
        },
        {
          "issue_url": "https://github.com/Jasig/cas/issues/4937",
          "issue_title": "GitServiceRepository:  settings branchesToClone fix",
          "issue_number": 4937,
          "issue_text": "There isn't possible to set branchesToClone.I found what is the problem.",
          "issue_comments": []
        },
        {
          "issue_url": "https://github.com/Jasig/cas/issues/4928",
          "issue_title": "Log4j config update",
          "issue_number": 4928,
          "issue_text": "The current LoggingUtils class only logs stack traces if debug level is turned on for the particular logger logging the exception.\\r\\nI like to see stack traces if errors are happening and not have to wait to hear about a problem, turn on debug and then recreate the problem to see the stack trace. \\r\\nStack traces do make a mess of the logs so this config puts them in their own log file where they can be matched up via timestamp to messages from console or from regular log file where stack traces are turned off by default. \\r\\nThe old behavior (stack traces in console or file and no separate stack trace file) can be achieved via system properties or updating the log4j2 properties. \\r\\n\\r\\nOld/Current behavior could be achieved via:\\r\\n```\\r\\n-Dlog.console.stacktraces=true\\r\\n-Dlog.file.stacktraces=true\\r\\n-Dlog.stacktraceappender=null\\r\\n-Dlog.include.location=true\\r\\n```\\r\\n\\r\\nThis also changes the way Async logging is turned on and there are fairly significant performance benefits to doing it this way according to this document: (\"all async\" method vs. using AsyncLogger or AsyncAppender)\\r\\n[Log4j \"all async\" Logging](https://logging.apache.org/log4j/2.x/manual/async.html#Asynchronous_Logging_Performance)\\r\\n\\r\\nTurning on AsyncLogging would be done via:\\r\\n```\\r\\n-Dlog4j2.contextSelector=org.apache.logging.log4j.core.async.AsyncLoggerContextSelector\\r\\n```\\r\\nI have seen cases in the past during development where CAS dies on startup and async logging is not flushed. With this config as the default, most users would have regular logging during development and could turn on async logging when their server is in production and performance matters. \\r\\n\\r\\nI also disabled location info (`includeLocation`) by default and made it configurable b/c the log4j2 docs say it is 30-100 times slower when doing async logging and I don't see where the location info is being used in the pattern layouts. \\r\\n\\r\\n\\r\\n\\r\\nThe benefit of LoggingUtils is that it replaces null messages with the classname of the exception so logger.error(e.getMessage()); is more useful if the message is null. \\r\\nAnother benefit is that it replaces the following with a one liner:\\r\\n```\\r\\ncatch (Exception e) {\\r\\n  if (logger.isDebugEnable()) {\\r\\n    logger.error(msg, e);\\r\\n  } else {\\r\\n    logger.error(msg);\\r\\n  }\\r\\n```\\r\\n\\r\\nI would rather go ahead and log the exceptions to their own log file and not have to turn on debug to get the stack traces but I suppose it could be made configurable. \\r\\n\\r\\nIf there is a NPE or some other runtime exception, the stack trace can tell you where and the code can be updated to avoid that error. If there are expected types of exceptions and a desire to catch them and continue, I would rather catch those specific exceptions and log them at an appropriate level with no stack trace (if they are expected you already know where they came from), and still catch Exception if there is a desire to continue regardless but log the unexpected exception with a stack trace so it can be avoided or added to the expected exception list. \\r\\n\\r\\nThis is WIP pending feedback and because there is documentation to update (AsyncLogger -> Logger), other log4j2.xml files to update and I should probably test it. ",
          "issue_comments": [
            {
              "comment_username": "mmoayyed",
              "comment_create_time": "2020-08-19T08:03:49Z",
              "comment_edit_time": "2020-08-19T08:03:49Z",
              "comment_text": "This all looks great to me.\\r\\n\\r\\nI'd only add that it would be best if `org.apache.logging.log4j.core.async.AsyncLoggerContextSelector` could in fact be the default, where folks could then turn this off via a setting. Are there any downsides to making the system be prod-ready by default besides the possible albeit rare flushing issues? My concern is that if we are asking folks to set `-Dlog4j2.contextSelector=org.apache.logging.log4j.core.async.AsyncLoggerContextSelector`, that's just one extra thing to set, to remember, to document, etc in scenarios where performance might be an issue and it would waste time and resources to figure out if perf is in fact hit by less-than-ideal logging config vs actually presenting the best possible config by default. Letting the system run at peak performance seems to come at no significant cost, and *\"It should just work\"* without extra *magic* settings....unless log4j sort of makes that difficult to configure as the default option?"
            },
            {
              "comment_username": "hdeadman",
              "comment_create_time": "2020-08-19T14:26:36Z",
              "comment_edit_time": "2020-08-19T14:26:36Z",
              "comment_text": "The documentation doesn't mention a way to turn it on programmatically (that I saw), but I will look into whether the switch can be done after some classes have been loaded. The fact that they don't have a way to turn it on in log42.xml config makes me suspect that it is a decision that is made early on by one of the first classes loaded in log4j2. "
            },
            {
              "comment_username": "mmoayyed",
              "comment_create_time": "2020-08-19T14:54:18Z",
              "comment_edit_time": "2020-08-19T14:54:18Z",
              "comment_text": "If there is no option to make this be the default, then let's not add code to make it so since I imagine it would be difficult to write tests for this sort of thing. Rather, we could build support for the option in the overlay where the build script could auto-set the system property before launching the war, etc. Same goes for Docker builds too in the same overlay. I think that should be enough as a default option. "
            },
            {
              "comment_username": "apereocas-bot",
              "comment_create_time": "2020-09-21T01:41:29Z",
              "comment_edit_time": "2020-09-21T01:41:31Z",
              "comment_text": "Thank you very much for submitting this pull request! \n\nThis patch contains a very large number of commits or changed files and is quite impractical to evaluate and review. Please make sure your changes are broken down into smaller pull requests so that members can assist and review as quickly as possible. Furthermore, make sure your patch is based on the appropriate branch, is a feature branch and targets the correct CAS branch here to avoid conflicts. \nIf you believe this to be an error, please post your explanation here as a comment and it will be reviewed as quickly as possible. \nFor additional details, please review https://apereo.github.io/cas/developer/Contributor-Guidelines.html\n\nIf you are seeking assistance and have a question about your CAS deployment, please visit https://apereo.github.io/cas/Support.html to learn more about support options.\n\n<blockquote><div><strong><a href=\"https://apereo.github.io/cas/developer/Contributor-Guidelines.html\">CAS - Contributor Guidelines</a></strong></div><div>CAS - Enterprise Single Sign-On for the Web</div></blockquote>\n<blockquote><div><strong><a href=\"https://apereo.github.io/cas/Support.html\">CAS - Support</a></strong></div><div>CAS - Enterprise Single Sign-On for the Web</div></blockquote>"
            },
            {
              "comment_username": "hdeadman",
              "comment_create_time": "2020-09-21T01:57:03Z",
              "comment_edit_time": "2020-09-21T01:57:03Z",
              "comment_text": "I have been trying to add a custom filter that would only log messages with a stack trace in a dedicated stack trace log file. I think I was probably having difficulties due to the way the CasAppender wraps the regular appender but it seems to work when I add the filter to the CasAppender. It might be better to make it so CasAppender just supports adding any filter rather than having it only accept this one type of filter. I will try that when I get a chance.\\r\\n\\r\\nI added the shadowJar plugin which makes a file `core/cas-server-core-logging-api/out/production/classes/META-INF/org/apache/logging/log4j/core/config/plugins/Log4j2Plugins.dat` that helps Log4j2 find custom plugins, although it probably isn't necessary if the log4j2.xml file has `packages=\"org.apereo.cas.logging\"`. One or the other is probably sufficient. \\r\\n\\r\\nThis filter should allow for exceptions to be logged at higher levels (WARN/ERROR) without a stack trace and they will show up as one liners in the main logfile and the console but their stack trace will show up in the `cas_stacktrace.log` that someone can consult for the details. Alternatively, someone could get rid of that appender and just turn on stack traces for the main log file and/or the console appender. "
            },
            {
              "comment_username": "hdeadman",
              "comment_create_time": "2020-09-27T02:16:33Z",
              "comment_edit_time": "2020-09-27T02:16:33Z",
              "comment_text": "Now async is on by default using log4j2.component.properties. This could be overridden in an overlay (or on command line) if someone didn't want async logging. "
            },
            {
              "comment_username": "codecov[bot]",
              "comment_create_time": "2020-09-27T02:30:50Z",
              "comment_edit_time": "2020-09-28T02:47:07Z",
              "comment_text": "# [Codecov](https://codecov.io/gh/apereo/cas/pull/4928?src=pr&el=h1) Report\n> Merging [#4928](https://codecov.io/gh/apereo/cas/pull/4928?src=pr&el=desc) into [master](https://codecov.io/gh/apereo/cas/commit/49eb3617c9a870e65faa1f3d04384000adfb9a6f?el=desc) will **decrease** coverage by `82.29007%`.\n> The diff coverage is `0.00000%`.\n\n[![Impacted file tree graph](https://codecov.io/gh/apereo/cas/pull/4928/graphs/tree.svg?width=650&height=150&src=pr&token=T0wbvlaNTv)](https://codecov.io/gh/apereo/cas/pull/4928?src=pr&el=tree)\n\n```diff\n@@                  Coverage Diff                  @@\n##                master      #4928          +/-   ##\n=====================================================\n- Coverage     85.27752%   2.98745%   -82.29007%     \n+ Complexity       14483        657       -13826     \n=====================================================\n  Files             2404       2405           +1     \n  Lines            55276      55298          +22     \n  Branches          4560       4560                  \n=====================================================\n- Hits             47138       1652       -45486     \n- Misses            5008      53484       +48476     \n+ Partials          3130        162        -2968     \n```\n\n| Flag | Coverage Δ | Complexity Δ | |\n|---|---|---|---|\n| #actuator | `?` | `?` | |\n| #attributes | `?` | `?` | |\n| #audit | `?` | `?` | |\n| #authentication | `?` | `?` | |\n| #aws | `?` | `?` | |\n| #cas | `?` | `?` | |\n| #cas-config | `?` | `?` | |\n| #cassandra | `?` | `?` | |\n| #cosmosdb | `?` | `?` | |\n| #couchbase | `?` | `?` | |\n| #couchdb | `?` | `?` | |\n| #dynamodb | `?` | `?` | |\n| #ehcache | `?` | `?` | |\n| #expiration-policy | `?` | `?` | |\n| #filesystem | `?` | `?` | |\n| #groovy | `?` | `?` | |\n| #hazelcast | `?` | `?` | |\n| #ignite | `2.98745% <0.00000%> (-0.00843%)` | `657.00000 <0.00000> (-2.00000)` | |\n| #infinispan | `?` | `?` | |\n| #influxdb | `?` | `?` | |\n| #jdbc | `?` | `?` | |\n| #jms | `?` | `?` | |\n| #jmx | `?` | `?` | |\n| #kafka | `?` | `?` | |\n| #ldap | `?` | `?` | |\n| #logout | `?` | `?` | |\n| #mail | `?` | `?` | |\n| #mariadb | `?` | `?` | |\n| #memcached | `?` | `?` | |\n| #metrics | `?` | `?` | |\n| #mfa | `?` | `?` | |\n| #mongo | `?` | `?` | |\n| #mssql | `?` | `?` | |\n| #mysql | `?` | `?` | |\n| #oauth | `?` | `?` | |\n| #oidc | `?` | `?` | |\n| #oracle | `?` | `?` | |\n| #password-ops | `?` | `?` | |\n| #postgres | `?` | `?` | |\n| #radius | `?` | `?` | |\n| #redis | `?` | `?` | |\n| #rest | `?` | `?` | |\n| #saml | `?` | `?` | |\n| #services | `?` | `?` | |\n| #shell | `?` | `?` | |\n| #simple | `?` | `?` | |\n| #sms | `?` | `?` | |\n| #spnego | `?` | `?` | |\n| #tickets | `?` | `?` | |\n| #uma | `?` | `?` | |\n| #util | `?` | `?` | |\n| #web | `?` | `?` | |\n| #webapp | `?` | `?` | |\n| #webflow | `?` | `?` | |\n| #webflow-actions | `?` | `?` | |\n| #webflow-config | `?` | `?` | |\n| #webflow-events | `?` | `?` | |\n| #webflow-mfa-actions | `?` | `?` | |\n| #wsfed | `?` | `?` | |\n| #x509 | `?` | `?` | |\n| #zookeeper | `?` | `?` | |\n\nFlags with carried forward coverage won't be shown. [Click here](https://docs.codecov.io/docs/carryforward-flags#carryforward-flags-in-the-pull-request-comment) to find out more.\n\n| [Impacted Files](https://codecov.io/gh/apereo/cas/pull/4928?src=pr&el=tree) | Coverage Δ | Complexity Δ | |\n|---|---|---|---|\n| [.../main/java/org/apereo/cas/logging/CasAppender.java](https://codecov.io/gh/apereo/cas/pull/4928/diff?src=pr&el=tree#diff-Y29yZS9jYXMtc2VydmVyLWNvcmUtbG9nZ2luZy1hcGkvc3JjL21haW4vamF2YS9vcmcvYXBlcmVvL2Nhcy9sb2dnaW5nL0Nhc0FwcGVuZGVyLmphdmE=) | `0.00000% <0.00000%> (-75.00000%)` | `0.00000 <0.00000> (-3.00000)` | |\n| [...va/org/apereo/cas/logging/ExceptionOnlyFilter.java](https://codecov.io/gh/apereo/cas/pull/4928/diff?src=pr&el=tree#diff-Y29yZS9jYXMtc2VydmVyLWNvcmUtbG9nZ2luZy1hcGkvc3JjL21haW4vamF2YS9vcmcvYXBlcmVvL2Nhcy9sb2dnaW5nL0V4Y2VwdGlvbk9ubHlGaWx0ZXIuamF2YQ==) | `0.00000% <0.00000%> (ø)` | `0.00000 <0.00000> (?)` | |\n| [...rc/main/java/org/apereo/cas/util/LoggingUtils.java](https://codecov.io/gh/apereo/cas/pull/4928/diff?src=pr&el=tree#diff-Y29yZS9jYXMtc2VydmVyLWNvcmUtdXRpbC1hcGkvc3JjL21haW4vamF2YS9vcmcvYXBlcmVvL2Nhcy91dGlsL0xvZ2dpbmdVdGlscy5qYXZh) | `0.00000% <0.00000%> (-85.71429%)` | `0.00000 <0.00000> (-6.00000)` | |\n| [...i/src/main/java/org/apereo/cas/util/JsonUtils.java](https://codecov.io/gh/apereo/cas/pull/4928/diff?src=pr&el=tree#diff-Y29yZS9jYXMtc2VydmVyLWNvcmUtdXRpbC1hcGkvc3JjL21haW4vamF2YS9vcmcvYXBlcmVvL2Nhcy91dGlsL0pzb25VdGlscy5qYXZh) | `0.00000% <0.00000%> (-100.00000%)` | `0.00000% <0.00000%> (-4.00000%)` | |\n| [.../src/main/java/org/apereo/cas/util/RegexUtils.java](https://codecov.io/gh/apereo/cas/pull/4928/diff?src=pr&el=tree#diff-Y29yZS9jYXMtc2VydmVyLWNvcmUtdXRpbC1hcGkvc3JjL21haW4vamF2YS9vcmcvYXBlcmVvL2Nhcy91dGlsL1JlZ2V4VXRpbHMuamF2YQ==) | `0.00000% <0.00000%> (-100.00000%)` | `0.00000% <0.00000%> (-13.00000%)` | |\n| [...y/src/main/java/org/apereo/cas/CasJettyBanner.java](https://codecov.io/gh/apereo/cas/pull/4928/diff?src=pr&el=tree#diff-d2ViYXBwL2Nhcy1zZXJ2ZXItd2ViYXBwLWpldHR5L3NyYy9tYWluL2phdmEvb3JnL2FwZXJlby9jYXMvQ2FzSmV0dHlCYW5uZXIuamF2YQ==) | `0.00000% <0.00000%> (-100.00000%)` | `0.00000% <0.00000%> (-2.00000%)` | |\n| [...src/main/java/org/apereo/cas/util/SocketUtils.java](https://codecov.io/gh/apereo/cas/pull/4928/diff?src=pr&el=tree#diff-Y29yZS9jYXMtc2VydmVyLWNvcmUtdXRpbC1hcGkvc3JjL21haW4vamF2YS9vcmcvYXBlcmVvL2Nhcy91dGlsL1NvY2tldFV0aWxzLmphdmE=) | `0.00000% <0.00000%> (-100.00000%)` | `0.00000% <0.00000%> (-1.00000%)` | |\n| [.../src/main/java/org/apereo/cas/CasTomcatBanner.java](https://codecov.io/gh/apereo/cas/pull/4928/diff?src=pr&el=tree#diff-d2ViYXBwL2Nhcy1zZXJ2ZXItd2ViYXBwLXRvbWNhdC9zcmMvbWFpbi9qYXZhL29yZy9hcGVyZW8vY2FzL0Nhc1RvbWNhdEJhbm5lci5qYXZh) | `0.00000% <0.00000%> (-100.00000%)` | `0.00000% <0.00000%> (-2.00000%)` | |\n| [...ain/java/org/apereo/cas/util/http/HttpMessage.java](https://codecov.io/gh/apereo/cas/pull/4928/diff?src=pr&el=tree#diff-Y29yZS9jYXMtc2VydmVyLWNvcmUtdXRpbC1hcGkvc3JjL21haW4vamF2YS9vcmcvYXBlcmVvL2Nhcy91dGlsL2h0dHAvSHR0cE1lc3NhZ2UuamF2YQ==) | `0.00000% <0.00000%> (-100.00000%)` | `0.00000% <0.00000%> (-4.00000%)` | |\n| [...rc/main/java/org/apereo/cas/CasUndertowBanner.java](https://codecov.io/gh/apereo/cas/pull/4928/diff?src=pr&el=tree#diff-d2ViYXBwL2Nhcy1zZXJ2ZXItd2ViYXBwLXVuZGVydG93L3NyYy9tYWluL2phdmEvb3JnL2FwZXJlby9jYXMvQ2FzVW5kZXJ0b3dCYW5uZXIuamF2YQ==) | `0.00000% <0.00000%> (-100.00000%)` | `0.00000% <0.00000%> (-2.00000%)` | |\n| ... and [2277 more](https://codecov.io/gh/apereo/cas/pull/4928/diff?src=pr&el=tree-more) | |\n\n------\n\n[Continue to review full report at Codecov](https://codecov.io/gh/apereo/cas/pull/4928?src=pr&el=continue).\n> **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta)\n> `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`\n> Powered by [Codecov](https://codecov.io/gh/apereo/cas/pull/4928?src=pr&el=footer). Last update [49eb361...16245d8](https://codecov.io/gh/apereo/cas/pull/4928?src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).\n"
            },
            {
              "comment_username": "hdeadman",
              "comment_create_time": "2020-09-29T14:34:21Z",
              "comment_edit_time": "2020-09-29T15:04:33Z",
              "comment_text": "This may need some more documentation but at this point I am ready for feedback. Async is now on by default but someone in an overlay can turn it off by using system property or putting `log4j2.component.properties` in their WEB-INF/classes to override the one in in logging-api module that turns on async. My hope is that more bad things with exceptions can be logged at ERROR or WARN and not clutter up the main log files b/c exceptions will go to their own logfile (and won't be logged as DEBUG which forces adjusting log level and re-creating error to get stack trace). I also would need to update log4j2.xml in the cas-overlay.\r\n\r\nLog4j2 supports a programmatic property source interface that one can implement and it would use that to get properties instead of or in addition to `log42.component.properties` and I briefly went down road of letting async be turned on by a regular spring boot property, but all of that happens too late, long after log4j would have been initialized, although I think CAS may re-initialize log4j so maybe it would be there in time for that. It probably is simpler the way it is since most people won't adjust async."
            }
          ]
        },
        {
          "issue_url": "https://github.com/Jasig/cas/issues/4906",
          "issue_title": "WIP - DelegatedClientID being lost",
          "issue_number": 4906,
          "issue_text": "I believe this is a bug, but wanted to verify before I create a fix. \r\n\r\n**Issue:**\r\nCannot get delegatedClientId param added to query string URL when creating customized webflow using Pac4j library.\r\n\r\n**Evidence:**\r\nIn CAS 5.3.x \r\n\"support/cas-server-support-pac4j-webflow/src/main/java/org/apereo/cas/web/DelegatedClientWebflowManager.java\" the clientId was added to configuration.customParams \r\n```\r\n        if (client instanceof CasClient) {\r\n            final CasClient casClient = (CasClient) client;\r\n            casClient.getConfiguration().addCustomParam(DelegatedClientWebflowManager.PARAMETER_CLIENT_ID, ticketId);\r\n        }\r\n```\r\nThen \"org/pac4j/pac4j-core/3.6.1/pac4j-core-3.6.1.jar!/org/pac4j/core/http/callback/QueryParameterCallbackUrlResolver.class\" would pull the value from the same configuration params\r\n```\r\n        Entry entry;\r\n        for(Iterator var6 = this.customParams.entrySet().iterator(); var6.hasNext(); newUrl = CommonHelper.addParameter(newUrl, (String)entry.getKey(), (String)entry.getValue())) {\r\n            entry = (Entry)var6.next();\r\n        }\r\n```\r\n\r\nIn 6.1.6 and 6.2 \r\n\"support/cas-server-support-pac4j-webflow/src/main/java/org/apereo/cas/web/DelegatedClientWebflowManager.java\" saves to the session now\r\n```\r\n        if (client instanceof CasClient) {\r\n            sessionStore.set(webContext, CAS_CLIENT_ID_SESSION_KEY, ticket.getId());\r\n        }\r\n```\r\nbut Pac4j is still pulling from the configurations custom params:\r\n```\r\n        for (final Map.Entry<String, String> entry : this.customParams.entrySet()) {\r\n            newUrl = CommonHelper.addParameter(newUrl, entry.getKey(), entry.getValue());\r\n        }\r\n```\r\n\r\nIf this is truly a bug, I can work on creating the fix. But I am not sure if should go in the CAS repo in the  \"DelegatedClientWebflowManager.java\" file or should the fix be created in the Pac4j repo for file \"QueryParameterCallbackUrlResolver.class\"?\r\n\r\nThanks for your input!\r\n\r\n\r\n",
          "issue_comments": [
            {
              "comment_username": "codecov[bot]",
              "comment_create_time": "2020-07-15T22:29:38Z",
              "comment_edit_time": "2020-07-15T22:35:54Z",
              "comment_text": "# [Codecov](https://codecov.io/gh/apereo/cas/pull/4906?src=pr&el=h1) Report\n> Merging [#4906](https://codecov.io/gh/apereo/cas/pull/4906?src=pr&el=desc) into [master](https://codecov.io/gh/apereo/cas/commit/3444fb0cb60cea955a64e72f9b484294c5ea74c2&el=desc) will **decrease** coverage by `67.19312%`.\n> The diff coverage is `n/a`.\n\n[![Impacted file tree graph](https://codecov.io/gh/apereo/cas/pull/4906/graphs/tree.svg?width=650&height=150&src=pr&token=T0wbvlaNTv)](https://codecov.io/gh/apereo/cas/pull/4906?src=pr&el=tree)\n\n```diff\n@@                  Coverage Diff                   @@\n##                master       #4906          +/-   ##\n======================================================\n- Coverage     81.05142%   13.85830%   -67.19312%     \n+ Complexity       13471        2653       -10818     \n======================================================\n  Files             2341        2341                  \n  Lines            53946       53946                  \n  Branches          4497        4497                  \n======================================================\n- Hits             43724        7476       -36248     \n- Misses            6954       45775       +38821     \n+ Partials          3268         695        -2573     \n```\n\n| Flag | Coverage Δ | Complexity Δ | |\n|---|---|---|---|\n| #authentication | `?` | `?` | |\n| #aws | `?` | `?` | |\n| #casconfig | `0.60431% <ø> (ø)` | `103.00000 <ø> (ø)` | |\n| #cassandra | `?` | `?` | |\n| #cosmosdb | `?` | `?` | |\n| #couchbase | `?` | `?` | |\n| #couchdb | `?` | `?` | |\n| #dynamodb | `?` | `?` | |\n| #ehcache | `?` | `?` | |\n| #filesystem | `?` | `?` | |\n| #groovy | `?` | `?` | |\n| #hazelcast | `?` | `?` | |\n| #ignite | `?` | `?` | |\n| #infinispan | `?` | `?` | |\n| #influxdb | `?` | `?` | |\n| #jdbc | `?` | `?` | |\n| #jms | `?` | `?` | |\n| #jmx | `?` | `?` | |\n| #kafka | `?` | `?` | |\n| #ldap | `?` | `?` | |\n| #mail | `?` | `?` | |\n| #mariadb | `?` | `?` | |\n| #memcached | `?` | `?` | |\n| #mfa | `?` | `?` | |\n| #mongo | `?` | `?` | |\n| #mssql | `?` | `?` | |\n| #mysql | `?` | `?` | |\n| #oauth | `?` | `?` | |\n| #oidc | `11.20380% <ø> (ø)` | `2171.00000 <ø> (ø)` | |\n| #oracle | `?` | `?` | |\n| #postgres | `?` | `?` | |\n| #radius | `?` | `?` | |\n| #redis | `?` | `?` | |\n| #rest | `?` | `?` | |\n| #saml | `?` | `?` | |\n| #services | `?` | `?` | |\n| #shell | `?` | `?` | |\n| #simple | `?` | `?` | |\n| #sms | `?` | `?` | |\n| #spnego | `?` | `?` | |\n| #tickets | `?` | `?` | |\n| #uma | `?` | `?` | |\n| #util | `?` | `?` | |\n| #webapp | `?` | `?` | |\n| #webflow | `?` | `?` | |\n| #wsfed | `?` | `?` | |\n| #x509 | `4.06888% <ø> (+0.00742%)` | `847.00000 <ø> (+2.00000)` | |\n| #zookeeper | `0.00741% <ø> (?)` | `1.00000 <ø> (?)` | |\n\n| [Impacted Files](https://codecov.io/gh/apereo/cas/pull/4906?src=pr&el=tree) | Coverage Δ | Complexity Δ | |\n|---|---|---|---|\n| [.../apereo/cas/web/DelegatedClientWebflowManager.java](https://codecov.io/gh/apereo/cas/pull/4906/diff?src=pr&el=tree#diff-c3VwcG9ydC9jYXMtc2VydmVyLXN1cHBvcnQtcGFjNGotd2ViZmxvdy9zcmMvbWFpbi9qYXZhL29yZy9hcGVyZW8vY2FzL3dlYi9EZWxlZ2F0ZWRDbGllbnRXZWJmbG93TWFuYWdlci5qYXZh) | `0.00000% <ø> (-67.77778%)` | `0.00000 <0.00000> (-10.00000)` | |\n| [...i/src/main/java/org/apereo/cas/util/JsonUtils.java](https://codecov.io/gh/apereo/cas/pull/4906/diff?src=pr&el=tree#diff-Y29yZS9jYXMtc2VydmVyLWNvcmUtdXRpbC1hcGkvc3JjL21haW4vamF2YS9vcmcvYXBlcmVvL2Nhcy91dGlsL0pzb25VdGlscy5qYXZh) | `0.00000% <0.00000%> (-100.00000%)` | `0.00000% <0.00000%> (-4.00000%)` | |\n| [...y/src/main/java/org/apereo/cas/CasJettyBanner.java](https://codecov.io/gh/apereo/cas/pull/4906/diff?src=pr&el=tree#diff-d2ViYXBwL2Nhcy1zZXJ2ZXItd2ViYXBwLWpldHR5L3NyYy9tYWluL2phdmEvb3JnL2FwZXJlby9jYXMvQ2FzSmV0dHlCYW5uZXIuamF2YQ==) | `0.00000% <0.00000%> (-100.00000%)` | `0.00000% <0.00000%> (-2.00000%)` | |\n| [.../src/main/java/org/apereo/cas/CasTomcatBanner.java](https://codecov.io/gh/apereo/cas/pull/4906/diff?src=pr&el=tree#diff-d2ViYXBwL2Nhcy1zZXJ2ZXItd2ViYXBwLXRvbWNhdC9zcmMvbWFpbi9qYXZhL29yZy9hcGVyZW8vY2FzL0Nhc1RvbWNhdEJhbm5lci5qYXZh) | `0.00000% <0.00000%> (-100.00000%)` | `0.00000% <0.00000%> (-2.00000%)` | |\n| [...rc/main/java/org/apereo/cas/CasUndertowBanner.java](https://codecov.io/gh/apereo/cas/pull/4906/diff?src=pr&el=tree#diff-d2ViYXBwL2Nhcy1zZXJ2ZXItd2ViYXBwLXVuZGVydG93L3NyYy9tYWluL2phdmEvb3JnL2FwZXJlby9jYXMvQ2FzVW5kZXJ0b3dCYW5uZXIuamF2YQ==) | `0.00000% <0.00000%> (-100.00000%)` | `0.00000% <0.00000%> (-2.00000%)` | |\n| [.../java/org/apereo/cas/web/view/DynamicHtmlView.java](https://codecov.io/gh/apereo/cas/pull/4906/diff?src=pr&el=tree#diff-Y29yZS9jYXMtc2VydmVyLWNvcmUtd2ViLWFwaS9zcmMvbWFpbi9qYXZhL29yZy9hcGVyZW8vY2FzL3dlYi92aWV3L0R5bmFtaWNIdG1sVmlldy5qYXZh) | `0.00000% <0.00000%> (-100.00000%)` | `0.00000% <0.00000%> (-2.00000%)` | |\n| [...n/java/org/apereo/cas/ticket/ExpirationPolicy.java](https://codecov.io/gh/apereo/cas/pull/4906/diff?src=pr&el=tree#diff-YXBpL2Nhcy1zZXJ2ZXItY29yZS1hcGktdGlja2V0L3NyYy9tYWluL2phdmEvb3JnL2FwZXJlby9jYXMvdGlja2V0L0V4cGlyYXRpb25Qb2xpY3kuamF2YQ==) | `0.00000% <0.00000%> (-100.00000%)` | `0.00000% <0.00000%> (-1.00000%)` | |\n| [...a/org/apereo/cas/rest/BadRestRequestException.java](https://codecov.io/gh/apereo/cas/pull/4906/diff?src=pr&el=tree#diff-Y29yZS9jYXMtc2VydmVyLWNvcmUtcmVzdC9zcmMvbWFpbi9qYXZhL29yZy9hcGVyZW8vY2FzL3Jlc3QvQmFkUmVzdFJlcXVlc3RFeGNlcHRpb24uamF2YQ==) | `0.00000% <0.00000%> (-100.00000%)` | `0.00000% <0.00000%> (-1.00000%)` | |\n| [.../java/org/apereo/cas/DefaultMessageDescriptor.java](https://codecov.io/gh/apereo/cas/pull/4906/diff?src=pr&el=tree#diff-Y29yZS9jYXMtc2VydmVyLWNvcmUtdXRpbC1hcGkvc3JjL21haW4vamF2YS9vcmcvYXBlcmVvL2Nhcy9EZWZhdWx0TWVzc2FnZURlc2NyaXB0b3IuamF2YQ==) | `0.00000% <0.00000%> (-100.00000%)` | `0.00000% <0.00000%> (-1.00000%)` | |\n| [...c/main/java/org/apereo/cas/jpa/JpaBeanFactory.java](https://codecov.io/gh/apereo/cas/pull/4906/diff?src=pr&el=tree#diff-c3VwcG9ydC9jYXMtc2VydmVyLXN1cHBvcnQtanBhLXV0aWwvc3JjL21haW4vamF2YS9vcmcvYXBlcmVvL2Nhcy9qcGEvSnBhQmVhbkZhY3RvcnkuamF2YQ==) | `0.00000% <0.00000%> (-100.00000%)` | `0.00000% <0.00000%> (-1.00000%)` | |\n| ... and [1960 more](https://codecov.io/gh/apereo/cas/pull/4906/diff?src=pr&el=tree-more) | |\n\n------\n\n[Continue to review full report at Codecov](https://codecov.io/gh/apereo/cas/pull/4906?src=pr&el=continue).\n> **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta)\n> `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`\n> Powered by [Codecov](https://codecov.io/gh/apereo/cas/pull/4906?src=pr&el=footer). Last update [3444fb0...285e3ce](https://codecov.io/gh/apereo/cas/pull/4906?src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).\n"
            },
            {
              "comment_username": "mmoayyed",
              "comment_create_time": "2020-07-16T19:31:58Z",
              "comment_edit_time": "2020-07-16T19:32:13Z",
              "comment_text": "Based on what you're describing, this is an issue that should be fixed here and likely in `DelegatedClientWebflowManager`. I'd suggest that you begin by creating a test case that duplicates the problem, and then proceed with the actual fix. "
            },
            {
              "comment_username": "astohn",
              "comment_create_time": "2020-07-17T20:14:05Z",
              "comment_edit_time": "2020-07-17T20:14:05Z",
              "comment_text": "> Based on what you're describing, this is an issue that should be fixed here and likely in `DelegatedClientWebflowManager`. I'd suggest that you begin by creating a test case that duplicates the problem, and then proceed with the actual fix.\r\n\r\nWill do!  Did you want to see a test case before I code a solution?\r\n\r\nAxel"
            },
            {
              "comment_username": "mmoayyed",
              "comment_create_time": "2020-07-20T14:55:42Z",
              "comment_edit_time": "2020-07-20T14:55:42Z",
              "comment_text": "It's really up to you. In general, it would be better to see the test case first. "
            }
          ]
        }
      ]
    }
  },
  {
    "github_url": "https://github.com/wildfly-security/jboss-negotiation",
    "github_info": {
      "name": "wildfly-security/jboss-negotiation",
      "language": "Java"
    },
    "github_branches": {
      "branch_datas": [
        {
          "branch_version": "2.2.x",
          "branch_url": "https://github.com/wildfly-security/jboss-negotiation/tree/2.2.x",
          "branch_download_url": "https://github.com/wildfly-security/jboss-negotiation/archive/2.2.x.zip"
        },
        {
          "branch_version": "2.3.x",
          "branch_url": "https://github.com/wildfly-security/jboss-negotiation/tree/2.3.x",
          "branch_download_url": "https://github.com/wildfly-security/jboss-negotiation/archive/2.3.x.zip"
        },
        {
          "branch_version": "master",
          "branch_url": "https://github.com/wildfly-security/jboss-negotiation/tree/master",
          "branch_download_url": "https://github.com/wildfly-security/jboss-negotiation/archive/master.zip"
        },
        {
          "branch_version": "security-negotiation-2.0.x",
          "branch_url": "https://github.com/wildfly-security/jboss-negotiation/tree/security-negotiation-2.0.x",
          "branch_download_url": "https://github.com/wildfly-security/jboss-negotiation/archive/security-negotiation-2.0.x.zip"
        },
        {
          "branch_version": "security-negotiation-2.0.3.SP",
          "branch_url": "https://github.com/wildfly-security/jboss-negotiation/tree/security-negotiation-2.0.3.SP",
          "branch_download_url": "https://github.com/wildfly-security/jboss-negotiation/archive/security-negotiation-2.0.3.SP.zip"
        },
        {
          "branch_version": "security-negotiation-2.0.3.SP1_JBPAPP-3944",
          "branch_url": "https://github.com/wildfly-security/jboss-negotiation/tree/security-negotiation-2.0.3.SP1_JBPAPP-3944",
          "branch_download_url": "https://github.com/wildfly-security/jboss-negotiation/archive/security-negotiation-2.0.3.SP1_JBPAPP-3944.zip"
        },
        {
          "branch_version": "security-negotiation-2.0.3.SP03_JBPAPP-10717",
          "branch_url": "https://github.com/wildfly-security/jboss-negotiation/tree/security-negotiation-2.0.3.SP03_JBPAPP-10717",
          "branch_download_url": "https://github.com/wildfly-security/jboss-negotiation/archive/security-negotiation-2.0.3.SP03_JBPAPP-10717.zip"
        },
        {
          "branch_version": "security-negotiation-2.1.x",
          "branch_url": "https://github.com/wildfly-security/jboss-negotiation/tree/security-negotiation-2.1.x",
          "branch_download_url": "https://github.com/wildfly-security/jboss-negotiation/archive/security-negotiation-2.1.x.zip"
        },
        {
          "branch_version": "security-negotiation-2.1.3.GA_JBPAPP-10613",
          "branch_url": "https://github.com/wildfly-security/jboss-negotiation/tree/security-negotiation-2.1.3.GA_JBPAPP-10613",
          "branch_download_url": "https://github.com/wildfly-security/jboss-negotiation/archive/security-negotiation-2.1.3.GA_JBPAPP-10613.zip"
        },
        {
          "branch_version": "security-negotiation-2.2.5.Final_BZ-1040008",
          "branch_url": "https://github.com/wildfly-security/jboss-negotiation/tree/security-negotiation-2.2.5.Final_BZ-1040008",
          "branch_download_url": "https://github.com/wildfly-security/jboss-negotiation/archive/security-negotiation-2.2.5.Final_BZ-1040008.zip"
        }
      ]
    },
    "github_pull_requests": { "pull_datas": [] },
    "github_issues": {
      "issue_datas": [
        {
          "issue_url": "https://github.com/wildfly-security/jboss-negotiation/issues/34",
          "issue_title": "Typo in error page",
          "issue_number": 34,
          "issue_text": "If SPNEGO doesn't work, I get an error page containing the message\r\n\r\n`If this page is displayed your web broweser is not taking part in the SPNEGO process, a username and password can be entered instead to fall back to username/password authentication.`\r\n\r\nThe word 'broweser' should be written 'browser'.",
          "issue_comments": []
        }
      ]
    }
  },
  {
    "github_url": "https://github.com/eclipse/jetty.project",
    "github_info": {
      "name": "eclipse/jetty.project",
      "language": "Java"
    },
    "github_branches": {
      "branch_datas": [
        {
          "branch_version": "dependabot/maven/jetty-9.4.x/org.apache.kerby-kerb-simplekdc-2.0.1",
          "branch_url": "https://github.com/eclipse/jetty.project/tree/dependabot/maven/jetty-9.4.x/org.apache.kerby-kerb-simplekdc-2.0.1",
          "branch_download_url": "https://github.com/eclipse/jetty.project/archive/dependabot/maven/jetty-9.4.x/org.apache.kerby-kerb-simplekdc-2.0.1.zip"
        },
        {
          "branch_version": "dependabot/maven/jetty-9.4.x/org.apache.openwebbeans-openwebbeans-web-2.0.18",
          "branch_url": "https://github.com/eclipse/jetty.project/tree/dependabot/maven/jetty-9.4.x/org.apache.openwebbeans-openwebbeans-web-2.0.18",
          "branch_download_url": "https://github.com/eclipse/jetty.project/archive/dependabot/maven/jetty-9.4.x/org.apache.openwebbeans-openwebbeans-web-2.0.18.zip"
        },
        {
          "branch_version": "dependabot/maven/jetty-10.0.x/asm.version-9.0",
          "branch_url": "https://github.com/eclipse/jetty.project/tree/dependabot/maven/jetty-10.0.x/asm.version-9.0",
          "branch_download_url": "https://github.com/eclipse/jetty.project/archive/dependabot/maven/jetty-10.0.x/asm.version-9.0.zip"
        },
        {
          "branch_version": "dependabot/maven/jetty-11.0.x/org.codehaus.plexus-plexus-compiler-javac-errorprone-2.8.8",
          "branch_url": "https://github.com/eclipse/jetty.project/tree/dependabot/maven/jetty-11.0.x/org.codehaus.plexus-plexus-compiler-javac-errorprone-2.8.8",
          "branch_download_url": "https://github.com/eclipse/jetty.project/archive/dependabot/maven/jetty-11.0.x/org.codehaus.plexus-plexus-compiler-javac-errorprone-2.8.8.zip"
        },
        {
          "branch_version": "jetty-8-jdk8",
          "branch_url": "https://github.com/eclipse/jetty.project/tree/jetty-8-jdk8",
          "branch_download_url": "https://github.com/eclipse/jetty.project/archive/jetty-8-jdk8.zip"
        },
        {
          "branch_version": "jetty-8.1.x",
          "branch_url": "https://github.com/eclipse/jetty.project/tree/jetty-8.1.x",
          "branch_download_url": "https://github.com/eclipse/jetty.project/archive/jetty-8.1.x.zip"
        },
        {
          "branch_version": "jetty-8.2.x",
          "branch_url": "https://github.com/eclipse/jetty.project/tree/jetty-8.2.x",
          "branch_download_url": "https://github.com/eclipse/jetty.project/archive/jetty-8.2.x.zip"
        },
        {
          "branch_version": "jetty-9.2.x",
          "branch_url": "https://github.com/eclipse/jetty.project/tree/jetty-9.2.x",
          "branch_download_url": "https://github.com/eclipse/jetty.project/archive/jetty-9.2.x.zip"
        },
        {
          "branch_version": "jetty-9.3.x",
          "branch_url": "https://github.com/eclipse/jetty.project/tree/jetty-9.3.x",
          "branch_download_url": "https://github.com/eclipse/jetty.project/archive/jetty-9.3.x.zip"
        },
        {
          "branch_version": "jetty-9.4.x-4954-byte-count-api",
          "branch_url": "https://github.com/eclipse/jetty.project/tree/jetty-9.4.x-4954-byte-count-api",
          "branch_download_url": "https://github.com/eclipse/jetty.project/archive/jetty-9.4.x-4954-byte-count-api.zip"
        },
        {
          "branch_version": "jetty-9.4.x-5022-FilterChainCleanup",
          "branch_url": "https://github.com/eclipse/jetty.project/tree/jetty-9.4.x-5022-FilterChainCleanup",
          "branch_download_url": "https://github.com/eclipse/jetty.project/archive/jetty-9.4.x-5022-FilterChainCleanup.zip"
        },
        {
          "branch_version": "jetty-9.4.x-5048-TempBufferReview",
          "branch_url": "https://github.com/eclipse/jetty.project/tree/jetty-9.4.x-5048-TempBufferReview",
          "branch_download_url": "https://github.com/eclipse/jetty.project/archive/jetty-9.4.x-5048-TempBufferReview.zip"
        },
        {
          "branch_version": "jetty-9.4.x-5291-TrieSize",
          "branch_url": "https://github.com/eclipse/jetty.project/tree/jetty-9.4.x-5291-TrieSize",
          "branch_download_url": "https://github.com/eclipse/jetty.project/archive/jetty-9.4.x-5291-TrieSize.zip"
        },
        {
          "branch_version": "jetty-9.4.x-5360-weblistener-origin",
          "branch_url": "https://github.com/eclipse/jetty.project/tree/jetty-9.4.x-5360-weblistener-origin",
          "branch_download_url": "https://github.com/eclipse/jetty.project/archive/jetty-9.4.x-5360-weblistener-origin.zip"
        },
        {
          "branch_version": "jetty-9.4.x-GitHubCI",
          "branch_url": "https://github.com/eclipse/jetty.project/tree/jetty-9.4.x-GitHubCI",
          "branch_download_url": "https://github.com/eclipse/jetty.project/archive/jetty-9.4.x-GitHubCI.zip"
        },
        {
          "branch_version": "jetty-9.4.x_hazelcast_remote_testing",
          "branch_url": "https://github.com/eclipse/jetty.project/tree/jetty-9.4.x_hazelcast_remote_testing",
          "branch_download_url": "https://github.com/eclipse/jetty.project/archive/jetty-9.4.x_hazelcast_remote_testing.zip"
        },
        {
          "branch_version": "jetty-9.4.x_no_wagon_ssh",
          "branch_url": "https://github.com/eclipse/jetty.project/tree/jetty-9.4.x_no_wagon_ssh",
          "branch_download_url": "https://github.com/eclipse/jetty.project/archive/jetty-9.4.x_no_wagon_ssh.zip"
        },
        {
          "branch_version": "jetty-9.4.x_skip_h2spec_if_no_docker",
          "branch_url": "https://github.com/eclipse/jetty.project/tree/jetty-9.4.x_skip_h2spec_if_no_docker",
          "branch_download_url": "https://github.com/eclipse/jetty.project/archive/jetty-9.4.x_skip_h2spec_if_no_docker.zip"
        },
        {
          "branch_version": "jetty-9.4.x",
          "branch_url": "https://github.com/eclipse/jetty.project/tree/jetty-9.4.x",
          "branch_download_url": "https://github.com/eclipse/jetty.project/archive/jetty-9.4.x.zip"
        },
        {
          "branch_version": "jetty-10.0.x-4918-extendedDecoder",
          "branch_url": "https://github.com/eclipse/jetty.project/tree/jetty-10.0.x-4918-extendedDecoder",
          "branch_download_url": "https://github.com/eclipse/jetty.project/archive/jetty-10.0.x-4918-extendedDecoder.zip"
        },
        {
          "branch_version": "jetty-10.0.x-5287-CompressionPool-Benchmark",
          "branch_url": "https://github.com/eclipse/jetty.project/tree/jetty-10.0.x-5287-CompressionPool-Benchmark",
          "branch_download_url": "https://github.com/eclipse/jetty.project/archive/jetty-10.0.x-5287-CompressionPool-Benchmark.zip"
        },
        {
          "branch_version": "jetty-10.0.x-5287-CompressionPool",
          "branch_url": "https://github.com/eclipse/jetty.project/tree/jetty-10.0.x-5287-CompressionPool",
          "branch_download_url": "https://github.com/eclipse/jetty.project/archive/jetty-10.0.x-5287-CompressionPool.zip"
        },
        {
          "branch_version": "jetty-10.0.x-AutobahnDocker",
          "branch_url": "https://github.com/eclipse/jetty.project/tree/jetty-10.0.x-AutobahnDocker",
          "branch_download_url": "https://github.com/eclipse/jetty.project/archive/jetty-10.0.x-AutobahnDocker.zip"
        },
        {
          "branch_version": "jetty-10.0.x-HttpInput-redux",
          "branch_url": "https://github.com/eclipse/jetty.project/tree/jetty-10.0.x-HttpInput-redux",
          "branch_download_url": "https://github.com/eclipse/jetty.project/archive/jetty-10.0.x-HttpInput-redux.zip"
        },
        {
          "branch_version": "jetty-10.0.x-demo-spec-reorg",
          "branch_url": "https://github.com/eclipse/jetty.project/tree/jetty-10.0.x-demo-spec-reorg",
          "branch_download_url": "https://github.com/eclipse/jetty.project/archive/jetty-10.0.x-demo-spec-reorg.zip"
        },
        {
          "branch_version": "jetty-10.0.x-updates-to-doco",
          "branch_url": "https://github.com/eclipse/jetty.project/tree/jetty-10.0.x-updates-to-doco",
          "branch_download_url": "https://github.com/eclipse/jetty.project/archive/jetty-10.0.x-updates-to-doco.zip"
        },
        {
          "branch_version": "jetty-10.0.x_ipv6",
          "branch_url": "https://github.com/eclipse/jetty.project/tree/jetty-10.0.x_ipv6",
          "branch_download_url": "https://github.com/eclipse/jetty.project/archive/jetty-10.0.x_ipv6.zip"
        },
        {
          "branch_version": "jetty-10.0.x_surefire_no_skip_errors",
          "branch_url": "https://github.com/eclipse/jetty.project/tree/jetty-10.0.x_surefire_no_skip_errors",
          "branch_download_url": "https://github.com/eclipse/jetty.project/archive/jetty-10.0.x_surefire_no_skip_errors.zip"
        },
        {
          "branch_version": "jetty-10.0.x",
          "branch_url": "https://github.com/eclipse/jetty.project/tree/jetty-10.0.x",
          "branch_download_url": "https://github.com/eclipse/jetty.project/archive/jetty-10.0.x.zip"
        },
        {
          "branch_version": "jetty-11.0.x",
          "branch_url": "https://github.com/eclipse/jetty.project/tree/jetty-11.0.x",
          "branch_download_url": "https://github.com/eclipse/jetty.project/archive/jetty-11.0.x.zip"
        }
      ]
    },
    "github_pull_requests": {
      "pull_datas": [
        {
          "pull_number": 5367,
          "pull_title": "Reorg of /demos/ with focus on demo-spec downstream dependencies.",
          "pull_version": "eclipse:jetty-10.0.x",
          "pull_version_url": "https://github.com/eclipse/jetty.project/tree/jetty-10.0.x"
        },
        {
          "pull_number": 5349,
          "pull_title": "Bump asm.version from 8.0.1 to 9.0",
          "pull_version": "eclipse:jetty-10.0.x",
          "pull_version_url": "https://github.com/eclipse/jetty.project/tree/jetty-10.0.x"
        },
        {
          "pull_number": 5337,
          "pull_title": "Bump plexus-compiler-javac-errorprone from 2.8.2 to 2.8.8",
          "pull_version": "eclipse:jetty-11.0.x",
          "pull_version_url": "https://github.com/eclipse/jetty.project/tree/jetty-11.0.x"
        },
        {
          "pull_number": 5332,
          "pull_title": "Bump kerb-simplekdc from 1.1.1 to 2.0.1",
          "pull_version": "eclipse:jetty-9.4.x",
          "pull_version_url": "https://github.com/eclipse/jetty.project/tree/jetty-9.4.x"
        },
        {
          "pull_number": 5330,
          "pull_title": "Bump openwebbeans-web from 2.0.11 to 2.0.18",
          "pull_version": "eclipse:jetty-9.4.x",
          "pull_version_url": "https://github.com/eclipse/jetty.project/tree/jetty-9.4.x"
        },
        {
          "pull_number": 5295,
          "pull_title": "Issue #5287 - rework CompressionPool to use the jetty-util pool",
          "pull_version": "eclipse:jetty-10.0.x",
          "pull_version_url": "https://github.com/eclipse/jetty.project/tree/jetty-10.0.x"
        },
        {
          "pull_number": 5293,
          "pull_title": "Issue #5291 - Capacity Check in ArrayTernaryTrie",
          "pull_version": "eclipse:jetty-9.4.x",
          "pull_version_url": "https://github.com/eclipse/jetty.project/tree/jetty-9.4.x"
        },
        {
          "pull_number": 5271,
          "pull_title": "Issue #5022 Filter Cache cleanup",
          "pull_version": "eclipse:jetty-9.4.x",
          "pull_version_url": "https://github.com/eclipse/jetty.project/tree/jetty-9.4.x"
        },
        {
          "pull_number": 5091,
          "pull_title": "Issue #5048 - Review temporary buffer usage",
          "pull_version": "eclipse:jetty-9.4.x",
          "pull_version_url": "https://github.com/eclipse/jetty.project/tree/jetty-9.4.x"
        },
        {
          "pull_number": 4556,
          "pull_title": "HttpInput refactoring",
          "pull_version": "eclipse:jetty-10.0.x",
          "pull_version_url": "https://github.com/eclipse/jetty.project/tree/jetty-10.0.x"
        },
        {
          "pull_number": 3677,
          "pull_title": "Issue #3676 CachingWebAppClassLoader enhancement:",
          "pull_version": "eclipse:jetty-9.4.x",
          "pull_version_url": "https://github.com/eclipse/jetty.project/tree/jetty-9.4.x"
        }
      ]
    },
    "github_issues": {
      "issue_datas": [
        {
          "issue_url": "https://github.com/eclipse/jetty.project/issues/5369",
          "issue_title": "Unable to get KeyManagerFactory instance for algorithm [SunX509] on provider [Conscrypt]",
          "issue_number": 5369,
          "issue_text": "**jetty-distribution-9.4.31.v20200723**\\r\\n\\r\\n**openjdk version \"11.0.8\" 2020-07-14 / oracle jdk 11**\\r\\n\\r\\n**ubuntu 20.04**\\r\\n\\r\\n**I've run the steps described on the official jetty documentation :https://www.eclipse.org/jetty/documentation/9.4.31.v20200723/jetty-ssl-distribution.html. but with no success..\\r\\n\\r\\nSteps to recreate:\\r\\n\\r\\n    java -jar start.jar --create-startd --add-to-start=ssl\\r\\n    java -jar start.jar --add-to-start=ssl,conscrypt\\r\\n    java -jar start.jar --add-to-startd=ssl,https\\r\\n\\r\\nUpdate start.d/ssl.ini with the values:\\r\\n\\r\\n--module=ssl\\r\\njetty.ssl.host=0.0.0.0\\r\\njetty.ssl.port=8583\\r\\njetty.sslContext.keyStorePath=etc/keystore\\r\\njetty.sslContext.trustStorePath=etc/keystore\\r\\njetty.sslContext.keyStorePassword=OBF:\\r\\njetty.sslContext.keyManagerPassword=OBF:\\r\\njetty.sslContext.trustStorePassword=OBF:\\r\\n #Enable client certificate authentication.\\r\\njetty.sslContext.needClientAuth=true\\r\\n\\r\\nFor generating the keystore I followed the steps:\\r\\n\\r\\nopenssl genrsa -des3 -out jetty.key\\r\\nopenssl req -new -x509 -key jetty.key -out jetty.crt\\r\\nkeytool -keystore keystore -import -alias jetty -file jetty.crt -trustcacerts\\r\\nopenssl req -new -key jetty.key -out jetty.csr\\r\\nopenssl pkcs12 -inkey jetty.key -in jetty.crt -export -out jetty.pkcs12\\r\\nkeytool -importkeystore -srckeystore jetty.pkcs12 -srcstoretype PKCS12 -destkeystore keystore\\r\\n\\r\\nAs last step I've obfuscated the password and updated it into start.d/ssl.ini file.\\r\\n\\r\\n\\r\\nISSUE\\r\\n\\r\\nI'm facing with an error which say something like\\r\\n\\r\\n2020-09-28 13:51:46.896:INFO::main: Logging initialized @523ms to org.eclipse.jetty.util.log.StdErrLog\\r\\n2020-09-28 13:51:47.387:WARN:oejs.HomeBaseWarning:main: This instance of Jetty is not running from a separate {jetty.base} directory, this is not recommended.  See documentation at http://www.eclipse.org/jetty/documentation/current/startup.html\\r\\n2020-09-28 13:51:47.414:INFO:oejs.Server:main: jetty-9.4.31.v20200723; built: 2020-07-23T17:57:36.812Z; git: 450ba27947e13e66baa8cd1ce7e85a4461cacc1d; jvm 11.0.8+10-LTS\\r\\n2020-09-28 13:51:47.460:INFO:oejdp.ScanningAppProvider:main: Deployment monitor [file:///opt/jetty-distribution-9.4.31.v20200723/webapps/] at interval 1\\r\\n2020-09-28 13:51:47.796:INFO:oejus.SslContextFactory:main: x509=X509@6853425f(jetty,h=[jettyhttp2sample.organicdesign.org],w=[]) for Server@5a9f4771[provider=Conscrypt,keyStore=file:///opt/jetty-distribution-9.4.31.v20200723/etc/keystore,trustStore=file:///opt/jetty-distribution-9.4.31.v20200723/etc/keystore]\\r\\n**2020-09-28 13:51:47.799:INFO:oejus.SslContextFactory:main: Unable to get KeyManagerFactory instance for algorithm [SunX509] on provider [Conscrypt], using default**\\r\\n2020-09-28 13:51:47.882:INFO:oejs.AbstractConnector:main: Started ServerConnector@46c1a1fc{SSL, (ssl, http/1.1)}{localhost:8443}\\r\\n2020-09-28 13:51:47.886:INFO:oejs.AbstractConnector:main: Started ServerConnector@7b205dbd{HTTP/1.1, (http/1.1)}{0.0.0.0:8080}\\r\\n2020-09-28 13:51:47.887:INFO:oejs.Server:main: Started @1514ms\\r\\n\\r\\nUnable to get KeyManagerFactory instance for algorithm [SunX509] on provider [Conscrypt], using default\\r\\n\\r\\n\\r\\n\\r\\nHow should I resolve this kind of issue? \\r\\n\\r\\nThank you**\\r\\n\\r\\n\\r\\n\\r\\n",
          "issue_comments": [
            {
              "comment_username": "joakime",
              "comment_create_time": "2020-09-29T15:47:39Z",
              "comment_edit_time": "2020-09-29T15:47:39Z",
              "comment_text": "The `KeyManagerFactory` is created by using [`javax.net.ssl.KeyManagerFactory.getInstance(String algorithm, String provider)`](https://docs.oracle.com/javase/8/docs/api/javax/net/ssl/KeyManagerFactory.html#getInstance-java.lang.String-java.lang.String-)\r\n\r\nThe `algorithm` is a String populated from using [`javax.net.ssl.KeyManagerFactory.getDefaultAlgorithm()`](https://docs.oracle.com/javase/8/docs/api/javax/net/ssl/KeyManagerFactory.html#getDefaultAlgorithm--), which on your system defaults to `SunX509`.\r\n\r\nThe `provider` comes from the conscrypt libraries you are using, which on your system is `Conscrypt`.\r\n\r\nAn attempt is made to use that algorithm (`SunX509`) against the provider (`Conscrypt`) and conscrypt doesn't like it.\r\nSo the implementation makes note of this and performs a provider-less request using [`javax.net.ssl.KeyManagerFactory.getInstance(String algorithm)`](https://docs.oracle.com/javase/8/docs/api/javax/net/ssl/KeyManagerFactory.html#getInstance-java.lang.String-).\r\n\r\nThe message you are getting is merely an informational message, and not a warning or error.\r\n\r\nThis fallback is valid in many alternate Provider scenarios, where the provider itself doesn't implement the algorithm, but instead relies on the JVM default implementation.\r\n\r\nYou can see this in the code ...\r\n\r\nhttps://github.com/eclipse/jetty.project/blob/450ba27947e13e66baa8cd1ce7e85a4461cacc1d/jetty-util/src/main/java/org/eclipse/jetty/util/ssl/SslContextFactory.java#L1789-L1810"
            }
          ]
        },
        {
          "issue_url": "https://github.com/eclipse/jetty.project/issues/5368",
          "issue_title": "WebSocket text event execute in same thread as running binary event and destroy Threadlocal",
          "issue_number": 5368,
          "issue_text": "**Jetty version**: 9.4.31\r\n\r\n**Java version**: 11.09 but also with 8.0.x\r\n\r\n**OS type/version**: any\r\n\r\n**Description**\r\n\r\nIf the WebSocket MessageHandler is registered as:\r\n\r\n```\r\npublic void onOpen( Session session, EndpointConfig config ) {\r\n  session.addMessageHandler( new MessageHandler.Whole<InputStream>() {\r\n    public void onMessage( InputStream message ) {\r\n    ...\r\n    }\r\n  }\r\n}\r\n```\r\nthen is can occur that inside `message.read()` in the same thread a text message will be processed. In this case two MessageHandler are share the same thread local variables. On security reasons our MessageHandlers destroy all ThreadLocal variables on finish for the current thread. The result for the first binary MessageHandler is that there is no current user scope after some `message.read()`.\r\n\r\nThis has work in version 9.4.20. It is a regression in a micro version or fatal behavior change. We was not able to find a documentation that forbids Threadlocal with WebSocket events handling. We expected that an intermediate event will process in another thread or later.\r\n\r\nA workaround is to switch to `MessageHandler.Whole<byte[]>`. Then there is no `message.read()` call anymore. But it sounds fewer effective.\r\n\r\n",
          "issue_comments": [
            {
              "comment_username": "joakime",
              "comment_create_time": "2020-09-29T11:58:09Z",
              "comment_edit_time": "2020-09-29T11:58:09Z",
              "comment_text": "Your description has not been a supported feature before or now.\r\nThe fact that it worked in 9.4.20 is likely a total accident.\r\n\r\nHow it works ...\r\nA new thread is created for dispatching the initial message for `InputStream`, once you receive the `onMessage(InputStream)` you are in your own thread (a requirement of the javax.websocket spec) until you return from that dispatch/method call.\r\nAll updates to the `InputStream` from the underlying websocket connection is asynchronous (and can/will use multiple threads over the `InputStream` lifespan)\r\nSo, don't spawn a new thread or anything from the `onMessage(InputStream)` to handle that InputStream.\r\n\r\nOther notes about your question/comment ...\r\nAlso, know that if `MessageHandler.Whole<byte[]>` is even an option for you, then you are likely not using the API most effectively.\r\nThe `MessageHandler.Whole<InputStream>` API is best used for very large (not able to fit into memory) messages and long term messages (think voip, steaming, gaming, etc), as that API is very heavy handed with its Thread requirements.\r\nIf you have many messages a minute then you are better off (performance wise, and scalability wise) to use the `MessageHandler.Whole<byte[]>` or `MessageHandler.Whole<ByteBuffer>` APIs instead.\r\nYou can often handle many multiples of more users by switching to whole message delivery vs streaming delivery.\r\n"
            }
          ]
        },
        {
          "issue_url": "https://github.com/eclipse/jetty.project/issues/5367",
          "issue_title": "Reorg of /demos/ with focus on demo-spec downstream dependencies.",
          "issue_number": 5367,
          "issue_text": "Signed-off-by: Joakim Erdfelt <joakim.erdfelt@gmail.com>",
          "issue_comments": [
            {
              "comment_username": "joakime",
              "comment_create_time": "2020-09-28T21:54:09Z",
              "comment_edit_time": "2020-09-28T21:54:09Z",
              "comment_text": "@janbartel do you want these \"demo-spec-*\" artifacts to be used in the osgi testing too?"
            }
          ]
        },
        {
          "issue_url": "https://github.com/eclipse/jetty.project/issues/5365",
          "issue_title": "org.eclipse.jetty.server.Request throws NullPointerException if SessionHandler newHttpSession returns null",
          "issue_number": 5365,
          "issue_text": "**Jetty version**\\r\\nJetty 9.4.32-snapshot as of September 28, 2020\\r\\n\\r\\n**Java version**\\r\\nD:\\\\niagara\\\\r410>java -version\\r\\njava version \"1.8.0_261\"\\r\\nJava(TM) SE Runtime Environment (build 1.8.0_261-b33)\\r\\nJava HotSpot(TM) 64-Bit Server VM (build 25.261-b33, mixed mode)\\r\\n\\r\\n**OS type/version**\\r\\nWindows 10, amd64\\r\\n\\r\\n**Description**\\r\\nI'm not entirely sure if this is intentional behavior, but it _seemed_ wrong, or at least overly painful, so please excuse if this was better suited as a question then a bug. I know the title seems like a 'well, duh' scenario, but this all takes place internally to the Request class where I can't change the behavior without overriding, so bear with me for a moment.\\r\\n\\r\\nIf the SessionHandler newHttpSession() functions returns null, as would be the case if an exception occurred:\\r\\n\\r\\n```\\r\\n    public HttpSession newHttpSession(HttpServletRequest request)\\r\\n    {\\r\\n        try\\r\\n        {\\r\\n           ...\\r\\n        }\\r\\n        catch (Exception e)\\r\\n        {\\r\\n            LOG.warn(e);\\r\\n            return null;\\r\\n        }\\r\\n    }\\r\\n```\\r\\n\\r\\nor could be the case if the class was extended and the session creation failed for some application/subclass specific reason, then the cookie replacement handling that takes place in org.eclipse.jetty.server.Request:\\r\\n\\r\\n```\\r\\n    /*\\r\\n     * @see javax.servlet.http.HttpServletRequest#getSession(boolean)\\r\\n     */\\r\\n    @Override\\r\\n    public HttpSession getSession(boolean create)\\r\\n    {\\r\\n\\r\\n...\\r\\n\\r\\n        _session = _sessionHandler.newHttpSession(this);\\r\\n        HttpCookie cookie = _sessionHandler.getSessionCookie(_session, getContextPath(), isSecure());\\r\\n        if (cookie != null)\\r\\n            _channel.getResponse().replaceCookie(cookie);\\r\\n```\\r\\nwill throw a NullPointerException in the getSessionCookie() function when the session extendedId is determined:\\r\\n\\r\\n```\\r\\n    public HttpCookie getSessionCookie(HttpSession session, String contextPath, boolean requestIsSecure)\\r\\n    {\\r\\n        if (isUsingCookies())\\r\\n        {\\r\\n            String sessionPath = (_cookieConfig.getPath() == null) ? contextPath : _cookieConfig.getPath();\\r\\n            sessionPath = (StringUtil.isEmpty(sessionPath)) ? \"/\" : sessionPath;\\r\\n            String id = getExtendedId(session);\\r\\n\\r\\n...\\r\\n```\\r\\n\\r\\n```\\r\\n    public String getExtendedId(HttpSession session)\\r\\n    {\\r\\n        Session s = ((SessionIf)session).getSession();\\r\\n        return s.getExtendedId();\\r\\n    }\\r\\n```\\r\\n\\r\\nConsidering the amount of null condition protection elsewhere in the Request class, including null checks on the return value of getSession() itself, is it reasonable to think that this NullPointerException could be prevented with some basic assertion in session cookie calculation? \\r\\n\\r\\nConsider the possibility that an IllegalStateException is thrown when creating the new session in AbstractSessionCache:\\r\\n\\r\\n```\\r\\n                throw new IllegalStateException(\"Session \" + id + \" already in cache\");\\r\\n```\\r\\n\\r\\nwhich causes the catch block to activate in the above mentioned newHttpSession() function, which causes the session to return null. Further, getSession then passes this null session to the session cookie function, which throws the NPE exception. This might be easily reproducible if you were to extend the SessionHandler and then always return null for newHttpSession().\\r\\n\\r\\n\\r\\n\\r\\n",
          "issue_comments": [
            {
              "comment_username": "janbartel",
              "comment_create_time": "2020-09-29T10:26:57Z",
              "comment_edit_time": "2020-09-29T10:26:57Z",
              "comment_text": "I agree with you. Ideally I think SessionHandler.newHttpSession(Request) should throw Exception. That's a bit difficult to do in jetty-9.4 as it changes the method signature, but it can be done in jetty-10. For jetty-9.4 maybe I could throw IllegalStateException in Request.getSession(boolean)."
            }
          ]
        },
        {
          "issue_url": "https://github.com/eclipse/jetty.project/issues/5362",
          "issue_title": "Default ProxyServlet cannot proxy to https urls",
          "issue_number": 5362,
          "issue_text": "**Jetty version**\r\n9.4.x\r\n\r\n**Description**\r\nIf you have a default proxy setup like this ...\r\n\r\n**WEB-INF/web.xml**\r\n\r\n``` xml\r\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\r\n<web-app xmlns=\"http://java.sun.com/xml/ns/javaee\" \r\nxmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" \r\nxsi:schemaLocation=\"http://java.sun.com/xml/ns/javaee http://java.sun.com/xml/ns/javaee/web-app_3_0.xsd\" \r\nmetadata-complete=\"false\" \r\nversion=\"3.0\">\r\n\r\n  <display-name>Transparent Proxy WebApp</display-name>\r\n\r\n  <servlet>\r\n    <servlet-name>JavadocTransparentProxy</servlet-name>\r\n    <servlet-class>org.eclipse.jetty.proxy.ProxyServlet$Transparent</servlet-class>\r\n    <init-param>\r\n      <param-name>proxyTo</param-name>\r\n      <param-value>https://www.eclipse.org/jetty/javadoc/</param-value>\r\n    </init-param>\r\n    <init-param>\r\n      <param-name>hostHeader</param-name>\r\n      <param-value>www.eclipse.org</param-value>\r\n    </init-param>\r\n    <load-on-startup>1</load-on-startup>\r\n    <async-supported>true</async-supported>\r\n  </servlet>\r\n\r\n  <servlet-mapping>\r\n    <servlet-name>JavadocTransparentProxy</servlet-name>\r\n    <url-pattern>/current/*</url-pattern>\r\n  </servlet-mapping>\r\n\r\n</web-app>\r\n```\r\n\r\nThen the ProxyServlet will fail as the internal HttpClient has no support for SSL/TLS enabled.\r\n\r\n\r\n\r\n",
          "issue_comments": [
            {
              "comment_username": "joakime",
              "comment_create_time": "2020-09-28T16:55:16Z",
              "comment_edit_time": "2020-09-28T16:55:16Z",
              "comment_text": "Talked with @sbordet and we'll just enable a default `SslContextFactory.Client` within `AbstractProxyServlet.newHttpClient()`\r\n\r\nBut do we want to introduce a `newSslContextClient()` method?"
            }
          ]
        },
        {
          "issue_url": "https://github.com/eclipse/jetty.project/issues/5361",
          "issue_title": "Some elements do not have an origin attribute in quickstart-web.xml",
          "issue_number": 5361,
          "issue_text": "**Jetty-9.4**\r\n\r\no.e.j.annotations.WebListenerAnnotation class does not set the metadata origin. This means that the `origin` attribute will be skipped in quickstart-web.xml.\r\n\r\no.e.j.annotations.ServletAnnotation does not set the metadata origin for servlet-mappings in the annotation, thus the `origin` attribute will be missing in quickstart-web.xml.\r\n\r\no.e.j.annotations.FilterAnnotation does not set the metadata origin for filter-mappings in the annotation, thus the `origin` attribute will be missing in quickstart-web.xml.\r\n\r\n",
          "issue_comments": [
            {
              "comment_username": "joakime",
              "comment_create_time": "2020-09-28T17:54:00Z",
              "comment_edit_time": "2020-09-28T17:54:00Z",
              "comment_text": "@janbartel are you fixing this for the 9.4.32 release?"
            },
            {
              "comment_username": "janbartel",
              "comment_create_time": "2020-09-29T09:21:36Z",
              "comment_edit_time": "2020-09-29T09:21:36Z",
              "comment_text": "@joakime no, it's not critical for 9.4.32."
            },
            {
              "comment_username": "janbartel",
              "comment_create_time": "2020-09-29T09:25:00Z",
              "comment_edit_time": "2020-09-29T09:43:24Z",
              "comment_text": "Some other things I'm recording here:\r\n\r\n* Quickstart generator doesn't generate `origin` for filter-mappings at all\r\n* Quickstart generates an `origin` only for the top-level of servlet-mappings, although technically each mapping could come from a different descriptor\r\n* Quickstart doesn't record the origin of context-params\r\n* `origin` is missing from welcome-file-list\r\n* `origin` is missing from  security-role\r\n* `origin` missing from env-entry\r\n* `origin` missing from session-config"
            }
          ]
        },
        {
          "issue_url": "https://github.com/eclipse/jetty.project/issues/5359",
          "issue_title": "jetty-documentation JXURL should point to github tagged contents, not maven XREF",
          "issue_number": 5359,
          "issue_text": "**Jetty version**\r\n9.4.x\r\n\r\n**Description**\r\nCurrently the jetty-documentation has references to maven xref results like this ...\r\n\r\n``` xml\r\n<JDURL>https://eclipse.org/jetty/javadoc/${javadoc.version}</JDURL>\r\n<JXURL>http://download.eclipse.org/jetty/stable-9/xref</JXURL>\r\n```\r\n\r\nit should probably be like ...\r\n\r\n``` xml\r\n<JDURL>https://eclipse.org/jetty/javadoc/${javadoc.version}</JDURL>\r\n<JXURL>https://github.com/eclipse/jetty.project/tree/${jetty.version}</JXURL>\r\n```\r\n",
          "issue_comments": []
        },
        {
          "issue_url": "https://github.com/eclipse/jetty.project/issues/5357",
          "issue_title": "Update http://eclipse.org to https://eclipse.org in source",
          "issue_number": 5357,
          "issue_text": "**Jetty version**\r\n9.4.x\r\n\r\n**Description**\r\nWe have many references in source to the older `http://eclipse.org/...` URLs.\r\n\r\nThose are no longer valid, and will result in a redirect to the secure version `https://eclipse.org/...`\r\n\r\n\r\n\r\n",
          "issue_comments": []
        },
        {
          "issue_url": "https://github.com/eclipse/jetty.project/issues/5355",
          "issue_title": "org.springframework.web.multipart.MultipartException: Failed to parse multipart servlet request; nested exception is java.lang.OutOfMemoryError: Java heap space",
          "issue_number": 5355,
          "issue_text": "Jetty version: 9.4.27\r\nJava version: 1.8\r\nJvm options: -Xms2048m -Xmx2048m\r\nDescription: This exception is thrown when uploading a large file： \r\n         org.springframework.web.multipart.MultipartException: Failed to parse multipart servlet request; nested exception is java.lang.OutOfMemoryError: Java heap space\r\n\r\n\r\n\r\n",
          "issue_comments": [
            {
              "comment_username": "joakime",
              "comment_create_time": "2020-09-28T09:18:05Z",
              "comment_edit_time": "2020-09-28T09:18:05Z",
              "comment_text": "Not enough information to work with.\r\n\r\nWhat was your stacktrace?\r\nWhat was your Multipart configuration?\r\nWhat did the data look like?"
            }
          ]
        },
        {
          "issue_url": "https://github.com/eclipse/jetty.project/issues/5354",
          "issue_title": "h2spec tests fail to run",
          "issue_number": 5354,
          "issue_text": "**Jetty version**\r\n9.4.x\r\n\r\n**Description**\r\nInstalled `docker` as specified [here](https://docs.docker.com/engine/install/) - in particular, for Ubuntu, *not* `apt install docker`, but as documented `apt-get install docker-ce docker-ce-cli containerd.io`.\r\n\r\nTried to run the h2spec tests yields:\r\n\r\n```\r\n[INFO] docker-machine executable was not found on PATH ([/home/simon/programs/jdk15/bin, /home/simon/.cargo/bin, /home/simon/bin, /usr/local/sbin, /usr/local/bin, /usr/sbin, /usr/bin, /sbin, /bin, /usr/games, /usr/local/games, /snap/bin])\r\n```\r\n\r\nTrying branch `jetty-9.4.x_skip_h2spec_if_no_docker` yields:\r\n\r\n```\r\n[INFO] --- h2spec-maven-plugin:1.0.1-SNAPSHOT:h2spec (h2spec) @ http2-server ---\r\n[INFO] docker-machine executable was not found on PATH ([/home/simon/programs/jdk15/bin, /home/simon/.cargo/bin, /home/simon/bin, /usr/local/sbin, /usr/local/bin, /usr/sbin, /usr/bin, /sbin, /bin, /usr/games, /usr/local/games, /snap/bin])\r\n[INFO] Found Docker environment with local Unix socket (unix:///var/run/docker.sock)\r\n[INFO] Docker host IP address is localhost\r\n[INFO] Connected to docker: \r\n  Server Version: 19.03.13\r\n  API Version: 1.40\r\n  Operating System: Ubuntu 20.04.1 LTS\r\n  Total Memory: 31916 MB\r\n[WARNING] Failure when attempting to lookup auth config (dockerImageName: testcontainers/ryuk:0.3.0, configFile: /home/simon/.docker/config.json. Falling back to docker-java default behaviour. Exception message: /home/simon/.docker/config.json (No such file or directory)\r\n[INFO] Starting to pull image\r\n[INFO] Pulling image layers:  0 pending,  0 downloaded,  0 extracted, (0 bytes/0 bytes)\r\n[INFO] Pulling image layers:  2 pending,  1 downloaded,  0 extracted, (2 MB/? MB)\r\n[INFO] Pulling image layers:  1 pending,  2 downloaded,  0 extracted, (4 MB/? MB)\r\n[INFO] Pulling image layers:  1 pending,  2 downloaded,  1 extracted, (4 MB/? MB)\r\n[INFO] Pulling image layers:  1 pending,  2 downloaded,  2 extracted, (4 MB/? MB)\r\n[INFO] Pulling image layers:  1 pending,  2 downloaded,  3 extracted, (5 MB/? MB)\r\n[INFO] Ryuk started - will monitor and terminate Testcontainers containers on JVM exit\r\n[INFO] Checking the system...\r\n[INFO] ✔︎ Docker server version should be at least 1.6.0\r\n[INFO] ✔︎ Docker environment should have more than 2GB free disk space\r\n2020-09-28 09:52:48.543:INFO::Thread-4: Logging initialized @27368ms to org.eclipse.jetty.util.log.StdErrLog\r\n2020-09-28 09:52:48.574:INFO:oejs.Server:Thread-4: jetty-9.4.32-SNAPSHOT; built: 2020-09-28T07:45:31.737Z; git: a5b887d13138af825593b2e6091ba111cb5f5d5c; jvm 15+36-1562\r\n2020-09-28 09:52:48.589:INFO:oejs.AbstractConnector:Thread-4: Started ServerConnector@42fd2f8d{HTTP/1.1, (http/1.1, h2c)}{0.0.0.0:39383}\r\n2020-09-28 09:52:48.589:INFO:oejs.Server:Thread-4: Started @27414ms\r\n[INFO] !!! Exclude specs\r\n[INFO] 3.5 - Sends invalid connection preface\r\n[INFO] running image: summerwind/h2spec:2.6.0 with command: -h host.testcontainers.internal -p 39383 -j /tmp/junit.xml -o 2 --max-header-length 4000\r\n[INFO] Pulling docker image: testcontainers/sshd:1.0.0. Please be patient; this may take some time but only needs to be done once.\r\n[INFO] Starting to pull image\r\n[INFO] Pulling image layers:  0 pending,  0 downloaded,  0 extracted, (0 bytes/0 bytes)\r\n[INFO] Pulling image layers:  1 pending,  1 downloaded,  0 extracted, (20 KB/? MB)\r\n[INFO] Pulling image layers:  1 pending,  1 downloaded,  1 extracted, (1 MB/? MB)\r\n[INFO] Pulling image layers:  0 pending,  2 downloaded,  1 extracted, (2 MB/3 MB)\r\n[INFO] Pulling image layers:  0 pending,  2 downloaded,  2 extracted, (3 MB/3 MB)\r\n[INFO] Pull complete. 2 layers, pulled in 1s (downloaded 3 MB at 3 MB/s)\r\n[INFO] Creating container for image: testcontainers/sshd:1.0.0\r\n[INFO] Starting container with ID: e50f4893b199012f2a4fd26e9a5c9c14b85799ea40589e3d0973f0efc11b4b33\r\n[INFO] Container testcontainers/sshd:1.0.0 is starting: e50f4893b199012f2a4fd26e9a5c9c14b85799ea40589e3d0973f0efc11b4b33\r\n[INFO] Container testcontainers/sshd:1.0.0 started in PT3.74023695S\r\n[INFO] Pulling docker image: summerwind/h2spec:2.6.0. Please be patient; this may take some time but only needs to be done once.\r\n[INFO] Starting to pull image\r\n[INFO] Pulling image layers:  0 pending,  0 downloaded,  0 extracted, (0 bytes/0 bytes)\r\n[INFO] Pulling image layers:  4 pending,  1 downloaded,  0 extracted, (423 bytes/? MB)\r\n[INFO] Pulling image layers:  3 pending,  2 downloaded,  0 extracted, (273 KB/? MB)\r\n[INFO] Pulling image layers:  2 pending,  3 downloaded,  0 extracted, (23 MB/? MB)\r\n[INFO] Pulling image layers:  1 pending,  4 downloaded,  0 extracted, (23 MB/? MB)\r\n[INFO] Pulling image layers:  1 pending,  4 downloaded,  1 extracted, (25 MB/? MB)\r\n[INFO] Pulling image layers:  1 pending,  4 downloaded,  2 extracted, (25 MB/? MB)\r\n[INFO] Pulling image layers:  1 pending,  4 downloaded,  3 extracted, (25 MB/? MB)\r\n[INFO] Pulling image layers:  1 pending,  4 downloaded,  4 extracted, (25 MB/? MB)\r\n[INFO] Pulling image layers:  0 pending,  5 downloaded,  4 extracted, (28 MB/30 MB)\r\n[INFO] Pulling image layers:  0 pending,  5 downloaded,  5 extracted, (30 MB/30 MB)\r\n[INFO] Pull complete. 5 layers, pulled in 3s (downloaded 30 MB at 10 MB/s)\r\n[INFO] Creating container for image: summerwind/h2spec:2.6.0\r\n[INFO] Starting container with ID: 370282c1c029399bf80cb45cfdd963aa897f6dd31113778513efcf5380687a90\r\n[INFO] Container summerwind/h2spec:2.6.0 is starting: 370282c1c029399bf80cb45cfdd963aa897f6dd31113778513efcf5380687a90\r\n[INFO] Container summerwind/h2spec:2.6.0 started in PT6.160675663S\r\n[INFO] All test cases passed. 1 test cases ignored.\r\n```\r\n\r\nBut the h2spec tests are not run, or at least there is no evidence that they are actually run.\r\n\r\n",
          "issue_comments": []
        },
        {
          "issue_url": "https://github.com/eclipse/jetty.project/issues/5349",
          "issue_title": "Bump asm.version from 8.0.1 to 9.0",
          "issue_number": 5349,
          "issue_text": "Bumps `asm.version` from 8.0.1 to 9.0.\nUpdates `asm-commons` from 8.0.1 to 9.0\n\nUpdates `asm` from 8.0.1 to 9.0\n\nUpdates `asm-tree` from 8.0.1 to 9.0\n\nUpdates `asm-analysis` from 8.0.1 to 9.0\n\nUpdates `asm-util` from 8.0.1 to 9.0\n\n\nDependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.\n\n[//]: # (dependabot-automerge-start)\n[//]: # (dependabot-automerge-end)\n\n---\n\n<details>\n<summary>Dependabot commands and options</summary>\n<br />\n\nYou can trigger Dependabot actions by commenting on this PR:\n- `@dependabot rebase` will rebase this PR\n- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it\n- `@dependabot merge` will merge this PR after your CI passes on it\n- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it\n- `@dependabot cancel merge` will cancel a previously requested merge and block automerging\n- `@dependabot reopen` will reopen this PR if it is closed\n- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually\n- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)\n\n\n</details>",
          "issue_comments": [
            {
              "comment_username": "joakime",
              "comment_create_time": "2020-09-28T09:46:12Z",
              "comment_edit_time": "2020-09-28T09:46:35Z",
              "comment_text": "@janbartel do we want this from Jetty 9.4.x onwards?\r\nOr from Jetty 10 onwards (like this PR shows?)\r\n\r\nI realize we are still waiting on the CQ/PR for this."
            }
          ]
        },
        {
          "issue_url": "https://github.com/eclipse/jetty.project/issues/5337",
          "issue_title": "Bump plexus-compiler-javac-errorprone from 2.8.2 to 2.8.8",
          "issue_number": 5337,
          "issue_text": "Bumps plexus-compiler-javac-errorprone from 2.8.2 to 2.8.8.\n\n\n[![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=org.codehaus.plexus:plexus-compiler-javac-errorprone&package-manager=maven&previous-version=2.8.2&new-version=2.8.8)](https://docs.github.com/en/github/managing-security-vulnerabilities/configuring-github-dependabot-security-updates)\n\nDependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.\n\n[//]: # (dependabot-automerge-start)\n[//]: # (dependabot-automerge-end)\n\n---\n\n<details>\n<summary>Dependabot commands and options</summary>\n<br />\n\nYou can trigger Dependabot actions by commenting on this PR:\n- `@dependabot rebase` will rebase this PR\n- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it\n- `@dependabot merge` will merge this PR after your CI passes on it\n- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it\n- `@dependabot cancel merge` will cancel a previously requested merge and block automerging\n- `@dependabot reopen` will reopen this PR if it is closed\n- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually\n- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)\n\n\n</details>",
          "issue_comments": []
        },
        {
          "issue_url": "https://github.com/eclipse/jetty.project/issues/5332",
          "issue_title": "Bump kerb-simplekdc from 1.1.1 to 2.0.1",
          "issue_number": 5332,
          "issue_text": "Bumps kerb-simplekdc from 1.1.1 to 2.0.1.\n\n\n[![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=org.apache.kerby:kerb-simplekdc&package-manager=maven&previous-version=1.1.1&new-version=2.0.1)](https://docs.github.com/en/github/managing-security-vulnerabilities/configuring-github-dependabot-security-updates)\n\nDependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.\n\n[//]: # (dependabot-automerge-start)\n[//]: # (dependabot-automerge-end)\n\n---\n\n<details>\n<summary>Dependabot commands and options</summary>\n<br />\n\nYou can trigger Dependabot actions by commenting on this PR:\n- `@dependabot rebase` will rebase this PR\n- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it\n- `@dependabot merge` will merge this PR after your CI passes on it\n- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it\n- `@dependabot cancel merge` will cancel a previously requested merge and block automerging\n- `@dependabot reopen` will reopen this PR if it is closed\n- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually\n- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)\n\n\n</details>",
          "issue_comments": [
            {
              "comment_username": "joakime",
              "comment_create_time": "2020-09-27T23:08:01Z",
              "comment_edit_time": "2020-09-27T23:08:01Z",
              "comment_text": "@sbordet this is a jetty-client dependency update."
            }
          ]
        },
        {
          "issue_url": "https://github.com/eclipse/jetty.project/issues/5330",
          "issue_title": "Bump openwebbeans-web from 2.0.11 to 2.0.18",
          "issue_number": 5330,
          "issue_text": "Bumps [openwebbeans-web](https://github.com/apache/openwebbeans) from 2.0.11 to 2.0.18.\\n<details>\\n<summary>Commits</summary>\\n<ul>\\n<li><a href=\"https://github.com/apache/openwebbeans/commit/2a97bc8250c4ed0d7cf36399c495c8898dbec216\"><code>2a97bc8</code></a> [maven-release-plugin] prepare release openwebbeans-2.0.18</li>\\n<li><a href=\"https://github.com/apache/openwebbeans/commit/c9f08c2405cace06d471d1a71c0b1aff5c1f1eb7\"><code>c9f08c2</code></a> add release notes for 2.0.18</li>\\n<li><a href=\"https://github.com/apache/openwebbeans/commit/311a8692718a24aaabfe11e1250cafb5d122b18d\"><code>311a869</code></a> OWB-1347 upgrade to apache-23</li>\\n<li><a href=\"https://github.com/apache/openwebbeans/commit/8b8c82505a2336bec91e89e2dacb09b2dfd8263b\"><code>8b8c825</code></a> OWB-1281 handle UnsatisfiedLinkError</li>\\n<li><a href=\"https://github.com/apache/openwebbeans/commit/488f058292f5c3d5135865dc2ad1e726a21a67b5\"><code>488f058</code></a> Merge pull request <a href=\"https://github-redirect.dependabot.com/apache/openwebbeans/issues/32\">#32</a> from a-rekkusu/master</li>\\n<li><a href=\"https://github.com/apache/openwebbeans/commit/9d628beba7afbb9598a7ed91d773c55fdbc42871\"><code>9d628be</code></a> OWB-1346 added Apache Header</li>\\n<li><a href=\"https://github.com/apache/openwebbeans/commit/2b84bf36f2fc228bff7b026570001a93e8682dc7\"><code>2b84bf3</code></a> Merge pull request <a href=\"https://github-redirect.dependabot.com/apache/openwebbeans/issues/31\">#31</a> from a-rekkusu/master</li>\\n<li><a href=\"https://github.com/apache/openwebbeans/commit/09e75a2247942d08fb113da0fe0ef0fb388f5c29\"><code>09e75a2</code></a> OWB-1346 added Vetoed test</li>\\n<li><a href=\"https://github.com/apache/openwebbeans/commit/9bf00969ad87f1c327bef558c1deb2cb11378920\"><code>9bf0096</code></a> OWB-1346 prevent scanning of generated proxies</li>\\n<li><a href=\"https://github.com/apache/openwebbeans/commit/b3d0f171c8e7b71a6b76411874f43e2d52756e23\"><code>b3d0f17</code></a> Merge pull request <a href=\"https://github-redirect.dependabot.com/apache/openwebbeans/issues/2\">#2</a> from apache/master</li>\\n<li>Additional commits viewable in <a href=\"https://github.com/apache/openwebbeans/compare/openwebbeans-2.0.11...openwebbeans-2.0.18\">compare view</a></li>\\n</ul>\\n</details>\\n<br />\\n\\n\\n[![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=org.apache.openwebbeans:openwebbeans-web&package-manager=maven&previous-version=2.0.11&new-version=2.0.18)](https://docs.github.com/en/github/managing-security-vulnerabilities/configuring-github-dependabot-security-updates)\\n\\nDependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.\\n\\n[//]: # (dependabot-automerge-start)\\n[//]: # (dependabot-automerge-end)\\n\\n---\\n\\n<details>\\n<summary>Dependabot commands and options</summary>\\n<br />\\n\\nYou can trigger Dependabot actions by commenting on this PR:\\n- `@dependabot rebase` will rebase this PR\\n- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it\\n- `@dependabot merge` will merge this PR after your CI passes on it\\n- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it\\n- `@dependabot cancel merge` will cancel a previously requested merge and block automerging\\n- `@dependabot reopen` will reopen this PR if it is closed\\n- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually\\n- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)\\n- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)\\n- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)\\n\\n\\n</details>",
          "issue_comments": [
            {
              "comment_username": "joakime",
              "comment_create_time": "2020-09-27T23:07:20Z",
              "comment_edit_time": "2020-09-27T23:07:20Z",
              "comment_text": "If this PR build is green, I think we should merge it"
            }
          ]
        },
        {
          "issue_url": "https://github.com/eclipse/jetty.project/issues/5326",
          "issue_title": "Illegal reflective access because of partial implementation of ProxyServices?",
          "issue_number": 5326,
          "issue_text": "**Jetty version**\r\n9.4.32-SNAPSHOT\r\n\r\n**Java version**\r\nOpenJDK 11\r\n\r\n**OS type/version**\r\nDebian stable\r\n\r\n**Description**\r\nFollowing the new instructions for setting up CDI with Weld leads to an “illegal reflective access” (according to the log). Looking up the Weld source code, it mentions that the reason could be that “the integrator does not fully implement ProxyServices”. Is this something that jetty can solve, or due to me not having properly understood the instructions for setting up Weld + Jetty?\r\n\r\n**Details**\r\nI built the new version of the doc from source. I used the jetty-snapshot available in maven, contrary to instructions, I hope that is okay.\r\n\r\nThe log is the following when my application starts.\r\n\r\n```\r\n19:28:25.524 [main] INFO  org.eclipse.jetty.util.log - Logging initialized @713ms to org.eclipse.jetty.util.log.Slf4jLog\r\n19:28:25.724 [main] INFO  io.github.oliviercailloux.jetty.App - Set handler: HandlerList@11a9e7c8{STOPPED}.\r\n19:28:25.736 [main] INFO  org.eclipse.jetty.server.Server - jetty-9.4.32-SNAPSHOT; built: 2020-09-18T11:03:24.291Z; git: e3ed05fc1c1dca35add2ff5da29f910c693e23e2; jvm 11.0.8+10-post-Debian-1deb10u1\r\n19:28:25.890 [main] INFO  o.j.w.environment.servletWeldServlet - WELD-ENV-001008: Initialize Weld using ServletContainerInitializer\r\n19:28:25.927 [main] INFO  org.jboss.weld.Version - WELD-000900: 3.1.5 (SP1)\r\n19:28:26.208 [main] INFO  org.jboss.weld.Bootstrap - WELD-ENV-000020: Using jandex for bean discovery\r\nWARNING: An illegal reflective access operation has occurred\r\nWARNING: Illegal reflective access by org.jboss.weld.util.bytecode.ClassFileUtils$1 (file:/home/olivier/.m2/repository/org/jboss/weld/weld-core-impl/3.1.5.SP1/weld-core-impl-3.1.5.SP1.jar) to method java.lang.ClassLoader.defineClass(java.lang.String,byte[],int,int)\r\nWARNING: Please consider reporting this to the maintainers of org.jboss.weld.util.bytecode.ClassFileUtils$1\r\nWARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\r\nWARNING: All illegal access operations will be denied in a future release\r\n19:28:26.497 [main] INFO  org.jboss.weld.Bootstrap - WELD-000101: Transactional services not available. Injection of @Inject UserTransaction not available. Transactional observers will be invoked synchronously.\r\n19:28:26.814 [main] INFO  o.j.weld.environment.servletJetty - WELD-ENV-001200: Jetty 7.2+ detected, CDI injection will be available in Servlets and Filters. Injection into Listeners should work on Jetty 9.1.1 and newer.\r\n19:28:27.152 [main] INFO  o.e.j.c.CdiServletContainerInitializer - CdiDecoratingListener enabled in ServletContext@o.e.j.s.ServletContextHandler@1dd0e7c4{/api,null,STARTING}\r\n19:28:27.162 [main] INFO  o.e.j.server.handler.ContextHandler - Started o.e.j.s.ServletContextHandler@1dd0e7c4{/api,null,AVAILABLE}\r\n19:28:27.185 [main] INFO  o.e.jetty.server.AbstractConnector - Started ServerConnector@4461c7e3{HTTP/1.1, (http/1.1)}{0.0.0.0:8080}\r\n19:28:27.185 [main] INFO  org.eclipse.jetty.server.Server - Started @2381ms\r\n```\r\n\r\nThe full application is [here](https://github.com/oliviercailloux/sample-jetty/tree/injection), with the main content [here](https://github.com/oliviercailloux/sample-jetty/blob/injection/src/main/java/io/github/oliviercailloux/jetty/App.java).\r\n\r\nThe illegal reflective access happens in [this class](https://github.com/weld/core/blob/master/impl/src/main/java/org/jboss/weld/util/bytecode/ClassFileUtils.java), where the quote above about ProxyServices is taken from. \r\n\r\n(I posted this [here](https://github.com/eclipse/jetty.project/issues/5162#issuecomment-695338000) already, but I figured out that this probably deserves a new issue, I hope this is adequate.)\r\n\r\n**Possibly related questions**\r\n\r\n1. The log is not exactly as documented, letting me wonder whether I am unwillingly using some deprecated access instead of the recommended one. I get “Jetty 7.2+ detected, CDI injection will be available in Servlets and Filters. Injection into Listeners should work on Jetty 9.1.1 and newer.” whereas according to the doc, I should get “INFO: WELD-ENV-001212: Jetty CdiDecoratingListener support detected, CDI injection will be available in Listeners, Servlets and Filters.”\r\n2. I use `org.jboss.weld.servlet:weld-servlet-core:3.1.5.SP1` as Weld dependency, whereas the documentation mentions `org.jboss.weld.servlet:weld-servlet` (which seems [unmaintained](https://search.maven.org/search?q=org.jboss.weld.servlet) since 2014), is this the recommended dependency to use with jetty embedded? \r\n3. I use the `jetty-cdi` Maven dependency, and not `cdi-decorate` as documented (which I can’t find in [Maven central](https://search.maven.org/search?q=org.eclipse.jetty%20cdi)), is this all right?\r\n",
          "issue_comments": [
            {
              "comment_username": "joakime",
              "comment_create_time": "2020-09-25T18:18:32Z",
              "comment_edit_time": "2020-09-25T18:19:52Z",
              "comment_text": "Our CDI integration is vendor neutral ATM.\r\n\r\nIt supports both Weld and OpenWebBeans.\r\nLooks like `org.jboss.weld.serialization.spi.ProxyServices` would be weld specific.\r\n\r\nIf we implemented it, then wouldn't that mean CDI is no longer a Server level feature?\r\n\r\nAs an implementation of `ProxyServices` expects to be registered at each individual `WebAppClassLoader`, which the server level one is not responsible for, right?\r\n\r\nSince weld Services register at the `WeldStartup` time that means the `ProxyServices` implementation would be loaded far too soon to have any impact on the `WebAppClassLoader` (where the `ProxyServices` would need to be).   We would need to delay ServicLoader lookup of the `ProxyServices` for when the specific `WebAppClassLoader` is started.  ugh.\r\n\r\n\r\n\r\n"
            },
            {
              "comment_username": "oliviercailloux",
              "comment_create_time": "2020-09-26T11:30:49Z",
              "comment_edit_time": "2020-09-26T11:30:49Z",
              "comment_text": "I understand the concern that Jetty should be vendor neutral.\r\n\r\nCould you confirm that my sample code implemented Jetty CDI integration in the recommended way (i.e. that I made no mistake in interpreting the documentation), considering my three “related questions” in particular?\r\n"
            },
            {
              "comment_username": "gregw",
              "comment_create_time": "2020-09-26T14:57:03Z",
              "comment_edit_time": "2020-09-26T14:57:03Z",
              "comment_text": "@oliviercailloux there are actually a lot of different ways to integrate weld into Jetty! See: https://github.com/eclipse/jetty.project/blob/jetty-9.4.x/jetty-cdi/src/test/java/org/eclipse/jetty/embedded/EmbeddedWeldTest.java\r\n\r\nVery hard to say what is the preferred.  Personally like the `CdiServletContainerInitializer+EnhancedListener` as that is using the SPI, which is meant to be the way to integrate any CDI implementation.  \r\nHowever the Weld folks say that somethings might not work that way.... not sure what they are.\r\n\r\nSo 2nd best is probably  `CdiServletContainerInitializer(CdiDecoratingListener)+EnhancedListener`\r\n\r\nI think you code looks close... except I think you have the order wrong: ie add the `EnhancedListener` after the `CdiServletContainerInitializer`.  So your code is trying to init CDI before Jetty has initialised it's CDI support.\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n"
            },
            {
              "comment_username": "gregw",
              "comment_create_time": "2020-09-26T14:58:21Z",
              "comment_edit_time": "2020-09-26T14:58:21Z",
              "comment_text": "oh yes our documentation does need to catch up a bit with some recent changes to how this is done embedded. "
            }
          ]
        },
        {
          "issue_url": "https://github.com/eclipse/jetty.project/issues/5320",
          "issue_title": "Using WebSocketClient with jetty-websocket-httpclient.xml in a Jetty web application causes ClassCastException",
          "issue_number": 5320,
          "issue_text": "**Jetty version**\\r\\n9.4.31\\r\\n**Java version**\\r\\nOpenJDK 1.8.0_251\\r\\n**OS type/version**\\r\\nWindows 10 64 bit\\r\\n**Description**\\r\\nTypically, using a JSR-356 javax.websocket.ClientEndpoint annotated websocket client with the javax.websocket.ContainerProvider$WebSocketContainer works fine within a Jetty web application. \\r\\n\\r\\nHowever, when you add the jetty-websocket-httpclient.xml file to perform any configuration (e.g. for the ssl context factory) you will see an exception similar to below.\\r\\n\\r\\n```\\r\\njava.lang.ClassCastException: org.eclipse.jetty.client.HttpClient cannot be cast to org.eclipse.jetty.client.HttpClient\\r\\n    at org.eclipse.jetty.websocket.client.XmlBasedHttpClientProvider.get (XmlBasedHttpClientProvider.java:41)\\r\\n    at sun.reflect.NativeMethodAccessorImpl.invoke0 (Native Method)\\r\\n    at sun.reflect.NativeMethodAccessorImpl.invoke (NativeMethodAccessorImpl.java:62)\\r\\n    at sun.reflect.DelegatingMethodAccessorImpl.invoke (DelegatingMethodAccessorImpl.java:43)\\r\\n    at java.lang.reflect.Method.invoke (Method.java:498)\\r\\n    at org.eclipse.jetty.websocket.client.HttpClientProvider.get (HttpClientProvider.java:37)\\r\\n    at org.eclipse.jetty.websocket.client.WebSocketClient.<init> (WebSocketClient.java:264)\\r\\n    at org.eclipse.jetty.websocket.jsr356.ClientContainer.<init> (ClientContainer.java:158)\\r\\n    at org.eclipse.jetty.websocket.jsr356.server.ServerContainer.<init> (ServerContainer.java:94)\\r\\n    at org.eclipse.jetty.websocket.jsr356.server.deploy.WebSocketServerContainerInitializer.initialize (WebSocketServerContainerInitializer.java:210)\\r\\n    at org.eclipse.jetty.websocket.jsr356.server.deploy.WebSocketServerContainerInitializer.onStartup (WebSocketServerContainerInitializer.java:277)\\r\\n    at org.eclipse.jetty.plus.annotation.ContainerInitializer.callStartup (ContainerInitializer.java:145)\\r\\n    at org.eclipse.jetty.annotations.ServletContainerInitializersStarter.doStart (ServletContainerInitializersStarter.java:64)\\r\\n    at org.eclipse.jetty.util.component.AbstractLifeCycle.start (AbstractLifeCycle.java:72)\\r\\n...\\r\\n```\\r\\nThis can easily be reproduced by creating a new web-app that contains only a basic jetty.xml, web.xml and jetty-websocket-httpclient.xml (and pom.xml assuming you are using maven). The above ClassCastException will be thrown shortly before the \"Started Jetty Server\" message.\\r\\n\\r\\nOn a possibly related note, if you instead use the Jetty implemented websocket (org.eclipse.jetty.websocket.client.WebSocketClient) and use the constructor that takes an HttpClient you receive a LinkageError similar to this:\\r\\n```\\r\\njava.lang.LinkageError: loader constraint violation: when resolving method \"org.eclipse.jetty.websocket.client.WebSocketClient.<init>(Lorg/eclipse/jetty/client/HttpClient;)V\" the class loader (instance of org/eclipse/jetty/webapp/WebAppClassLoader) of the current class, com/example/WebSocketClientWrapper, and the class loader (instance of org/codehaus/plexus/classworlds/realm/ClassRealm) for the method's defining class, org/eclipse/jetty/websocket/client/WebSocketClient, have different Class objects for the type org/eclipse/jetty/client/HttpClient used in the signature\\r\\n\\tat com.example.WebSocketClientWrapper.<init>(WebSocketClientWrapper.java:23)\\r\\n...\\r\\n```\\r\\n\\r\\n\\r\\n",
          "issue_comments": [
            {
              "comment_username": "joakime",
              "comment_create_time": "2020-09-23T21:09:21Z",
              "comment_edit_time": "2020-09-23T21:09:21Z",
              "comment_text": "You have `org.eclipse.jetty.client.HttpClient` in your `WEB-INF/lib/` too. \r\nRemove it from there."
            },
            {
              "comment_username": "michael-nilson-yardi",
              "comment_create_time": "2020-09-23T22:12:54Z",
              "comment_edit_time": "2020-09-23T22:12:54Z",
              "comment_text": "I have removed the jetty-client.jar which contains HttpClient from my WEB-INF/lib/ and now am getting a `java.lang.ClassNotFoundException: org.eclipse.jetty.client.HttpClient`. I'm assuming I need the following modules in my start.ini so I have added them `server,client,webapp,websocket`\r\n"
            },
            {
              "comment_username": "joakime",
              "comment_create_time": "2020-09-24T00:00:35Z",
              "comment_edit_time": "2020-09-24T00:00:35Z",
              "comment_text": "@lachlan-roberts can you take a look at this issue?"
            },
            {
              "comment_username": "michael-nilson-yardi",
              "comment_create_time": "2020-09-25T13:47:08Z",
              "comment_edit_time": "2020-09-25T13:47:08Z",
              "comment_text": "We stumbled across [this article](https://www.eclipse.org/lists/jetty-dev/msg03096.html) yesterday and it seemed very similar to our circumstances when encountering the LinkageError. We gave it a try both with the approach that worked for the OP as well as the alternate approach you had provided Joakime but we still had the same result; both when programatically manipulating the SslContextFactory and when relying on the jetty-websocket-httpclient.xml approach."
            },
            {
              "comment_username": "joakime",
              "comment_create_time": "2020-09-25T15:08:03Z",
              "comment_edit_time": "2020-09-25T15:08:03Z",
              "comment_text": "The LinkageError is when you have the same class in multiple classloaders, but from different origins, and there's an attempt to use the classes across the classloaders.\r\n\r\nThe `jetty-websocket-httpclient.xml` is loaded by the [`org.eclipse.jetty.websocket.client.WebSocketClient`](https://github.com/eclipse/jetty.project/blob/jetty-9.4.31.v20200723/jetty-websocket/websocket-client/src/main/java/org/eclipse/jetty/websocket/client/WebSocketClient.java) class.\r\nThe `WebSocketClient` loads the `org.eclipse.jetty.client.HttpClient` class from the server classloader too.\r\n\r\nBoth the `WebSocketClient` and the `HttpClient` should only exist on the server classloader.\r\nRemoving the `jetty-client.jar` from your `WEB-INF/lib` was a partial solution.\r\n\r\nThe `websocket-client.jar` should also be removed, **along with any other Jetty specific jar files in `WEB-INF/lib`**, as there is rarely a reason for them to be in `WEB-INF/lib`.\r\n\r\nYour exception occurs at `org.eclipse.jetty.websocket.jsr356.server.deploy.WebSocketServerContainerInitializer` which is a server component that loads the `javax.websocket` implementation from the Server classloader and configures it into the webapp's ServletContext layer."
            },
            {
              "comment_username": "michael-nilson-yardi",
              "comment_create_time": "2020-09-25T23:20:02Z",
              "comment_edit_time": "2020-09-25T23:20:02Z",
              "comment_text": "I've been attempting to recreate the problem and have used your sample project [found here](https://github.com/joakime/javaxwebsocket-client-returncode/tree/34d67aa364264cb2e2d16388d10289e3fabe592c) as a starting point. I have adapted it slightly to add the config to make it a webapp and add a servlet for testing purposes. I've uploaded my repo [here](https://github.com/michael-nilson-yardi/WebsocketClassNotFoundException). I've uploaded a gist including the jetty log [here](https://gist.github.com/michael-nilson-yardi/4e36141100156c061ea209916d1ae420). As you can see on line 608, when running it without the jetty jars included in web-inf/lib we are encountering a ClassNotFoundException for HttpClient when it parses the jetty-websocket-httpclient.xml file. \r\n\r\nI'm certain this is something I'm doing incorrectly for config on my jetty-distribution but haven't been able to nail it down yet. Any help would be appreciated. Thanks for your feedback thus far, it has been helpful."
            },
            {
              "comment_username": "michael-nilson-yardi",
              "comment_create_time": "2020-09-28T13:22:01Z",
              "comment_edit_time": "2020-09-28T13:22:01Z",
              "comment_text": "I recognized that we were still dropping a lot of warnings due to having the javax.servlet jar included in our web-inf/lib. I've pulled that jar out and uploaded a [new gist](https://gist.github.com/michael-nilson-yardi/e77f0dc3b0d69cf5a818dd8e7f7f7024) that contains the startup logs. Still the same issue though: ClassNotFoundException for org.eclipse.jetty.client.HttpClient."
            },
            {
              "comment_username": "lachlan-roberts",
              "comment_create_time": "2020-09-28T15:00:15Z",
              "comment_edit_time": "2020-09-28T15:00:15Z",
              "comment_text": "@michael-nilson-yardi I have been able to replicate this issue and I don't think it is due to incorrect configuration on your part. You can try adding these lines to your `start.ini` as a workaround while we work on a better solution.\r\n\r\n```\r\njetty.webapp.addSystemClasses+=,org.eclipse.jetty.client.\r\njetty.webapp.addServerClasses+=,-org.eclipse.jetty.client.\r\njetty.webapp.addSystemClasses+=,org.eclipse.jetty.util.ssl.\r\njetty.webapp.addServerClasses+=,-org.eclipse.jetty.util.ssl.\r\n```"
            },
            {
              "comment_username": "michael-nilson-yardi",
              "comment_create_time": "2020-09-28T15:14:49Z",
              "comment_edit_time": "2020-09-28T15:14:49Z",
              "comment_text": "Thanks for the help @lachlan-roberts . After adding the above config we saw the below output in the logs after making a websocket connection. \r\n```\r\n2020-09-28 09:07:29.969:DBUG:oejw.WebAppContext:Thread-20: isServerClass==true interface org.eclipse.jetty.util.component.LifeCycle\r\nException in thread \"Thread-20\" java.lang.NoClassDefFoundError: org/eclipse/jetty/util/component/LifeCycle\r\n\tat org.eclipse.jetty.demo.App.lambda$stop$0(App.java:47)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: java.lang.ClassNotFoundException: org.eclipse.jetty.util.component.LifeCycle\r\n\tat org.eclipse.jetty.webapp.WebAppClassLoader.loadClass(WebAppClassLoader.java:565)\r\n\tat java.lang.ClassLoader.loadClass(Unknown Source)\r\n\t... 2 more\r\n```\r\n\r\nI tried adding similar config for org.eclipse.jetty.util.component which got us past the NoClassDefFoundError for LifeCycle but then the jetty server would stop immediately after the websocket connection closed. I can get more debug info if it is helpful."
            },
            {
              "comment_username": "lachlan-roberts",
              "comment_create_time": "2020-09-28T16:05:28Z",
              "comment_edit_time": "2020-09-28T16:07:48Z",
              "comment_text": "@michael-nilson-yardi if you are calling `LifeCycle.start(webSocketClient)` or `LifeCycle.stop(webSocketClient)` try replacing that with `webSocketClient.start()` / `webSocketClient.stop()`.\r\n\r\nOr try adding this to `start.ini`\r\n\r\n```\r\njetty.webapp.addSystemClasses+=,org.eclipse.jetty.util.\r\njetty.webapp.addServerClasses+=,-org.eclipse.jetty.util.\r\n```"
            },
            {
              "comment_username": "michael-nilson-yardi",
              "comment_create_time": "2020-09-28T16:22:52Z",
              "comment_edit_time": "2020-09-28T16:22:52Z",
              "comment_text": "@lachlan-roberts the issue with jetty stopping was related to an oversight in our sample project. Will there be an issue we can track for watching for the \"better solution\"? In the meantime we'll proceed with testing the start.ini config you have provided with our actual product instead of the sample. Thanks again."
            },
            {
              "comment_username": "joakime",
              "comment_create_time": "2020-09-28T17:55:10Z",
              "comment_edit_time": "2020-09-28T17:55:10Z",
              "comment_text": "This is the issue to track.\r\n\r\nThe suggestion from @lachlan-roberts is merely a temporary workaround until we can get a fix into place (and a release with it in place)."
            }
          ]
        },
        {
          "issue_url": "https://github.com/eclipse/jetty.project/issues/5318",
          "issue_title": "EofException during flush",
          "issue_number": 5318,
          "issue_text": "**Jetty version**\r\nJetty version 9.4.5 \r\n**Java version**\r\njdk1.8\r\n**Question**\r\nRecently iam getting the below issue in the Prod Enviornment \r\n\r\norg.eclipse.jetty.io.EofException\r\n\tat org.eclipse.jetty.io.ChannelEndPoint.flush(ChannelEndPoint.java:292)\r\n\tat org.eclipse.jetty.io.WriteFlusher.flush(WriteFlusher.java:429)\r\n\tat org.eclipse.jetty.io.WriteFlusher.completeWrite(WriteFlusher.java:384)\r\n\tat org.eclipse.jetty.io.ChannelEndPoint$3.run(ChannelEndPoint.java:139)\r\n\tat org.eclipse.jetty.util.thread.Invocable.invokePreferred(Invocable.java:128)\r\n\tat org.eclipse.jetty.util.thread.Invocable$InvocableExecutor.invoke(Invocable.java:222)\r\n\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:294)\r\n\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:199)\r\n\tat org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:672)\r\n\tat org.eclipse.jetty.util.thread.QueuedThreadPool$2.run(QueuedThreadPool.java:590)\r\n\tat java.lang.Thread.run(Thread.java:745)\r\nCaused by: java.io.IOException: Connection reset by peer\r\n\tat sun.nio.ch.FileDispatcherImpl.write0(Native Method)\r\n\tat sun.nio.ch.SocketDispatcher.write(SocketDispatcher.java:47)\r\n\tat sun.nio.ch.IOUtil.writeFromNativeBuffer(IOUtil.java:93)\r\n\tat sun.nio.ch.IOUtil.write(IOUtil.java:51)\r\n\tat sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:471)\r\n\tat org.eclipse.jetty.io.ChannelEndPoint.flush(ChannelEndPoint.java:270)\r\n\t... 10 more\r\nif i received this error then my server load is increased and the web requests are slow . once i restarted the server then the application is working fine. \r\nPlease let me know what could be the issue and provide the solution . \r\nThanks \r\nChaitanya Kiran PVN \r\n\r\n",
          "issue_comments": [
            {
              "comment_username": "sbordet",
              "comment_create_time": "2020-09-23T19:52:32Z",
              "comment_edit_time": "2020-09-23T19:52:32Z",
              "comment_text": "The stack trace shows that your server was writing to an connection that was TCP congested, and when it resumed writing the connection was closed by the client.\r\n\r\nThe exception is reported to the application that failed to write, so it has the chance to do something.\r\nIf the application lets the exception to flow out, Jetty by default will log it at DEBUG level.\r\n\r\n"
            },
            {
              "comment_username": "joakime",
              "comment_create_time": "2020-09-23T19:58:58Z",
              "comment_edit_time": "2020-09-23T19:58:58Z",
              "comment_text": "> Jetty version 9.4.5\r\n\r\nYou should upgrade, you are subject to several Security issues with that version of Jetty.\r\n\r\nSee: https://www.eclipse.org/jetty/security-reports.html\r\n"
            }
          ]
        },
        {
          "issue_url": "https://github.com/eclipse/jetty.project/issues/5315",
          "issue_title": "Linking to .mod files in the documentation",
          "issue_number": 5315,
          "issue_text": "**Jetty -10**\r\n\r\nCurrently the operations guide links through to the .mod files when discussing configuration parameters. My concern is that this will lead to users mistakenly modifying the `.mod` files to change params instead of the `.ini` files. Although we do document that `.ini` files should be edited instead of `.mod` files, unless you've read the whole guide very closely you can miss this point; if you're in a hurry and just dive into the section on the module you've been told to enable you'll be lead to the `.mod` file, and you might easily think you need to modify that to effect changes.  IMHO it would be better to link through to something that _appears_ to be a `.ini` file - whether that's achieved by only linking to the `[ini-template]` subsection of `.mod` files or by some other means.\r\n\r\n\r\n\r\n",
          "issue_comments": [
            {
              "comment_username": "gregw",
              "comment_create_time": "2020-09-23T11:55:09Z",
              "comment_edit_time": "2020-09-23T11:55:09Z",
              "comment_text": "It should be pretty easy to write a script that can generate adoc files from out mod files that would well present the name, description, dependencies and the ini template, without the need to just import the whole file"
            }
          ]
        },
        {
          "issue_url": "https://github.com/eclipse/jetty.project/issues/5314",
          "issue_title": "Websocket connection closes with closure code 1011 and closure reason EoFException",
          "issue_number": 5314,
          "issue_text": "**Jetty version**\\r\\n9.4.31.v20200723 (Recently upgraded from 9.4.6.v20170531)\\r\\n\\r\\n**Java version**\\r\\nopenjdk version \"1.8.0_265\"\\r\\n\\r\\n**Question**\\r\\nWe are using jetty as our websocket server. Recently upgraded the version to the one mentioned above.\\r\\n\\r\\nSample (truncated) logs :\\r\\n\\r\\n9/21/209:02:09.015 PM | INFO  \\\\| 2020-09-21 21:02:09,015 \\\\| qtp1663166483-132 \\\\|  [WsListener::onWebSocketClose::70]    wsSessionIdentifier=gSN4v/vB/lmviGlDL75K08Obqq8=; **closureCode=1011; closureReason=\"EofException\";** \\r\\n\\r\\n\\xa0 | 9/21/209:02:09.015 PM | ERROR \\\\| 2020-09-21 21:02:09,015 \\\\| qtp1663166483-132 \\\\|  [WsListener::onWebSocketError::146]  wsSessionIdentifier=gSN4v/vB/lmviGlDL75K08Obqq8=; errorReason=\"null\"; \\\\|\\r\\n\\r\\nI have observed this during certain high load periods.\\r\\n\\r\\nCan you help me understand this closureReason: EofException. Also, any tips on reproducing this in a development environment.\\r\\n\\r\\n**Additional Info** : Also observed  \"Too many open files\" at the same time interval but not always\\r\\n\\r\\n(Also I do not remember seeing this on the earlier version - I am not sure though)\\r\\n\\r\\nThanks,\\r\\nBala\\r\\n\\r\\n\\r\\n",
          "issue_comments": [
            {
              "comment_username": "joakime",
              "comment_create_time": "2020-09-23T12:07:00Z",
              "comment_edit_time": "2020-09-23T12:07:00Z",
              "comment_text": "Usually an `EofException` points to a connection that was closed.\r\n\r\nThe stacktraces would tell us more about the nature of the specific EofException you are encountering."
            },
            {
              "comment_username": "lachlan-roberts",
              "comment_create_time": "2020-09-23T23:09:26Z",
              "comment_edit_time": "2020-09-23T23:09:26Z",
              "comment_text": "@bala-sundhar You could get a `EofException` in the case that the underlying connection was closed before the websocket close handshake could complete. Likely that it was writing a frame at the time and `EndPoint.flush()` threw `EofException`. You can look at the stacktrace from your `onWebSocketError` method to be sure."
            }
          ]
        },
        {
          "issue_url": "https://github.com/eclipse/jetty.project/issues/5310",
          "issue_title": "Jetty Http2 client discards the response fames when there is GOAWAY and sends RST_STREAM",
          "issue_number": 5310,
          "issue_text": "**Jetty version**  9.4.27.v20200227\r\n\r\n**Java version**  OpenJdk 14\r\n\r\n**OS type/version** Linux master 4.1.12-112.16.4.el7uek.x86_64 /Deployed in Kubernetes cluster\r\n\r\n**Description** \r\nWe are using Jetty Http2 client in our application. We have a scenario where the server will terminate the connection by sending GOAWAY frame with no error along with the response header and data frames. But Jetty client cancels the request by sending RST_STREAM  and terminates the connection by sending GOAWAY. In the application side client also throws \"java.nio.channels.AsynchronousCloseException\" without committing response.\r\n\r\n{\"thread\":\"@d8d9199-4177\",\"level\":\"ERROR\",\"loggerName\":\"Exception\",\"message\":\"Internal Server Error\",\"thrown\":{\"commonElementCount\":0,\"name\":\"java.nio.channels.AsynchronousCloseException\",\"extendedStackTrace\":[{\"class\":\"org.eclipse.jetty.http2.client.http.HttpConnectionOverHTTP2\",\"method\":\"close\",\"file\":\"HttpConnectionOverHTTP2.java\",\"line\":144,\"exact\":false,\"location\":\"http2-http-client-transport-9.4.27.v20200227.jar!/\",\"version\":\"9.4.27.v20200227\"}],\"suppressed\":[{\"commonElementCount\":0,\"localizedMessage\":\"\\nError has been observed at the following site(s):\\n\\t|_ checkpoint ⇢ Request to POST\r\n<img width=\"891\" alt=\"Capture\" src=\"https://user-images.githubusercontent.com/11327645/93871024-a5f48100-fceb-11ea-8525-39831ada9db7.PNG\">\r\n<img width=\"891\" alt=\"Capture\" src=\"https://user-images.githubusercontent.com/11327645/93871124-cae8f400-fceb-11ea-9981-ef1b543aa96c.PNG\">\r\n\r\n\r\n",
          "issue_comments": [
            {
              "comment_username": "sbordet",
              "comment_create_time": "2020-09-22T11:53:14Z",
              "comment_edit_time": "2020-09-22T11:53:14Z",
              "comment_text": "You do not say what the problem is, you just describe a normal scenario.\r\n\r\nWhy do you think what you describe is an issue?\r\n"
            },
            {
              "comment_username": "gregw",
              "comment_create_time": "2020-09-22T12:07:23Z",
              "comment_edit_time": "2020-09-22T12:07:23Z",
              "comment_text": "@sukawanth What we see in your trace does indicate your server is a little strange.  We see\r\n```text\r\nClient <-------------> Server\r\n <---- Settings    // probably response to settings in the upgrade\r\n <---- Settings    // strange second settings frame here, but no harm\r\n Settings ---->    // client responding to server settings frame\r\nHeaders ---->      // POST\r\nData ---->         // POST body... but can't tell if that is complete?\r\nSettings ---->     // More settings? rather strange???? @sbordet any idea why?\r\n<---- GOAWAY       // The first go away! Maybe it didn't like the settings?\r\nRST ---->           // Client resets stream! because of go away\r\nGOAWAY ---->       // Client responds to go away\r\nHeaders ---->     // Another POST???? is this the same connection?  Can you turn on ports?\r\n```\r\n\r\nVery strange there is more conversation after the goaway and its response.  Also the multiple settings are confusing.  \r\nCan you display source and destination port as well as I think multiple connections may be in play.\r\n\r\n"
            },
            {
              "comment_username": "sukawanth",
              "comment_create_time": "2020-09-22T14:26:47Z",
              "comment_edit_time": "2020-09-22T14:26:47Z",
              "comment_text": "hi @gregw  yes you are correct there are multiple connections in play. I filter out one connection and attached the pic. We are facing a scenario where server is gracefully shutting down and is gracefully terminating all the connections established to it. So when new connections are established to the server and request is sent to it, server is accepting connection and responding to request with 200OK along with GO AWAY frame. this is observed for grace period after which connections are not accepted at server.\r\n\r\n@sbordet we expect that the response should be processed which is received with GOAWAY frame in same TCP packet. why jetty client is sending RST frame for the request? And for the client side out code is not receiving response instead seeing connection \r\nerror.\r\n\r\n\r\n<img width=\"949\" alt=\"Capture\" src=\"https://user-images.githubusercontent.com/11327645/93895603-a140c480-fd0d-11ea-86bb-05618cb0eba5.PNG\">\r\n"
            },
            {
              "comment_username": "sbordet",
              "comment_create_time": "2020-09-22T14:48:33Z",
              "comment_edit_time": "2020-09-22T14:48:33Z",
              "comment_text": "The problem seems on the server.\r\n\r\nThe client sends a \"magic\" + its settings + window_update\r\nThe server replies with its own settings + settings reply acknowledging the client one + window_update\r\nThe client sends a POST request\r\nThe server sends a GOAWAY and then the response?!?\r\n\r\nWhat server is it (product and version)?\r\n\r\nWhat do you exactly mean by \"scenario where server is gracefully shutting down and is gracefully terminating all the connections established to it\"?\r\n\r\nIf the server is shutting down, the GOAWAY is expected, although the last stream id contained in the GOAWAY is bogus.\r\nWhat do the server logs say?\r\n\r\nCan you please write down _exactly_ what you are doing, what do you expect to happen, and what actually happens instead?"
            },
            {
              "comment_username": "sukawanth",
              "comment_create_time": "2020-09-23T08:30:04Z",
              "comment_edit_time": "2020-09-23T08:30:04Z",
              "comment_text": "Hi @sbordet, Thanks for the reply\r\n\r\nBelow is more details explanation is the issue:\r\n\r\nEnvironment:\r\nwe are using Spring cloud gateway as a reverse proxy in our environment. SCG receives a request and uses jetty http2 client library to send request to backend server.\r\nBackend server is Envoy proxy V1.11.0.\r\nWe are using jetty round robin connection pool to establish connections to backend.\r\nThis solution is deployed in kubernetes cluster as docker containers.\r\nWe have a single container for SCG which receives request and send to backend. Backend has multiple containers which map to one FQDN. We have customized round robin connection pool to resolve backend FQDN and create connections to all resolved IPs. And we have a scheduler in SCG to refresh the FQDN-IP mapping and update the connection pool with changed ip list.\r\n\r\nTest scenario:\r\nWhile traffic is running, one of the back end container is gracefully shutdown. So jetty client received GOAWAY on that connection with promised-stream-id as (2Pow31)-1. As per the HTTP2 specification, in this scenario jetty client shall wait for some time untill final GOAWAY is received from server.\r\nWe see server sending GOAWAY frame with promised stream id as (2Pow31)-1 indicating graceful shutdown and sending response to pending request later.\r\n\r\nExpected behavior of jetty client:\r\nupon receiving GOAWAY frame with promised stream id as (2Pow31)-1 from server jetty client shall wait for responses for pending requests on that connection or until final GOAWAY is received before closing connection.\r\nProcess any response received before final GOAWAY from server.\r\n\r\nObseravtion:\r\nUpon receiving GOAWAY frame with promised stream id as (2Pow31)-1 from server, jetty client is cancelling the pending request on the connection and sending GOAWAY to server.\r\n\r\nQuestion is why jetty is not waiting for responses on connection after receiving GOAWAY frame with promised stream id as (2Pow31)-1"
            },
            {
              "comment_username": "sbordet",
              "comment_create_time": "2020-09-23T16:23:42Z",
              "comment_edit_time": "2020-09-23T16:23:42Z",
              "comment_text": "Ok, so the HTTP/2 client implementation does not wait for responses after it receives a GOAWAY, that's why it resets the stream.\r\nThe spec contemplates for the scenario that the client should wait for responses, so we will implement it."
            }
          ]
        },
        {
          "issue_url": "https://github.com/eclipse/jetty.project/issues/5308",
          "issue_title": "Extract httpConfig and scheduler configuration out of jetty.xml",
          "issue_number": 5308,
          "issue_text": "**Jetty version**\r\n10.0.x\r\n\r\n**Description**\r\nI guess for historical reasons, `jetty.xml` contains `HttpConfiguration` configuration, which should instead be factored out into its own module.\r\n\r\nSimilarly, the scheduler configuration should be factored out, like it has already been done for the thread pool and the bytebuffer pool.\r\n\r\n\r\n",
          "issue_comments": []
        },
        {
          "issue_url": "https://github.com/eclipse/jetty.project/issues/5306",
          "issue_title": "Default jetty.*.acceptors should be 1",
          "issue_number": 5306,
          "issue_text": "**Jetty version**\r\n10.0.x\r\n\r\n**Description**\r\nGiven that connections are more and more persistent (due to protocols keeping them open for performance reasons), and many many cores, I think the current heuristic is too generous (up to 23 cores yields 2, up to 31 cores yields 3, more yields 4), and a fixed heuristic of `1` is plenty enough.",
          "issue_comments": [
            {
              "comment_username": "gregw",
              "comment_create_time": "2020-09-21T19:28:56Z",
              "comment_edit_time": "2020-09-21T19:28:56Z",
              "comment_text": "I think I'd prefer 0 to be the default so we can do async accepting.\r\n"
            },
            {
              "comment_username": "gregw",
              "comment_create_time": "2020-09-23T13:25:30Z",
              "comment_edit_time": "2020-09-23T13:25:30Z",
              "comment_text": "I'm bumping this to 10.0.1 so we can better evaluate impacts"
            }
          ]
        },
        {
          "issue_url": "https://github.com/eclipse/jetty.project/issues/5305",
          "issue_title": "Remove jetty.http.connectTimeout from the http module",
          "issue_number": 5305,
          "issue_text": "**Jetty version**\r\n9.4.x\r\n\r\n**Description**\r\n`jetty.http.connectTimeout` is a client property and has no sense on a server module.\r\n\r\n\r\n",
          "issue_comments": []
        },
        {
          "issue_url": "https://github.com/eclipse/jetty.project/issues/5299",
          "issue_title": "Documentation fails to build when project path contains accented characters",
          "issue_number": 5299,
          "issue_text": "**Jetty version**\r\nBoth 9.4.x and 10.0.x.\r\n\r\n**Java version**\r\nopenjdk version \"11.0.8\" 2020-07-14\r\n\r\n**OS type/version**\r\nDebian buster\r\n\r\n**Description**\r\nThe docbkx plugin, required to build the documentation, (sometimes?) fails to run when the project path contains an accented character (Développement, for example). Strangely enough, the problem does not arise when the path contains spaces and no accented character.\r\n\r\nTo reproduce, move into a folder whose full path contains an accented character, then:\r\n> git clone git@github.com:eclipse/jetty.project.git\r\n> cd jetty.project\r\n> cd jetty-documentation\r\n> mvn package\r\n\r\n=> you will obtain: `[ERROR] Failed to execute goal com.agilejava.docbkx:docbkx-maven-plugin:2.0.17:generate-html (html) on project jetty-documentation: Failed to transform index.xml.: Failure reading [fullpath]/jetty.project/jetty-documentation/target/generated-docs/index.xml: no protocol: [fullpath]/jetty.project/jetty-documentation/target/generated-docs/index.xml -> [Help 1]`\r\n\r\nI just reported [this bug](https://github.com/mimil/docbkx-tools/issues/135) to the `docbkx-tools` project for documentation purposes, but, considering that the relevant plugin is now unmaintained, it is highly unlikely that it will get fixed.\r\n\r\nI do not claim that this bug should get high priority, as it has a simple workaround, and as fixing it might be very hard (requiring to delve into a plugin which the jetty team has nothing to do about, I suppose). Therefore, I suggest to simply add a comment in the pom of the jetty-documentation module, to warn users so that they do not waste time (as I did) before realizing where the problem lies. Or, hopefully, this bug report will be enough, once indexed by the main web search engines.\r\n\r\nTo summarize, this bug report only intends to document this behavior so as to help others, not really ask for a patch. (Of course if you know of an easy patch, that’s even better.)",
          "issue_comments": [
            {
              "comment_username": "lachlan-roberts",
              "comment_create_time": "2020-09-21T00:03:37Z",
              "comment_edit_time": "2020-09-21T00:03:37Z",
              "comment_text": "Looks like from Jetty-10 onwards we are using only asciidoctor plugin to generate the documentation instead of using docbkx. I have tested on `10.0.x` and I could not reproduce the same problem."
            },
            {
              "comment_username": "oliviercailloux",
              "comment_create_time": "2020-09-21T06:45:34Z",
              "comment_edit_time": "2020-09-21T06:45:34Z",
              "comment_text": "You are right. My bad. The problem I encountered with `10.0.x` differs (doc requires other modules, and I can’t `mvn test` the whole project because some tests fail). So this bug report concerns only the `9.4.x` branch."
            },
            {
              "comment_username": "lachlan-roberts",
              "comment_create_time": "2020-09-21T07:24:55Z",
              "comment_edit_time": "2020-09-21T07:24:55Z",
              "comment_text": "@oliviercailloux If you want to build the documentation in `10.0.x` you could run `mvn clean install -DskipTests` for the whole project. Then go specifically to the `jetty-documentation` directory and run `mvn clean install`. That should allow you to build the documentation without running any tests.\r\n\r\n@WalkerWatch is there any reason we need docbkx for `9.4.x` and we can't do the same thing we do in jetty-10?"
            }
          ]
        },
        {
          "issue_url": "https://github.com/eclipse/jetty.project/issues/5297",
          "issue_title": "After upgrade from JDK 8.0.241 to JDK 8.0.261 Http2 client is not working. Throwing Timeout error",
          "issue_number": 5297,
          "issue_text": "Jetty version : 9.4.19\r\n\r\nJava version :jdk 8.0.261\r\n\r\nOS type/version :Red hat 7\r\n\r\nDescription  : After upgrade of JDK 8.0.261, our http2 client is not working. its unable to send the request and getting timeout error.\r\nGot an exception java.util.concurrent.TimeoutException: \r\n\r\nHence, we tried to upgrade jetty 9.4.28 and tried both by removing and adding alpn bootpath. only when the alpn boot path is added its working. otherwise its failing as No Client ALPN Processor.\r\n\r\nI noticed in 9.4.28 document, it is mentioned that for 9.4.28 jetty version alpn-boot jar is not required. \r\n![jetty_jar_name](https://user-images.githubusercontent.com/71491090/93584298-bd1d3100-f9c2-11ea-8ca1-0d904aac138c.jpg)\r\nplease let us know your input.\r\n\r\n\r\n",
          "issue_comments": [
            {
              "comment_username": "sbordet",
              "comment_create_time": "2020-09-18T13:28:48Z",
              "comment_edit_time": "2020-09-18T13:28:48Z",
              "comment_text": "Read https://webtide.com/jetty-alpn-java-8u252/.\r\n\r\nIf still does not work, we need details about the error (stack traces, etc.)."
            },
            {
              "comment_username": "joakime",
              "comment_create_time": "2020-09-18T14:11:10Z",
              "comment_edit_time": "2020-09-18T14:11:10Z",
              "comment_text": "Would recommend upgrading to 9.4.31, as 9.4.28 is subject to several security reports - https://www.eclipse.org/jetty/security-reports.html"
            },
            {
              "comment_username": "erstaae",
              "comment_create_time": "2020-09-21T13:38:55Z",
              "comment_edit_time": "2020-09-21T13:38:55Z",
              "comment_text": "> Read https://webtide.com/jetty-alpn-java-8u252/.\r\n> If still does not work, we need details about the error (stack traces, etc.).\r\n\r\n"
            },
            {
              "comment_username": "erstaae",
              "comment_create_time": "2020-09-21T13:39:34Z",
              "comment_edit_time": "2020-09-21T13:39:34Z",
              "comment_text": "I tested 9.4.31 without alpn. it is working fine. I want to just know any fix has been on 9.4.28 related to it."
            },
            {
              "comment_username": "sbordet",
              "comment_create_time": "2020-09-21T13:54:05Z",
              "comment_edit_time": "2020-09-21T13:54:05Z",
              "comment_text": "Related to what? It turned out there was no issue at all, so you need to be more specific."
            },
            {
              "comment_username": "erstaae",
              "comment_create_time": "2020-09-21T15:17:34Z",
              "comment_edit_time": "2020-09-21T15:17:34Z",
              "comment_text": "In 9.4.28, i tested without alpn-boot and it is not working. But when I upgrade to 9.4.31 it is working fine. Hence I thought to ask if any changes related to alpn-boot is wne in."
            },
            {
              "comment_username": "sbordet",
              "comment_create_time": "2020-09-21T15:47:56Z",
              "comment_edit_time": "2020-09-21T15:47:56Z",
              "comment_text": "We thoroughly tested 9.4.28.\r\nNothing has changed regarding ALPN between 9.4.28 and 9.4.31.\r\n\"Not working\" is not enough information, so please be more specific."
            }
          ]
        },
        {
          "issue_url": "https://github.com/eclipse/jetty.project/issues/5295",
          "issue_title": "Issue #5287 - rework CompressionPool to use the jetty-util pool",
          "issue_number": 5295,
          "issue_text": "This is based on PR #5248 which should be merged before this PR.",
          "issue_comments": []
        },
        {
          "issue_url": "https://github.com/eclipse/jetty.project/issues/5293",
          "issue_title": "Issue #5291 - Capacity Check in ArrayTernaryTrie",
          "issue_number": 5293,
          "issue_text": "#5291 Check capacity of ArrayTernaryTrie\r\nSwitch to ArrayTrie if over capacity",
          "issue_comments": [
            {
              "comment_username": "gregw",
              "comment_create_time": "2020-09-17T13:56:49Z",
              "comment_edit_time": "2020-09-17T13:56:49Z",
              "comment_text": "This is a very quick fix....\r\nIt needs at least tests, but it is poor approach as it leaks details of max capacity to ContextHandlerCollection"
            },
            {
              "comment_username": "janci007",
              "comment_create_time": "2020-09-28T10:00:44Z",
              "comment_edit_time": "2020-09-28T10:06:02Z",
              "comment_text": "This hotfix has multiple issues:\r\n- the capacity increasing in ContextHandlerCollection can overflow\r\n- the capacity check in ArrayTernaryTree is wrong, Character.MAX_VALUE is 65535 not 32768\r\n- if I understand correctly, the capacity limit is not of capacity parameter, but capacity * ROW_SIZE as _tree array is indexed by char-typed index\r\n- if over capacity limit, the implementation switches from case-sensitive to case-insensitive"
            }
          ]
        },
        {
          "issue_url": "https://github.com/eclipse/jetty.project/issues/5291",
          "issue_title": "ArrayTenaryTrie bug. Infinite Loop with many Handlers",
          "issue_number": 5291,
          "issue_text": "```java\\r\\nimport org.eclipse.jetty.util.ArrayTernaryTrie;\\r\\n\\r\\npublic class ArrayTenaryTrieTest {\\r\\n\\r\\n    public void loopTest(int size) throws Exception {\\r\\n        int calcSize = 0;\\r\\n        for(int i = 0; i< size; i++) {\\r\\n            calcSize += \"/test/group\".length()+ Integer.toString(i).length();\\r\\n        }\\r\\n\\r\\n        System.out.println(\"Estimated size: \"+calcSize);\\r\\n\\r\\n        ArrayTernaryTrie<Integer> trie = new ArrayTernaryTrie<>(calcSize*3);\\r\\n        for(int i = 0; i< size; i++) {\\r\\n            boolean added = trie.put(\"/test/group\" + i, i);\\r\\n            if(!added) {\\r\\n                throw new Exception(\"Could not add entry \"+\"/test/group\" + i);\\r\\n            }\\r\\n        }\\r\\n\\r\\n        System.out.println(\"added \"+size+\" entries successfully\");\\r\\n        System.out.println(\"trie.size() = \"+trie.size());\\r\\n\\r\\n        for(int i = 0; i< size; i++) {\\r\\n            Integer integer = trie.get(\"/test/group\" + i);\\r\\n            if(integer == null) {\\r\\n                throw new Exception(\"missing entry for '/test/group\" + i+\"'\");\\r\\n            } else if(integer.intValue() != i) {\\r\\n                throw new Exception(\"incorrect value for '/test/group\" + i+\"' (\"+integer+\")\");\\r\\n            }\\r\\n        }\\r\\n    }\\r\\n\\r\\n    public static void main(String[] args) throws Exception {\\r\\n        new ArrayTenaryTrieTest().loopTest(60_000);\\r\\n    }\\r\\n}\\r\\n\\r\\n```\\r\\n\\r\\nfails with\\r\\n```\\r\\nEstimated size: 948890\\r\\nadded 60000 entries successfully\\r\\ntrie.size() = 15375\\r\\nException in thread \"main\" java.lang.Exception: missing entry for '/test/group0'\\r\\n```\\r\\n\\r\\nThe trie only contains ~15k entries of the 60k that were all acknowledged with a true return on .put().\\r\\n\\r\\nThis fails no matter what capacity is given to the Trie.\\r\\n\\r\\nThis leads to an infinite loop in org.eclipse.jetty.server.handler.ContextHandlerCollection.newHandlers with a lot (>50k im my case) of handlers.",
          "issue_comments": [
            {
              "comment_username": "gregw",
              "comment_create_time": "2020-09-17T13:05:21Z",
              "comment_edit_time": "2020-09-17T13:05:21Z",
              "comment_text": "It fails at 32_763, so fells like a power of 2 thing going on...."
            },
            {
              "comment_username": "lambdaupb",
              "comment_create_time": "2020-09-17T13:37:25Z",
              "comment_edit_time": "2020-09-17T13:37:25Z",
              "comment_text": "Maybe some wraparound. Could try replacing the math with `Math.xxxExact(a,b)`, especially the hilo() method looks risky with its MAX_VALUE thing.\r\n\r\nHaven't tried this though, don't have the project checked out."
            },
            {
              "comment_username": "lambdaupb",
              "comment_create_time": "2020-09-17T13:42:30Z",
              "comment_edit_time": "2020-09-17T13:42:30Z",
              "comment_text": "scratch the `hilo()` bit, that does not seem to be called in `put()`"
            },
            {
              "comment_username": "joakime",
              "comment_create_time": "2020-09-17T13:42:55Z",
              "comment_edit_time": "2020-09-17T13:42:55Z",
              "comment_text": "Some discoveries on `loopTest(size)` ...\r\n\r\n* size is `21841` the last time you get 100% success.\r\n* size is `21842` you get 0 correct, 21842 missing, 0 incorrect.\r\n* size is `21843` you get 1 correct, 20,066 missing, 1776 incorrect."
            },
            {
              "comment_username": "gregw",
              "comment_create_time": "2020-09-17T13:51:04Z",
              "comment_edit_time": "2020-09-17T13:51:04Z",
              "comment_text": "OK I can see what is going on..... but before I get into the details I'll just say.... > 50k context handlers ... whaaaaaaaaaaaaat!?!?!?!\r\nI'm sure there is some very good reason for that, but it is kind of exceptional.... and we like jetty to be used in exceptional circumstances.... but still wow!!!!\r\n\r\nSo the problem is that `ArrayTenaryTrie`'s main data structure is a character array, with each 4 characters representing a row in the table and 3 of entries in the row are actually indexes into the table!  So once the number of rows in the table becomes greater than Character.MaxValue (0xFFFF = 32768), then the table indexing breaks down!\r\n\r\nSo first bug is that the class needs to check it's bounds and if we get too many rows it needs to throw... or probably it should have already thrown in the constructor if the capacity is too much.\r\n\r\nThen how do we support the demand for larger numbers?  I can see several solutions:\r\n\r\n- The indexes in the char table can be divided by the ROW_SIZE, so we'd get a 4 times larger at the cost of an extra multiplication per char lookup.\r\n- The array could be an array of ints, giving us a much bigger range, but costing a bit in size.\r\n- Use a different Trie impl\r\n\r\nNote that ArrayTrie itself goes to a much bigger size, but I think it may also have issues with not checking it's bounds.  So we need to make sure that it does.\r\n\r\nFor ContextHandlerCollection, it can either:\r\n 1. Use an ArrayTernaryTrie with expanded capacity.\r\n 2. use ArrayTrie (with bounds checking)\r\n 3. Start using ArrayTernayTrie and if the size demands increase beyond limit start using ArrayTrie\r\n\r\n2 is a quick fix for the OP, but we need to evaluate the space/time tradeoffs with all our Tries.\r\n\r\n \r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n"
            },
            {
              "comment_username": "gregw",
              "comment_create_time": "2020-09-17T13:52:00Z",
              "comment_edit_time": "2020-09-17T13:52:00Z",
              "comment_text": "@lorban I think this is a good one for you post your current rabbit hole! "
            },
            {
              "comment_username": "lambdaupb",
              "comment_create_time": "2020-09-17T13:58:40Z",
              "comment_edit_time": "2020-09-17T13:58:40Z",
              "comment_text": "> OK I can see what is going on..... but before I get into the details I'll just say.... > 50k context handlers ... whaaaaaaaaaaaaat!?!?!?!\r\n> I'm sure there is some very good reason for that, but it is kind of exceptional.... and we like jetty to be used in exceptional circumstances.... but still wow!!!!\r\n\r\n:smiley: \r\nIts either that, or essentially re-implementing path dispatch again in a catch-all handler, which was not very appealing."
            },
            {
              "comment_username": "joakime",
              "comment_create_time": "2020-09-17T14:01:27Z",
              "comment_edit_time": "2020-09-17T14:01:27Z",
              "comment_text": "You might want to look into the `PathMappings` component in Jetty.\r\n\r\nhttps://www.eclipse.org/jetty/javadoc/current/org/eclipse/jetty/http/pathmap/PathMappings.html\r\n\r\nExamples: https://github.com/eclipse/jetty.project/blob/jetty-9.4.x/jetty-http/src/test/java/org/eclipse/jetty/http/pathmap/PathMappingsTest.java"
            },
            {
              "comment_username": "gregw",
              "comment_create_time": "2020-09-17T14:01:35Z",
              "comment_edit_time": "2020-09-17T14:01:35Z",
              "comment_text": "I've created PR #5293 with a quick fix.  It lacks test harnesses and leaks impl details to the usage of the Trie.\r\nBut perhaps we sneak something like this into the next release while we contemplate a better solution....  @lambdaupb would you be able to work with your own local build of the PR branch to get you further while we take a few weeks to come up with a nicer solution?\r\n\r\nI think we still need to review all our Trie implementations for bounds checks and check that size/speed tradeoffs are still relevant.\r\n\r\nI also think that perhaps we should stop allowing specific Tries to be constructed.... instead have a static factory mechanism that can take the requirements: case insensitive, capacity, growing, expected characters etc. and then we can pick an impl for them.   This will alllow us to be smarter in future about swapping out Trie impls.\r\n\r\n\r\n\r\n\r\n\r\n"
            },
            {
              "comment_username": "lambdaupb",
              "comment_create_time": "2020-09-17T14:13:07Z",
              "comment_edit_time": "2020-09-17T14:13:07Z",
              "comment_text": "Possibly related: #1353 ?\r\n\r\n---\r\n\r\n\r\n> But perhaps we sneak something like this into the next release while we contemplate a better solution.... @lambdaupb would you be able to work with your own local build of the PR branch to get you further while we take a few weeks to come up with a nicer solution?\r\n\r\nFor better or for worse, for the time being, I'll probably have to re-implement the path lookup with a catch-all handler on our side, that seems a lower maintenance burden, so take your time to do it right.\r\n"
            },
            {
              "comment_username": "joakime",
              "comment_create_time": "2020-09-17T14:34:25Z",
              "comment_edit_time": "2020-09-17T14:34:25Z",
              "comment_text": "> For better or for worse, for the time being, I'll probably have to re-implement the path lookup with a catch-all handler on our side\\r\\n\\r\\nHere's a template for you to start with ...\\r\\n\\r\\n``` java\\r\\npackage org.eclipse.jetty.server.handlers;\\r\\n\\r\\nimport java.io.IOException;\\r\\nimport java.util.List;\\r\\nimport javax.servlet.ServletException;\\r\\nimport javax.servlet.http.HttpServletRequest;\\r\\nimport javax.servlet.http.HttpServletResponse;\\r\\n\\r\\nimport org.eclipse.jetty.http.pathmap.MappedResource;\\r\\nimport org.eclipse.jetty.http.pathmap.PathMappings;\\r\\nimport org.eclipse.jetty.http.pathmap.PathSpec;\\r\\nimport org.eclipse.jetty.server.Handler;\\r\\nimport org.eclipse.jetty.server.Request;\\r\\nimport org.eclipse.jetty.server.handler.AbstractHandler;\\r\\nimport org.eclipse.jetty.util.component.LifeCycle;\\r\\n\\r\\npublic class PathMappingHandler extends AbstractHandler\\r\\n{\\r\\n    public static final String ATTR_PATH_SPEC = PathMappingHandler.class.getName() + \".pathSpec\";\\r\\n    private final PathMappings<Handler> mappings = new PathMappings<>();\\r\\n\\r\\n    public void addMapping(PathSpec pathSpec, Handler handler)\\r\\n    {\\r\\n        mappings.put(pathSpec, handler);\\r\\n    }\\r\\n\\r\\n    @Override\\r\\n    public void handle(String target, Request baseRequest, HttpServletRequest request, HttpServletResponse response) throws IOException, ServletException\\r\\n    {\\r\\n        List<MappedResource<Handler>> matches = mappings.getMatches(target);\\r\\n        if (matches != null)\\r\\n        {\\r\\n            for (MappedResource<Handler> mappedResource : matches)\\r\\n            {\\r\\n                PathSpec pathSpec = mappedResource.getPathSpec();\\r\\n                request.setAttribute(ATTR_PATH_SPEC, pathSpec);\\r\\n                Handler handler = mappedResource.getResource();\\r\\n                handler.handle(target, baseRequest, request, response);\\r\\n                if (baseRequest.isHandled())\\r\\n                    return;\\r\\n            }\\r\\n        }\\r\\n\\r\\n        response.setStatus(404);\\r\\n        baseRequest.setHandled(true);\\r\\n    }\\r\\n\\r\\n    @Override\\r\\n    protected void doStart() throws Exception\\r\\n    {\\r\\n        for (MappedResource<Handler> handler : mappings.getMappings())\\r\\n        {\\r\\n            if (handler instanceof LifeCycle)\\r\\n            {\\r\\n                LifeCycle lifeCycle = (LifeCycle)handler;\\r\\n                if (!lifeCycle.isStarted())\\r\\n                {\\r\\n                    lifeCycle.start();\\r\\n                }\\r\\n            }\\r\\n        }\\r\\n\\r\\n        super.doStart();\\r\\n    }\\r\\n}\\r\\n\\r\\n```\\r\\n\\r\\nYou use it like this ...\\r\\n\\r\\n``` java\\r\\npackage org.eclipse.jetty.server.handlers;\\r\\n\\r\\nimport org.eclipse.jetty.http.pathmap.RegexPathSpec;\\r\\nimport org.eclipse.jetty.http.pathmap.ServletPathSpec;\\r\\nimport org.eclipse.jetty.server.Server;\\r\\nimport org.eclipse.jetty.server.handler.DefaultHandler;\\r\\n\\r\\npublic class PathMappingHandlerTest\\r\\n{\\r\\n    public static void main(String[] args) throws Exception\\r\\n    {\\r\\n        Server server = new Server(9090);\\r\\n        PathMappingHandler pathMappingHandler = new PathMappingHandler();\\r\\n        server.setHandler(pathMappingHandler);\\r\\n\\r\\n        pathMappingHandler.addMapping(new ServletPathSpec(\"/hello\"), new HelloHandler(\"greetings\"));\\r\\n        pathMappingHandler.addMapping(new RegexPathSpec(\"/hello/.*/earthling\"), new HelloHandler(\"earthling\"));\\r\\n        // final \"default\" path-spec that will be used if nothing above matches\\r\\n        pathMappingHandler.addMapping(new ServletPathSpec(\"/\"), new DefaultHandler());\\r\\n\\r\\n        server.start();\\r\\n        server.join();\\r\\n    }\\r\\n}\\r\\n```\\r\\n\\r\\nExample results.\\r\\n\\r\\n``` shell\\r\\n[joakim@hyperion junk][master*]$ curl http://localhost:9090/hello\\r\\ngreetings\\r\\n[joakim@hyperion junk][master*]$ curl http://localhost:9090/hello/fellow/earthling\\r\\nearthling\\r\\n[joakim@hyperion junk][master*]$ curl http://localhost:9090/something/not/found\\r\\n<html>\\r\\n<head>\\r\\n<meta http-equiv=\"Content-Type\" content=\"text/html;charset=utf-8\"/>\\r\\n<title>Error 404 Not Found</title>\\r\\n</head>\\r\\n<body><h2>HTTP ERROR 404 Not Found</h2>\\r\\n<table>\\r\\n<tr><th>URI:</th><td>/something/not/found</td></tr>\\r\\n<tr><th>STATUS:</th><td>404</td></tr>\\r\\n<tr><th>MESSAGE:</th><td>Not Found</td></tr>\\r\\n<tr><th>SERVLET:</th><td>-</td></tr>\\r\\n</table>\\r\\n<hr><a href=\"http://eclipse.org/jetty\">Powered by Jetty:// 9.4.30.v20200611</a><hr/>\\r\\n\\r\\n</body>\\r\\n</html>\\r\\n```"
            }
          ]
        },
        {
          "issue_url": "https://github.com/eclipse/jetty.project/issues/5290",
          "issue_title": "Programmatically set jetty-web.xml",
          "issue_number": 5290,
          "issue_text": "**Jetty version**\r\n10.0.x\r\n\r\n**Description**\r\nWould be good if there is a possibility to programmatically set a Jetty context XML file that is run _after_ the processing of `web.xml`.\r\nThe use case would be for the deployer of a web application to be able to add or override `init-param`s that depend on the deployment (e.g. the host, the timezone, the geographical location, the language, etc.).\r\n\r\nCurrently this mechanism exists, but it's hardcoded to look for the `WEB-INF/jetty-web.xml` file in the `*.war`.\r\n\r\nThe idea would be to introduce `ContextHandler.setPostProcessXML(Resource r)`, with the semantic that is the resource is present, it is processed as last step in the web application configuration.",
          "issue_comments": [
            {
              "comment_username": "joakime",
              "comment_create_time": "2020-09-17T12:16:41Z",
              "comment_edit_time": "2020-09-17T12:16:41Z",
              "comment_text": "The DeploymentManager has a context attributes configuration that will apply the context attributes for all deployed apps."
            },
            {
              "comment_username": "joakime",
              "comment_create_time": "2020-09-17T12:18:12Z",
              "comment_edit_time": "2020-09-17T12:18:12Z",
              "comment_text": "> (e.g. the host, the timezone, the geographical location, the language, etc.)\r\n\r\nThose would also be great subjects for server level JNDI configuration that the webapp just uses."
            }
          ]
        },
        {
          "issue_url": "https://github.com/eclipse/jetty.project/issues/5289",
          "issue_title": "Review async-demo",
          "issue_number": 5289,
          "issue_text": "**Jetty version**\r\n10.0.0-SNAPSHOT\r\n\r\n```\r\n2020-09-17 09:36:17.530:INFO :oejc.ResponseNotifier:HttpClient@24800026-38: Exception while notifying listener org.eclipse.jetty.example.asyncrest.AsyncRestServlet$1@4ee05075\r\njava.lang.NullPointerException\r\n\tat org.eclipse.jetty.example.asyncrest.AsyncRestServlet$AsyncRestRequest.onComplete(AsyncRestServlet.java:190)\r\n\tat org.eclipse.jetty.client.ResponseNotifier.notifyComplete(ResponseNotifier.java:218)\r\n\tat org.eclipse.jetty.client.ResponseNotifier.notifyComplete(ResponseNotifier.java:210)\r\n\tat org.eclipse.jetty.client.HttpReceiver.terminateResponse(HttpReceiver.java:532)\r\n\tat org.eclipse.jetty.client.HttpReceiver.terminateResponse(HttpReceiver.java:512)\r\n\tat org.eclipse.jetty.client.HttpReceiver.abort(HttpReceiver.java:608)\r\n\tat org.eclipse.jetty.client.HttpReceiver.responseFailure(HttpReceiver.java:504)\r\n\tat org.eclipse.jetty.client.http.HttpReceiverOverHTTP.failAndClose(HttpReceiverOverHTTP.java:414)\r\n\tat org.eclipse.jetty.client.http.HttpReceiverOverHTTP.earlyEOF(HttpReceiverOverHTTP.java:386)\r\n\tat org.eclipse.jetty.http.HttpParser.parseNext(HttpParser.java:1590)\r\n\tat org.eclipse.jetty.client.http.HttpReceiverOverHTTP.shutdown(HttpReceiverOverHTTP.java:274)\r\n\tat org.eclipse.jetty.client.http.HttpReceiverOverHTTP.process(HttpReceiverOverHTTP.java:196)\r\n\tat org.eclipse.jetty.client.http.HttpReceiverOverHTTP.receive(HttpReceiverOverHTTP.java:92)\r\n\tat org.eclipse.jetty.client.http.HttpChannelOverHTTP.receive(HttpChannelOverHTTP.java:95)\r\n\tat org.eclipse.jetty.client.http.HttpConnectionOverHTTP.onFillable(HttpConnectionOverHTTP.java:195)\r\n\tat org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:324)\r\n\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)\r\n\tat org.eclipse.jetty.io.ssl.SslConnection$DecryptedEndPoint.onFillable(SslConnection.java:528)\r\n\tat org.eclipse.jetty.io.ssl.SslConnection.onFillable(SslConnection.java:377)\r\n\tat org.eclipse.jetty.io.ssl.SslConnection$2.succeeded(SslConnection.java:163)\r\n\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)\r\n\tat org.eclipse.jetty.io.SocketChannelEndPoint$1.run(SocketChannelEndPoint.java:106)\r\n\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:338)\r\n\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:315)\r\n\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:173)\r\n\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:131)\r\n\tat org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:375)\r\n\tat org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:790)\r\n\tat org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:912)\r\n\tat java.base/java.lang.Thread.run(Thread.java:832)\r\n```\r\n\r\n",
          "issue_comments": [
            {
              "comment_username": "gregw",
              "comment_create_time": "2020-09-17T07:42:31Z",
              "comment_edit_time": "2020-09-17T07:42:31Z",
              "comment_text": "@lachlan-roberts can you try to reproduce???? This appears for me in about 1 in 10 reloads of the page.\r\nLooking at the code, I do not understand how it could even happen?!?!?"
            },
            {
              "comment_username": "gregw",
              "comment_create_time": "2020-09-17T12:54:28Z",
              "comment_edit_time": "2020-09-17T12:54:28Z",
              "comment_text": "... and it feels rather slow as well???"
            },
            {
              "comment_username": "gregw",
              "comment_create_time": "2020-09-21T12:35:18Z",
              "comment_edit_time": "2020-09-21T12:35:18Z",
              "comment_text": "I think this was just ebay having problems.  We could perhaps improve the error handling, but for now this doesn't need to be fixed in a hurry.\r\nPerhaps reivew, refresh and update at some stage."
            }
          ]
        }
      ]
    }
  },
  {
    "github_url": "https://github.com/netty/netty",
    "github_info": {
      "name": "netty/netty",
      "language": "Java"
    },
    "github_branches": {
      "branch_datas": [
        {
          "branch_version": "3.2",
          "branch_url": "https://github.com/netty/netty/tree/3.2",
          "branch_download_url": "https://github.com/netty/netty/archive/3.2.zip"
        },
        {
          "branch_version": "3.5",
          "branch_url": "https://github.com/netty/netty/tree/3.5",
          "branch_download_url": "https://github.com/netty/netty/archive/3.5.zip"
        },
        {
          "branch_version": "3.6",
          "branch_url": "https://github.com/netty/netty/tree/3.6",
          "branch_download_url": "https://github.com/netty/netty/archive/3.6.zip"
        },
        {
          "branch_version": "3.7",
          "branch_url": "https://github.com/netty/netty/tree/3.7",
          "branch_download_url": "https://github.com/netty/netty/archive/3.7.zip"
        },
        {
          "branch_version": "3.8",
          "branch_url": "https://github.com/netty/netty/tree/3.8",
          "branch_download_url": "https://github.com/netty/netty/archive/3.8.zip"
        },
        {
          "branch_version": "3.9",
          "branch_url": "https://github.com/netty/netty/tree/3.9",
          "branch_download_url": "https://github.com/netty/netty/archive/3.9.zip"
        },
        {
          "branch_version": "3.10",
          "branch_url": "https://github.com/netty/netty/tree/3.10",
          "branch_download_url": "https://github.com/netty/netty/archive/3.10.zip"
        },
        {
          "branch_version": "4.0",
          "branch_url": "https://github.com/netty/netty/tree/4.0",
          "branch_download_url": "https://github.com/netty/netty/archive/4.0.zip"
        },
        {
          "branch_version": "4.1",
          "branch_url": "https://github.com/netty/netty/tree/4.1",
          "branch_download_url": "https://github.com/netty/netty/archive/4.1.zip"
        },
        {
          "branch_version": "channels",
          "branch_url": "https://github.com/netty/netty/tree/channels",
          "branch_download_url": "https://github.com/netty/netty/archive/channels.zip"
        },
        {
          "branch_version": "compile_workaround_iouring",
          "branch_url": "https://github.com/netty/netty/tree/compile_workaround_iouring",
          "branch_download_url": "https://github.com/netty/netty/archive/compile_workaround_iouring.zip"
        },
        {
          "branch_version": "drop-npn",
          "branch_url": "https://github.com/netty/netty/tree/drop-npn",
          "branch_download_url": "https://github.com/netty/netty/archive/drop-npn.zip"
        },
        {
          "branch_version": "existing_stream",
          "branch_url": "https://github.com/netty/netty/tree/existing_stream",
          "branch_download_url": "https://github.com/netty/netty/archive/existing_stream.zip"
        },
        {
          "branch_version": "fix_prop_name",
          "branch_url": "https://github.com/netty/netty/tree/fix_prop_name",
          "branch_download_url": "https://github.com/netty/netty/archive/fix_prop_name.zip"
        },
        {
          "branch_version": "handler_removed",
          "branch_url": "https://github.com/netty/netty/tree/handler_removed",
          "branch_download_url": "https://github.com/netty/netty/archive/handler_removed.zip"
        },
        {
          "branch_version": "ignore_header_data_wip",
          "branch_url": "https://github.com/netty/netty/tree/ignore_header_data_wip",
          "branch_download_url": "https://github.com/netty/netty/archive/ignore_header_data_wip.zip"
        },
        {
          "branch_version": "io-uring",
          "branch_url": "https://github.com/netty/netty/tree/io-uring",
          "branch_download_url": "https://github.com/netty/netty/archive/io-uring.zip"
        },
        {
          "branch_version": "master_deprecated",
          "branch_url": "https://github.com/netty/netty/tree/master_deprecated",
          "branch_download_url": "https://github.com/netty/netty/archive/master_deprecated.zip"
        },
        {
          "branch_version": "master_with_aio_transport",
          "branch_url": "https://github.com/netty/netty/tree/master_with_aio_transport",
          "branch_download_url": "https://github.com/netty/netty/archive/master_with_aio_transport.zip"
        },
        {
          "branch_version": "master",
          "branch_url": "https://github.com/netty/netty/tree/master",
          "branch_download_url": "https://github.com/netty/netty/archive/master.zip"
        },
        {
          "branch_version": "unlink_handler",
          "branch_url": "https://github.com/netty/netty/tree/unlink_handler",
          "branch_download_url": "https://github.com/netty/netty/archive/unlink_handler.zip"
        },
        {
          "branch_version": "writability",
          "branch_url": "https://github.com/netty/netty/tree/writability",
          "branch_download_url": "https://github.com/netty/netty/archive/writability.zip"
        },
        {
          "branch_version": "writev_iouring",
          "branch_url": "https://github.com/netty/netty/tree/writev_iouring",
          "branch_download_url": "https://github.com/netty/netty/archive/writev_iouring.zip"
        }
      ]
    },
    "github_pull_requests": {
      "pull_datas": [
        {
          "pull_number": 10583,
          "pull_title": "Add validation check about websocket path",
          "pull_version": "netty:4.1",
          "pull_version_url": "https://github.com/netty/netty/tree/4.1"
        },
        {
          "pull_number": 10548,
          "pull_title": "Use SQPOLL mode & Kernel 5.10",
          "pull_version": "netty:io-uring",
          "pull_version_url": "https://github.com/netty/netty/tree/io-uring"
        },
        {
          "pull_number": 10530,
          "pull_title": "Reduce DefaultAttributeMap cost",
          "pull_version": "netty:4.1",
          "pull_version_url": "https://github.com/netty/netty/tree/4.1"
        },
        {
          "pull_number": 10527,
          "pull_title": "Add `null` rule check in `rules` array of RuleBasedIpFilter",
          "pull_version": "netty:4.1",
          "pull_version_url": "https://github.com/netty/netty/tree/4.1"
        },
        {
          "pull_number": 10486,
          "pull_title": "DNS-over-HTTPS (DoH) Support",
          "pull_version": "netty:4.1",
          "pull_version_url": "https://github.com/netty/netty/tree/4.1"
        },
        {
          "pull_number": 10432,
          "pull_title": "Track memory attributes to avoid memory leak",
          "pull_version": "netty:4.1",
          "pull_version_url": "https://github.com/netty/netty/tree/4.1"
        },
        {
          "pull_number": 10422,
          "pull_title": "[Feature]Add zstd encoder and decoder",
          "pull_version": "netty:4.1",
          "pull_version_url": "https://github.com/netty/netty/tree/4.1"
        },
        {
          "pull_number": 10382,
          "pull_title": "The CorsHandler should allow requests with same origin",
          "pull_version": "netty:4.1",
          "pull_version_url": "https://github.com/netty/netty/tree/4.1"
        },
        {
          "pull_number": 10380,
          "pull_title": "Fix `isOriginForm` always return `false` in `HttpUtil`",
          "pull_version": "netty:4.1",
          "pull_version_url": "https://github.com/netty/netty/tree/4.1"
        },
        {
          "pull_number": 10234,
          "pull_title": "Optimize stomp encoder",
          "pull_version": "netty:4.1",
          "pull_version_url": "https://github.com/netty/netty/tree/4.1"
        },
        {
          "pull_number": 10228,
          "pull_title": "zlib: use ByteBuffer inflater/deflater methods when possible",
          "pull_version": "netty:4.1",
          "pull_version_url": "https://github.com/netty/netty/tree/4.1"
        },
        {
          "pull_number": 10218,
          "pull_title": "DnsNameResolver: dns cache fallback",
          "pull_version": "netty:4.1",
          "pull_version_url": "https://github.com/netty/netty/tree/4.1"
        },
        {
          "pull_number": 10208,
          "pull_title": "Improve performance for Unpooled.copiedBuffer",
          "pull_version": "netty:4.1",
          "pull_version_url": "https://github.com/netty/netty/tree/4.1"
        },
        {
          "pull_number": 10091,
          "pull_title": "Add TransportSelector which helps users to select the optimal transpo…",
          "pull_version": "netty:4.1",
          "pull_version_url": "https://github.com/netty/netty/tree/4.1"
        },
        {
          "pull_number": 9989,
          "pull_title": "Add missing GraalVM substitutions for full native image support.",
          "pull_version": "netty:4.1",
          "pull_version_url": "https://github.com/netty/netty/tree/4.1"
        },
        {
          "pull_number": 9953,
          "pull_title": "Existing stream",
          "pull_version": "netty:4.1",
          "pull_version_url": "https://github.com/netty/netty/tree/4.1"
        },
        {
          "pull_number": 9942,
          "pull_title": "DecodeHexBenchmark is too branch-predictor friendly",
          "pull_version": "netty:4.1",
          "pull_version_url": "https://github.com/netty/netty/tree/4.1"
        },
        {
          "pull_number": 9935,
          "pull_title": "Let `EmbeddedChannel` support autoRead.",
          "pull_version": "netty:4.1",
          "pull_version_url": "https://github.com/netty/netty/tree/4.1"
        },
        {
          "pull_number": 9922,
          "pull_title": "Cooperative ByteToMessageDecoder (WIP)",
          "pull_version": "netty:4.1",
          "pull_version_url": "https://github.com/netty/netty/tree/4.1"
        },
        {
          "pull_number": 9892,
          "pull_title": "Make ResourceLeakDetector also emit recent access records for IllegalReferenceCountException",
          "pull_version": "netty:4.1",
          "pull_version_url": "https://github.com/netty/netty/tree/4.1"
        },
        {
          "pull_number": 9878,
          "pull_title": "ReadOnlyByteBuf writable bytes",
          "pull_version": "netty:4.1",
          "pull_version_url": "https://github.com/netty/netty/tree/4.1"
        },
        {
          "pull_number": 9847,
          "pull_title": "Never unset EPOLLOUT interest flag",
          "pull_version": "netty:master",
          "pull_version_url": "https://github.com/netty/netty/tree/master"
        },
        {
          "pull_number": 9843,
          "pull_title": "Introduce ByteBuf#capacityAndDiscard(int) method",
          "pull_version": "netty:4.1",
          "pull_version_url": "https://github.com/netty/netty/tree/4.1"
        },
        {
          "pull_number": 9777,
          "pull_title": "[WIP] ChannelHandler for adding Flight Recorder events",
          "pull_version": "netty:4.1",
          "pull_version_url": "https://github.com/netty/netty/tree/4.1"
        },
        {
          "pull_number": 9753,
          "pull_title": "ChunkedWriteHandler::queue should account outbound pending bytes",
          "pull_version": "netty:4.1",
          "pull_version_url": "https://github.com/netty/netty/tree/4.1"
        },
        {
          "pull_number": 9751,
          "pull_title": "Optimize ByteToMessageDecoder with batch decoder",
          "pull_version": "netty:4.1",
          "pull_version_url": "https://github.com/netty/netty/tree/4.1"
        },
        {
          "pull_number": 9681,
          "pull_title": "TCP Client Fast Open",
          "pull_version": "netty:4.1",
          "pull_version_url": "https://github.com/netty/netty/tree/4.1"
        },
        {
          "pull_number": 9672,
          "pull_title": "Correctly clean up memory attributes and file uploads",
          "pull_version": "netty:4.1",
          "pull_version_url": "https://github.com/netty/netty/tree/4.1"
        },
        {
          "pull_number": 9590,
          "pull_title": "Reinstate decoupled timerfd logic in EpollEventLoop",
          "pull_version": "netty:4.1",
          "pull_version_url": "https://github.com/netty/netty/tree/4.1"
        },
        {
          "pull_number": 9265,
          "pull_title": "POC: Tag-team event loop",
          "pull_version": "netty:4.1",
          "pull_version_url": "https://github.com/netty/netty/tree/4.1"
        }
      ]
    },
    "github_issues": {
      "issue_datas": [
        {
          "issue_url": "https://github.com/netty/netty/issues/10616",
          "issue_title": "native-image compilation of epoll transport fails",
          "issue_number": 10616,
          "issue_text": "### Expected behavior\\r\\nMy small repro compiles as a native image and works as expected if I use Nio* family of transports. My expectation was that switching to epoll transport would similarly work.\\r\\n\\r\\n### Actual behavior\\r\\nIf I switch to epoll (linux-x86_64) I get a compilation error:\\r\\n\\r\\n```\\r\\nError: Classes that should be initialized at run time got initialized during image building:\\r\\n io.netty.util.AbstractReferenceCounted the class was requested to be initialized at build time (from the command line). io.netty.util.AbstractReferenceCounted has been initialized without the native-image initialization instrumentation and the stack trace can't be tracked. Try avoiding to initialize the class that caused initialization of io.netty.util.AbstractReferenceCounted\\r\\n\\r\\ncom.oracle.svm.core.util.UserError$UserException: Classes that should be initialized at run time got initialized during image building:\\r\\n io.netty.util.AbstractReferenceCounted the class was requested to be initialized at build time (from the command line). io.netty.util.AbstractReferenceCounted has been initialized without the native-image initialization instrumentation and the stack trace can't be tracked. Try avoiding to initialize the class that caused initialization of io.netty.util.AbstractReferenceCounted\\r\\n\\r\\n\\tat com.oracle.svm.core.util.UserError.abort(UserError.java:65)\\r\\n\\tat com.oracle.svm.hosted.classinitialization.ConfigurableClassInitialization.checkDelayedInitialization(ConfigurableClassInitialization.java:510)\\r\\n\\tat com.oracle.svm.hosted.classinitialization.ClassInitializationFeature.duringAnalysis(ClassInitializationFeature.java:187)\\r\\n\\tat com.oracle.svm.hosted.NativeImageGenerator.lambda$runPointsToAnalysis$8(NativeImageGenerator.java:710)\\r\\n\\tat com.oracle.svm.hosted.FeatureHandler.forEachFeature(FeatureHandler.java:63)\\r\\n\\tat com.oracle.svm.hosted.NativeImageGenerator.runPointsToAnalysis(NativeImageGenerator.java:710)\\r\\n\\tat com.oracle.svm.hosted.NativeImageGenerator.doRun(NativeImageGenerator.java:530)\\r\\n\\tat com.oracle.svm.hosted.NativeImageGenerator.lambda$run$0(NativeImageGenerator.java:445)\\r\\n\\tat java.base/java.util.concurrent.ForkJoinTask$AdaptedRunnableAction.exec(ForkJoinTask.java:1407)\\r\\n\\tat java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:290)\\r\\n\\tat java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1020)\\r\\n\\tat java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1656)\\r\\n\\tat java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1594)\\r\\n\\tat java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:177)\\r\\n```\\r\\n\\r\\n### Minimal yet complete reproducer code (or URL to code)\\r\\n\\r\\nHere's a repro: https://github.com/perezd/netty-epoll-native-repro\\r\\n\\r\\nw/ Bazel 3.5.0 installed, run this command from the root of the project:\\r\\n\\r\\n```\\r\\nbazel run :main-native\\r\\n```\\r\\n\\r\\n### Netty version\\r\\n4.1.52.Final (linux-x86_64)\\r\\n\\r\\n### JVM version (e.g. `java -version`)\\r\\njava -version\\r\\nopenjdk version \"11.0.8\" 2020-07-14\\r\\nOpenJDK Runtime Environment (build 11.0.8+10-post-Ubuntu-0ubuntu120.04)\\r\\nOpenJDK 64-Bit Server VM (build 11.0.8+10-post-Ubuntu-0ubuntu120.04, mixed mode, sharing)\\r\\n\\r\\n### OS version (e.g. `uname -a`)\\r\\nLinux desktop 5.4.0-48-generic #52-Ubuntu SMP Thu Sep 10 10:58:49 UTC 2020 x86_64 x86_64 x86_64 GNU/Linux\\r\\n\\r\\nI've searched around and attempted various workarounds but nothing seems to work.\\r\\n",
          "issue_comments": [
            {
              "comment_username": "chrisvest",
              "comment_create_time": "2020-09-28T08:11:42Z",
              "comment_edit_time": "2020-09-28T08:11:42Z",
              "comment_text": "Can you add instructions for building and running your repro?"
            },
            {
              "comment_username": "perezd",
              "comment_create_time": "2020-09-28T16:03:16Z",
              "comment_edit_time": "2020-09-28T16:03:16Z",
              "comment_text": "Sorry yes, updated comment."
            },
            {
              "comment_username": "chrisvest",
              "comment_create_time": "2020-09-29T14:42:54Z",
              "comment_edit_time": "2020-09-29T14:45:13Z",
              "comment_text": "@perezd How do you make the repro use a Netty version that's built and installed into the local `~/.m2/` repository? I'm trying to make it use a local `4.1.53.Final-SNAPSHOT` version, but just changing the `NETTY_VERSION` constant in the `WORKSPACE` file is not enough."
            }
          ]
        },
        {
          "issue_url": "https://github.com/netty/netty/issues/10614",
          "issue_title": "UnorderedThreadPoolEventExecutor#scheduleAtFixedRate not working as expected",
          "issue_number": 10614,
          "issue_text": "### Expected behavior\r\nThe runnable passed to scheduleAtFixedRate() should run multiple times, not just once.\r\n\r\n### Actual behavior\r\nusing UnorderedThreadPoolEventExecutor the runnable is executed just once, using directly the ScheduledThreadPoolExecutor or DefaultEventExecutorGroup the task is executed at fixed rate as expected\r\n\r\n### Minimal yet complete reproducer code \r\n```java\r\npublic static void main(final String[] args) throws Exception {\r\n  final UnorderedThreadPoolEventExecutor executor = new UnorderedThreadPoolEventExecutor(2);\r\n  //final ScheduledThreadPoolExecutor executor = new ScheduledThreadPoolExecutor(2); // THIS WORKS\r\n  //final DefaultEventExecutorGroup executor = new DefaultEventExecutorGroup(2); // THIS WORKS\r\n  executor.scheduleAtFixedRate(() -> System.out.println(\"XXX\"), 1, 1, TimeUnit.SECONDS);\r\n  executor.awaitTermination(10, TimeUnit.SECONDS); // let it run...\r\n}\r\n```\r\n\r\n### Netty version\r\n4.1.52.Final \r\n\r\n### JVM version\r\nlatest OpenJDK 11 and 15\r\n",
          "issue_comments": []
        },
        {
          "issue_url": "https://github.com/netty/netty/issues/10607",
          "issue_title": "Websocket handshaker failed for  sec-websocket-version can't be recognized ",
          "issue_number": 10607,
          "issue_text": "### Expected behavior\\r\\nRequest's Header:\\r\\n\\r\\n> GET https:l /game.weplaycn.comf9001/websocket HTTF/1.1\\r\\nHost: game.weplayen.com\\r\\nonnection: Upgrade\\r\\nFragma: no-cache\\r\\nache-Control: no-cache\\r\\nUpgrade: websocket\\r\\n0rigin: https:/fres.weplaycn.com\\r\\nSec-WebSocket-Version: 13\\r\\nAccept-Encoding: gzip, deflate\\r\\nAccept-Language: zh-CH,zh;q-0.8,en-US ; q-0.6,en;q-0.5;q-0.4\\r\\nSec-WebSocket一Key: g2VPrctBwjppDsfDGi dBoA—\\r\\nSec-WebSocket-Extensions: permessage-deflate; client_max_window_bits\\r\\n\\r\\nResponse's header:\\r\\n\\r\\n> HTTP/1.1 101 Switching Protocols\\r\\nServer: nginx1.8.0\\r\\nDate: Thu,24 Sep 2020 12:57:0 GMr\\r\\nConnection: upgrade\\r\\nupgrade: websocket\\r\\nSec-WebSocket-Accept: +ckML/qljT1PEuAxOntWh49nBmE\\r\\nccess-Control-Allow-Origin: *\\r\\n\\r\\n### Actual behavior\\r\\nRequest's Header:\\r\\n\\r\\n> GET /websocket HTTP/1.1\\r\\nhost: 47.99.67.129:5131\\r\\nupgrade: websocket\\r\\nconnection: upgrade\\r\\nsec-websocket-key: pD7zzfEba9wxlH9tBoIX3A==\\r\\norigin: http://47.99.67.129:5131\\r\\n**sec-websocket-version**: 13\\r\\n\\r\\nResponse's header:\\r\\n> DefaultHttpResponse(decodeResult: success, version: HTTP/1.1)\\r\\nHTTP/1.1 400 Bad Request\\r\\n\\r\\n\\r\\n\\r\\n### Steps to reproduce\\r\\nthe root cause is as following : \\r\\nin other programming language ,for instance. JS or php ,the websocket hand shaker protocol ‘s only think the parameter\"**Sec-WebSocket-Version**\" is legal, \\r\\nBut in netty version for '4.1.52.Final',the parameter has been changed to \"sec-websocket-version\", it cause some application can't work OK now. it works ok before.I think  this version upgrade is go down.\\r\\nyou can use php as server,netty as client network framework,or use js as client,netty as server network framework,this bug is reproduced  easily.\\r\\n\\r\\n### Minimal yet complete reproducer code (or URL to code)\\r\\nfor compatibility,this code should be modified as follows:\\r\\nclient side:\\r\\n[io.netty.handler.codec.http.websocketx.WebSocketClientHandshaker13.newHandshakeRequest()](https://github.com/netty/netty/blob/4b22d8ab352912e9ac1217f51ecbff68c5b99572/codec-http/src/main/java/io/netty/handler/codec/http/websocketx/WebSocketClientHandshaker13.java#L200)\\r\\n\\r\\n\\r\\nserver side:\\r\\n[io.netty.handler.codec.http.websocketx.WebSocketServerHandshakerFactory.newHandshaker(HttpRequest)](https://github.com/netty/netty/blob/a4276e8dffd42ea7b990e6707c11c2c2037d2d82/codec-http/src/main/java/io/netty/handler/codec/http/websocketx/WebSocketServerHandshakerFactory.java#L129)\\r\\n\\r\\n### Netty version\\r\\n4.1.52.Final\\r\\n### JVM version (e.g. `java -version`)\\r\\nOpenJDK Runtime Environment (Alibaba 8.0.0) (build 1.8.0_66-b60)\\r\\n### OS version (e.g. `uname -a`)\\r\\nwin10 ",
          "issue_comments": [
            {
              "comment_username": "normanmaurer",
              "comment_create_time": "2020-09-24T13:59:27Z",
              "comment_edit_time": "2020-09-24T13:59:27Z",
              "comment_text": "@jiangguilong2000 this is bug with the other library as the RFC clearly state that header names are **not** case sensitive. "
            },
            {
              "comment_username": "jiangguilong2000",
              "comment_create_time": "2020-09-24T14:12:53Z",
              "comment_edit_time": "2020-09-25T03:10:05Z",
              "comment_text": "> @jiangguilong2000 this is bug with the other library as the RFC clearly state that header names are **not** case sensitive.\\r\\n\\r\\nas ur opinion,this code is not right:\\r\\n[io.netty.handler.codec.http.websocketx.WebSocketServerHandshakerFactory.newHandshaker(HttpRequest)](https://github.com/netty/netty/blob/a4276e8dffd42ea7b990e6707c11c2c2037d2d82/codec-http/src/main/java/io/netty/handler/codec/http/websocketx/WebSocketServerHandshakerFactory.java#L131)\\r\\n\\r\\n`CharSequence version = req.headers().get(HttpHeaderNames.SEC_WEBSOCKET_VERSION);`\\r\\n\\r\\nif client http head contains \"Sec-WebSocket-Version\",it also can't been recognized. for the server side's key value is \"sec-websocket-version\"\\r\\n@normanmaurer "
            },
            {
              "comment_username": "slandelle",
              "comment_create_time": "2020-09-24T14:15:36Z",
              "comment_edit_time": "2020-09-24T14:15:36Z",
              "comment_text": "@jiangguilong2000 Nah. HttpHeaders#get is case insensitive."
            }
          ]
        },
        {
          "issue_url": "https://github.com/netty/netty/issues/10604",
          "issue_title": "ClientCookieEncoder encode cookie unnecessary stripTrailingSeparator",
          "issue_number": 10604,
          "issue_text": "### Expected behavior\r\nthe cookie should be encoded as expected \r\nname=value\r\n\r\n### Actual behavior\r\nthe result string is missing two last characters\r\nname=val\r\n\r\n### Steps to reproduce\r\nClientCookieEncoder encoder;\r\n... skip new encoder initialization\r\nString result = encoder.encode(new DefaultCookie(\"name\", \"value\"));\r\n\r\n\r\n### Netty version\r\n4.1\r\n\r\n### JVM version (e.g. `java -version`)\r\njdk-11\r\n\r\n### OS version (e.g. `uname -a`)\r\nany",
          "issue_comments": [
            {
              "comment_username": "slandelle",
              "comment_create_time": "2020-09-23T16:18:15Z",
              "comment_edit_time": "2020-09-23T16:18:15Z",
              "comment_text": "Couldn't reproduce. Please provide a **full sample**."
            }
          ]
        },
        {
          "issue_url": "https://github.com/netty/netty/issues/10599",
          "issue_title": "What's the use of function addToVariablesToRemove() in FastLocalThread",
          "issue_number": 10599,
          "issue_text": "### Expected behavior\r\n\r\nI just what to know the use of function addToVariablesToRemove() in FastLocalThread , I think  variable index can as a identity of FastLocalThread. \r\n\r\n\r\n/*\r\n private static void addToVariablesToRemove(InternalThreadLocalMap threadLocalMap, FastThreadLocal<?> variable) {\r\n\r\n        Object v = threadLocalMap.indexedVariable(variablesToRemoveIndex);\r\n\r\n        Set<FastThreadLocal<?>> variablesToRemove;\r\n\r\n        if (v == InternalThreadLocalMap.UNSET || v == null) {\r\n\r\n            variablesToRemove = Collections.newSetFromMap(new IdentityHashMap<FastThreadLocal<?>, Boolean>());\r\n\r\n            threadLocalMap.setIndexedVariable(variablesToRemoveIndex, variablesToRemove);\r\n\r\n        } else {\r\n\r\n            variablesToRemove = (Set<FastThreadLocal<?>>) v;\r\n\r\n        }\r\n\r\n        variablesToRemove.add(variable);\r\n\r\n    }\r\n\r\n*/\r\n\r\nhope for your reply\r\n### Actual behavior\r\n\r\n### Steps to reproduce\r\n\r\n### Minimal yet complete reproducer code (or URL to code)\r\n\r\n### Netty version\r\n\r\n### JVM version (e.g. `java -version`)\r\n\r\n### OS version (e.g. `uname -a`)\r\n",
          "issue_comments": []
        },
        {
          "issue_url": "https://github.com/netty/netty/issues/10590",
          "issue_title": "A lot of log will be output and consumed a lot of CPU after network is disconnected on Android devices",
          "issue_number": 10590,
          "issue_text": "I have a trouble about netty when I use micronaut on Android. And I reported an issue to micronaut(https://github.com/micronaut-projects/micronaut-core/issues/4129). But they tell me that I should report the issue to netty.\r\n\r\n### Expected behavior\r\nMicronaut server stopped normally.\r\n\r\n### Actual behavior\r\nMicronaut server will be stopped, but a lot of log will be output. And this will take up a lot of CPU resource.\r\nLog:\r\n```\r\n [nioEventLoopGroup-1-1] WARN io.netty.channel.nio.NioEventLoop - Selector.select() returned prematurely 512 times in a row; rebuilding Selector io.netty.channel.nio.SelectedSelectionKeySetSelector@70c8d50.\r\n2020-09-18 14:58:18.405 25846-25923/com.example.androidwebtest I/System.out: 14:58:18.405 [nioEventLoopGroup-1-1] INFO io.netty.channel.nio.NioEventLoop - Migrated 1 channel(s) to the new Selector.\r\n2020-09-18 14:58:18.406 25846-25923/com.example.androidwebtest I/System.out: 14:58:18.406 [nioEventLoopGroup-1-1] WARN io.netty.channel.nio.NioEventLoop - Selector.select() returned prematurely 512 times in a row; rebuilding Selector io.netty.channel.nio.SelectedSelectionKeySetSelector@1a42c4e.\r\n2020-09-18 14:58:18.407 25846-25923/com.example.androidwebtest I/System.out: 14:58:18.407 [nioEventLoopGroup-1-1] INFO io.netty.channel.nio.NioEventLoop - Migrated 1 channel(s) to the new Selector.\r\n2020-09-18 14:58:18.409 25846-25923/com.example.androidwebtest I/System.out: 14:58:18.408 [nioEventLoopGroup-1-1] WARN io.netty.channel.nio.NioEventLoop - Selector.select() returned prematurely 512 times in a row; rebuilding Selector io.netty.channel.nio.SelectedSelectionKeySetSelector@8352e7c.\r\n2020-09-18 14:58:18.409 25846-25923/com.example.androidwebtest I/System.out: 14:58:18.409 [nioEventLoopGroup-1-1] INFO io.netty.channel.nio.NioEventLoop - Migrated 1 channel(s) to the new Selector.\r\n2020-09-18 14:58:18.411 25846-25923/com.example.androidwebtest I/System.out: 14:58:18.410 [nioEventLoopGroup-1-1] WARN io.netty.channel.nio.NioEventLoop - Selector.select() returned prematurely 512 times in a row; rebuilding Selector io.netty.channel.nio.SelectedSelectionKeySetSelector@21abf5a.\r\n2020-09-18 14:58:18.411 25846-25923/com.example.androidwebtest I/System.out: 14:58:18.411 [nioEventLoopGroup-1-1] INFO io.netty.channel.nio.NioEventLoop - Migrated 1 channel(s) to the new Selector.\r\n2020-09-18 14:58:18.413 25846-25923/com.example.androidwebtest I/System.out: 14:58:18.413 [nioEventLoopGroup-1-1] WARN io.netty.channel.nio.NioEventLoop - Selector.select() returned prematurely 512 times in a row; rebuilding Selector io.netty.channel.nio.SelectedSelectionKeySetSelector@2e4b668.\r\n2020-09-18 14:58:18.413 25846-25923/com.example.androidwebtest I/System.out: 14:58:18.413 [nioEventLoopGroup-1-1] INFO io.netty.channel.nio.NioEventLoop - Migrated 1 channel(s) to the new Selector.\r\n2020-09-18 14:58:18.415 25846-25923/com.example.androidwebtest I/System.out: 14:58:18.414 [nioEventLoopGroup-1-1] WARN io.netty.channel.nio.NioEventLoop - Selector.select() returned prematurely 512 times in a row; rebuilding Selector io.netty.channel.nio.SelectedSelectionKeySetSelector@7357726.\r\n2020-09-18 14:58:18.415 25846-25923/com.example.androidwebtest I/System.out: 14:58:18.415 [nioEventLoopGroup-1-1] INFO io.netty.channel.nio.NioEventLoop - Migrated 1 channel(s) to the new Selector.\r\n2020-09-18 14:58:18.417 25846-25923/com.example.androidwebtest I/System.out: 14:58:18.417 [nioEventLoopGroup-1-1] WARN io.netty.channel.nio.NioEventLoop - Selector.select() returned prematurely 512 times in a row; rebuilding Selector io.netty.channel.nio.SelectedSelectionKeySetSelector@482d114.\r\n2020-09-18 14:58:18.417 25846-25923/com.example.androidwebtest I/System.out: 14:58:18.417 [nioEventLoopGroup-1-1] INFO io.netty.channel.nio.NioEventLoop - Migrated 1 channel(s) to the new Selector.\r\n......\r\n```\r\nCPU:\r\n```\r\nTasks: 785 total,   4 running, 780 sleeping,   0 stopped,   1 zombie\r\n  Mem:      5.4G total,      5.3G used,      116M free,      175M buffers\r\n Swap:      2.5G total,      498M used,      2.0G free,      3.0G cached\r\n800%cpu  81%user  14%nice 196%sys 490%idle   0%iow  16%irq   3%sirq   0%host\r\n  PID USER         PR  NI VIRT  RES  SHR S[%CPU] %MEM     TIME+ ARGS\r\n25846 u0_a219      10 -10 5.3G 130M  79M S 99.3   2.3   0:15.86 com.example.androidwebtest\r\n```\r\n\r\n### Steps to reproduce\r\n1. Start micronaut and listen the IP of WiFi, but not listen \"0.0.0.0\".\r\n2. Disconect WiFi.\r\n\r\n### Minimal yet complete reproducer code (or URL to code)\r\nhttps://github.com/igiantpanda/AndroidWebTest/tree/disconnect-issue\r\n\r\n### Netty version\r\n4.1.48.Final (The corresponding version of Micronaut is 2.0.0)\r\n\r\n### JVM version (e.g. `java -version`)\r\n1.8\r\n\r\n### OS version (e.g. `uname -a`)\r\nAndroid 10 (Tested on Android emulator and OPPO Reno)\r\n\r\n",
          "issue_comments": [
            {
              "comment_username": "hyperxpro",
              "comment_create_time": "2020-09-19T05:28:29Z",
              "comment_edit_time": "2020-09-19T05:28:29Z",
              "comment_text": "Turn off logging for `io.netty.channel.nio.NioEventLoop`"
            },
            {
              "comment_username": "grober82",
              "comment_create_time": "2020-09-19T07:47:05Z",
              "comment_edit_time": "2020-09-19T07:47:05Z",
              "comment_text": "> Turn off logging for `io.netty.channel.nio.NioEventLoop`\r\n\r\nI hope that this is a joke. There must be a bug in Netty that causes these messages in the first place."
            },
            {
              "comment_username": "hyperxpro",
              "comment_create_time": "2020-09-19T07:48:59Z",
              "comment_edit_time": "2020-09-19T07:48:59Z",
              "comment_text": "> > Turn off logging for `io.netty.channel.nio.NioEventLoop`\r\n> \r\n> I hope that this is a joke. There must be a bug in Netty that causes these messages in the first place.\r\n\r\nThere is no bug in Netty but in Java NIO. So apart from turn off logging, you can switch to Epoll mechanism. Else, no other option. :)"
            },
            {
              "comment_username": "igiantpanda",
              "comment_create_time": "2020-09-19T07:49:48Z",
              "comment_edit_time": "2020-09-19T07:49:48Z",
              "comment_text": "> \r\n> \r\n> Turn off logging for `io.netty.channel.nio.NioEventLoop`\r\n\r\nI'm a beginner, could you tell me how to turn off logging for `io.netty.channel.nio.NioEventLoop`?\r\nThanks."
            },
            {
              "comment_username": "normanmaurer",
              "comment_create_time": "2020-09-19T07:50:46Z",
              "comment_edit_time": "2020-09-19T07:50:46Z",
              "comment_text": "I will have a look but this seems to be caused by a bug in the nio implementation of android as this should really never happen "
            },
            {
              "comment_username": "hyperxpro",
              "comment_create_time": "2020-09-19T08:51:55Z",
              "comment_edit_time": "2020-09-19T08:51:55Z",
              "comment_text": "> > Turn off logging for `io.netty.channel.nio.NioEventLoop`\r\n> \r\n> I'm a beginner, could you tell me how to turn off logging for `io.netty.channel.nio.NioEventLoop`?\r\n> Thanks.\r\n\r\nDepends on what logger you're using."
            },
            {
              "comment_username": "igiantpanda",
              "comment_create_time": "2020-09-21T03:32:21Z",
              "comment_edit_time": "2020-09-21T03:32:21Z",
              "comment_text": "> \r\n> \r\n> > > Turn off logging for `io.netty.channel.nio.NioEventLoop`\r\n> > \r\n> > \r\n> > I'm a beginner, could you tell me how to turn off logging for `io.netty.channel.nio.NioEventLoop`?\r\n> > Thanks.\r\n> \r\n> Depends on what logger you're using.\r\n\r\nAfter I turn off logging for `io.netty.channel.nio.NioEventLoop`, I also found a lot of the follow exception. And the exception is existed before I turn off logging for `io.netty.channel.nio.NioEventLoop`.\r\nAnd I tried to turn off logging for `io.netty.channel.DefaultChannelPipeline`, then the exception log will not output.\r\nBut the application still consumes a lot of CPU resource, even though I stopped or restarted the micronaut web server.\r\n```\r\n......\r\n2020-09-21 11:18:21.494 10931-11005/com.example.androidwebtest I/System.out: 03:18:21.494 [nioEventLoopGroup-1-1] WARN io.netty.channel.DefaultChannelPipeline - An exceptionCaught() event was fired, and it reached at the tail of the pipeline. It usually means the last handler in the pipeline did not handle the exception.\r\n2020-09-21 11:18:21.494 10931-11005/com.example.androidwebtest I/System.out: java.io.IOException: Invalid argument\r\n2020-09-21 11:18:21.494 10931-11005/com.example.androidwebtest I/System.out:     at sun.nio.ch.ServerSocketChannelImpl.accept0(Native Method)\r\n2020-09-21 11:18:21.495 10931-11005/com.example.androidwebtest I/System.out:     at sun.nio.ch.ServerSocketChannelImpl.accept(ServerSocketChannelImpl.java:422)\r\n2020-09-21 11:18:21.495 10931-11005/com.example.androidwebtest I/System.out:     at sun.nio.ch.ServerSocketChannelImpl.accept(ServerSocketChannelImpl.java:250)\r\n2020-09-21 11:18:21.495 10931-11005/com.example.androidwebtest I/System.out:     at io.netty.util.internal.SocketUtils$5.run(SocketUtils.java:119)\r\n2020-09-21 11:18:21.495 10931-11005/com.example.androidwebtest I/System.out:     at io.netty.util.internal.SocketUtils$5.run(SocketUtils.java:116)\r\n2020-09-21 11:18:21.495 10931-11005/com.example.androidwebtest I/System.out:     at java.security.AccessController.doPrivileged(AccessController.java:69)\r\n2020-09-21 11:18:21.495 10931-11005/com.example.androidwebtest I/System.out:     at io.netty.util.internal.SocketUtils.accept(SocketUtils.java:116)\r\n2020-09-21 11:18:21.495 10931-11005/com.example.androidwebtest I/System.out:     at io.netty.channel.socket.nio.NioServerSocketChannel.doReadMessages(NioServerSocketChannel.java:147)\r\n2020-09-21 11:18:21.495 10931-11005/com.example.androidwebtest I/System.out:     at io.netty.channel.nio.AbstractNioMessageChannel$NioMessageUnsafe.read(AbstractNioMessageChannel.java:75)\r\n2020-09-21 11:18:21.495 10931-11005/com.example.androidwebtest I/System.out:     at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:714)\r\n2020-09-21 11:18:21.495 10931-11005/com.example.androidwebtest I/System.out:     at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:650)\r\n2020-09-21 11:18:21.495 10931-11005/com.example.androidwebtest I/System.out:     at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:576)\r\n2020-09-21 11:18:21.495 10931-11005/com.example.androidwebtest I/System.out:     at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)\r\n2020-09-21 11:18:21.495 10931-11005/com.example.androidwebtest I/System.out:     at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)\r\n2020-09-21 11:18:21.495 10931-11005/com.example.androidwebtest I/System.out:     at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\r\n2020-09-21 11:18:21.495 10931-11005/com.example.androidwebtest I/System.out:     at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\r\n2020-09-21 11:18:21.495 10931-11005/com.example.androidwebtest I/System.out:     at java.lang.Thread.run(Thread.java:919)\r\n......\r\n```\r\nCPU:\r\n```\r\nTasks: 180 total,   1 running, 178 sleeping,   0 stopped,   1 zombie\r\n  Mem:      1.9G total,      1.5G used,      368M free,       29M buffers\r\n Swap:      1.4G total,      9.0M used,      1.4G free,      762M cached\r\n400%cpu  13%user   0%nice 188%sys 199%idle   0%iow   0%irq   0%sirq   0%host\r\n  PID USER         PR  NI VIRT  RES  SHR S[%CPU] %MEM     TIME+ ARGS\r\n10931 u0_a137      10 -10 1.8G 136M  81M S 99.6   6.8  10:49.49 com.example.androidwebtest\r\n```"
            },
            {
              "comment_username": "normanmaurer",
              "comment_create_time": "2020-09-21T13:23:21Z",
              "comment_edit_time": "2020-09-21T13:23:21Z",
              "comment_text": "This is definitely an android bug. There is not much we can do unfortunally :/"
            }
          ]
        },
        {
          "issue_url": "https://github.com/netty/netty/issues/10583",
          "issue_title": "Add validation check about websocket path",
          "issue_number": 10583,
          "issue_text": "- add uri check in case: / or ?\\r\\n\\r\\nMotivation:\\r\\n\\r\\nI add websocket handler in custom server with netty.\\r\\n\\r\\nI first add WebSocketServerProtocolHandler in my channel pipeline like example code.\\r\\n- example code\\r\\n```java\\r\\nprivate final String WEBSOCKET_PATH = \"/websocket\";\\r\\n\\r\\n// ~~\\r\\n\\r\\nfinal ChannelPipeline p = ch.pipeline();\\r\\n// ~~\\r\\np.addLast(new WebSocketServerProtocolHandler(WEBSOCKET_PATH, true));\\r\\n// ~~\\r\\n```\\r\\nIt does work! but I found that it can pass \"/websocketabc\" through. (I didn't want!!!) \\r\\n\\r\\nModification:\\r\\n\\r\\nSo I modified `isNotWebSocketPath()` method of `WebSocketServerProtocolHandshakeHandler`\\r\\n\\r\\nResult:\\r\\n\\r\\nLet explain some cases.\\r\\n\\r\\nwebsocket path is \"/websocket\".\\r\\n\\r\\ncase 1\\r\\n- /websocket \\r\\n: return false\\r\\n\\r\\ncase 2\\r\\n- /websocketabc \\r\\n: return true\\r\\n\\r\\ncase 3\\r\\n- /websocket/a \\r\\n: return false\\r\\n\\r\\ncase 4\\r\\n- /websocket?name=doyuni \\r\\n: return true\\r\\n\\r\\nFixes #10582 \\r\\n",
          "issue_comments": [
            {
              "comment_username": "netty-bot",
              "comment_create_time": "2020-09-16T10:39:04Z",
              "comment_edit_time": "2020-09-16T10:39:04Z",
              "comment_text": "Can one of the admins verify this patch?"
            },
            {
              "comment_username": "normanmaurer",
              "comment_create_time": "2020-09-16T11:34:46Z",
              "comment_edit_time": "2020-09-16T11:34:46Z",
              "comment_text": "Please add a unit test"
            },
            {
              "comment_username": "Doyuni",
              "comment_create_time": "2020-09-16T12:52:01Z",
              "comment_edit_time": "2020-09-19T01:16:38Z",
              "comment_text": "> Please add a unit test\r\n\r\nOkay.\r\nI'll push after checking test code."
            }
          ]
        },
        {
          "issue_url": "https://github.com/netty/netty/issues/10582",
          "issue_title": "Add validation check about websocket path",
          "issue_number": 10582,
          "issue_text": "### Expected behavior\\r\\n\\r\\nIf I add WebSocketServerProtocolHandler like below,\\r\\n```java\\r\\nprivate final String WEBSOCKET_PATH = \"/websocket\";\\r\\n\\r\\n// ~~\\r\\n\\r\\nfinal ChannelPipeline p = ch.pipeline();\\r\\n\\r\\np.addLast(new WebSocketServerProtocolHandler(WEBSOCKET_PATH, true));\\r\\n```\\r\\nI expected :\\r\\n `/websocket/a` : return true\\r\\n `/websocketabc` : return false\\r\\n `/websocket?name=doyuni` : return true\\r\\n\\r\\n### Actual behavior\\r\\n\\r\\nBut it return true on all above case.\\r\\n\\r\\nI don't want this handler to pass `/websocketabc` through.\\r\\n\\r\\n### Steps to reproduce\\r\\n\\r\\nJust modify one method like below code. \\r\\n\\r\\n### Minimal yet complete reproducer code (or URL to code)\\r\\n- WebSocketServerProtocolHandshakeHandler\\r\\n```java\\r\\nprivate boolean isNotWebSocketPath(FullHttpRequest req) {\\r\\n    String websocketPath = serverConfig.websocketPath();\\r\\n    char nextStartUri = '/';\\r\\n    if (req.uri().length() > websocketPath.length()) nextStartUri = req.uri().charAt(websocketPath.length());  \\r\\n    return serverConfig.checkStartsWith() ? !(req.uri().startsWith(websocketPath) && (nextStartUri == '/' || nextStartUri == '?')) \\r\\n                                          : !req.uri().equals(websocketPath);\\r\\n}\\r\\n```\\r\\n\\r\\n### Netty version\\r\\n\\r\\n- 4.1.52.Final\\r\\n\\r\\n### JVM version (e.g. `java -version`)\\r\\n```\\r\\njava version \"11.0.5\" 2019-10-15 LTS\\r\\nJava(TM) SE Runtime Environment 18.9 (build 11.0.5+10-LTS)\\r\\nJava HotSpot(TM) 64-Bit Server VM 18.9 (build 11.0.5+10-LTS, mixed mode)\\r\\n```\\r\\n### OS version (e.g. `uname -a`)\\r\\n\\r\\n- Mac OS Catalina v. 10.15.6",
          "issue_comments": [
            {
              "comment_username": "normanmaurer",
              "comment_create_time": "2020-09-16T09:57:17Z",
              "comment_edit_time": "2020-09-16T09:57:17Z",
              "comment_text": "@Doyuni please open a PR with the fix and a testcase- "
            },
            {
              "comment_username": "Doyuni",
              "comment_create_time": "2020-09-16T09:58:28Z",
              "comment_edit_time": "2020-09-16T09:58:28Z",
              "comment_text": "> @Doyuni please open a PR with the fix and a testcase-\r\n\r\nOkay I got it."
            }
          ]
        },
        {
          "issue_url": "https://github.com/netty/netty/issues/10579",
          "issue_title": "is there any way to customize ssl ClientHello message ?",
          "issue_number": 10579,
          "issue_text": "i am using netty as the http client , \r\nand  SslProvider  is  OPENSSL ,\r\ni want to modify the Extension part ,\r\nso is there any way to customize ssl ClientHello message ?\r\n\r\n### Expected behavior\r\n\r\nsupport a handler to customize the ClientHello message before handshake.\r\n\r\n### Actual behavior\r\n\r\n### Steps to reproduce\r\n\r\n### Minimal yet complete reproducer code (or URL to code)\r\n\r\n### Netty version\r\n4.1.50\r\n### JVM version (e.g. `java -version`)\r\njdk8\r\n### OS version (e.g. `uname -a`)\r\nwin7",
          "issue_comments": [
            {
              "comment_username": "hyperxpro",
              "comment_create_time": "2020-09-17T14:12:05Z",
              "comment_edit_time": "2020-09-17T14:12:15Z",
              "comment_text": "Add a handler before `SslHandler` and modify the `ByteBuf` containing `ClientHello` message of TLS."
            },
            {
              "comment_username": "normanmaurer",
              "comment_create_time": "2020-09-21T13:23:46Z",
              "comment_edit_time": "2020-09-21T13:23:46Z",
              "comment_text": "@swoky what exactly you want to modify there ?"
            },
            {
              "comment_username": "swoky",
              "comment_create_time": "2020-09-25T15:24:29Z",
              "comment_edit_time": "2020-09-25T15:24:29Z",
              "comment_text": "@normanmaurer  Actually i am using netty as the crawler , \r\nthe problem now is about the SSL Fingerprints ,  see [https://github.com/salesforce/ja3](url)  ,\r\nso i want to change the Extension part randomly, \r\nsuch as the supportedCurves , supportedPoints,  etc"
            }
          ]
        },
        {
          "issue_url": "https://github.com/netty/netty/issues/10574",
          "issue_title": "Obsolete line folding in HTTP headers enables CRLF injection on Internet Explorer",
          "issue_number": 10574,
          "issue_text": "### Expected behavior\\r\\nThe validator for HTTP header should follow RFC 7230 and will solve this issue. (https://tools.ietf.org/html/rfc7230#section-3.2.4)\\r\\n\\r\\n> Historically, HTTP header field values could be extended over\\r\\nmultiple lines by preceding each extra line with at least one space\\r\\nor horizontal tab (obs-fold).  This specification deprecates such\\r\\nline folding except within the message/http media type\\r\\n(Section 8.3.1).  **A sender MUST NOT generate a message that includes\\r\\nline folding** (i.e., that has any field-value that contains a match to\\r\\nthe obs-fold rule) unless the message is intended for packaging\\r\\nwithin the message/http media type.\\r\\n>\\r\\n> A server that receives an obs-fold in a request message that is not\\r\\nwithin a message/http container MUST either reject the message by\\r\\nsending a 400 (Bad Request), preferably with a representation\\r\\nexplaining that obsolete line folding is unacceptable, or replace\\r\\neach received obs-fold with one or more SP octets prior to\\r\\ninterpreting the field value or forwarding the message downstream.\\r\\n>\\r\\n> A proxy or gateway that receives an obs-fold in a response message\\r\\nthat is not within a message/http container MUST either discard the\\r\\nmessage and replace it with a 502 (Bad Gateway) response, preferably\\r\\nwith a representation explaining that unacceptable line folding was\\r\\nreceived, or replace each received obs-fold with one or more SP\\r\\noctets prior to interpreting the field value or forwarding the\\r\\nmessage downstream.\\r\\n>\\r\\n> A user agent that receives an obs-fold in a response message that is\\r\\nnot within a message/http container MUST replace each received\\r\\nobs-fold with one or more SP octets prior to interpreting the field\\r\\nvalue.\\r\\n\\r\\n\"obs-fold\" and related fields are defined as below. (https://tools.ietf.org/html/rfc7230#section-3.2)\\r\\n\\r\\n```\\r\\nheader-field   = field-name \":\" OWS field-value OWS\\r\\n\\r\\nfield-name     = token\\r\\nfield-value    = *( field-content / obs-fold )\\r\\nfield-content  = field-vchar [ 1*( SP / HTAB ) field-vchar ]\\r\\nfield-vchar    = VCHAR / obs-text\\r\\n\\r\\nobs-fold       = CRLF 1*( SP / HTAB )\\r\\n               ; obsolete line folding\\r\\n               ; see Section 3.2.4\\r\\n```\\r\\n\\r\\n### Actual behavior\\r\\nCurrently, the validation method for setting a value in HTTP header like `validateValueChar()` in `DefaultHttpHeaders` allow multiple lines HTTP header according to RFC 2616. (https://tools.ietf.org/html/rfc2616#section-2.2)\\r\\n\\r\\n> HTTP/1.1 header field values can be folded onto multiple lines if the\\r\\n> continuation line begins with a space or horizontal tab. All linear\\r\\n> white space, including folding, has the same semantics as SP. A\\r\\n> recipient MAY replace any linear white space with a single SP before\\r\\n> interpreting the field value or forwarding the message downstream.\\r\\n>\\r\\n> LWS            = [CRLF] 1*( SP | HT )\\r\\n\\r\\nHowever, Internet Explorer interpret this same as [CRLF] even if it's after \"LWS\" (or \"obs-fold\" in RFC 7230).\\r\\nThis enables HTTP header and body injection when a server side application set a HTTP header with \"LWS\".\\r\\n\\r\\nThis doesn't occur in Chrome, Firefox and Safari. They interpret \"LWS\" (or \"obs-fold\") as space character instead. \\r\\n\\r\\n### Steps to reproduce\\r\\n\\r\\nSee the example code below.\\r\\n\\r\\n### Minimal yet complete reproducer code (or URL to code)\\r\\n\\r\\nUsed example code from https://github.com/eugenp/tutorials/tree/9725f52fad7b336ff7f44d57ba9b5d648f63a6d1/netty and modified as following.\\r\\n\\r\\n```diff\\r\\ndiff --git a/netty/src/main/java/com/baeldung/http/server/CustomHttpServerHandler.java b/netty/src/main/java/com/baeldung/http/server/CustomHttpServerHandler.java\\r\\nindex abefbed0d9..96edc8e185 100644\\r\\n--- a/netty/src/main/java/com/baeldung/http/server/CustomHttpServerHandler.java\\r\\n+++ b/netty/src/main/java/com/baeldung/http/server/CustomHttpServerHandler.java\\r\\n@@ -79,6 +79,8 @@ public class CustomHttpServerHandler extends SimpleChannelInboundHandler<Object>\\r\\n                     .readableBytes());\\r\\n             httpResponse.headers()\\r\\n                 .set(HttpHeaderNames.CONNECTION, HttpHeaderValues.KEEP_ALIVE);\\r\\n+            httpResponse.headers()\\r\\n+                .set(\"X-Custom-Header\", \"custom\\\\r\\\\n Set-Cookie: foo=bar\\\\r\\\\n \\\\r\\\\n Modified!\");\\r\\n         }\\r\\n\\r\\n         ctx.write(httpResponse);\\r\\n```\\r\\n\\r\\nThis application sets \"foo=bar\" as cookie and display \"Modified!\" when accessing from IE. In this example, I use constant string value, but it would be a vulnerability if there's an implementation accepting value from user's input.\\r\\n\\r\\n### Netty version\\r\\n\\r\\n4.1.52.Final\\r\\n\\r\\n### JVM version (e.g. `java -version`)\\r\\n\\r\\n```\\r\\nopenjdk version \"11.0.8\" 2020-07-14\\r\\nOpenJDK Runtime Environment (build 11.0.8+10-post-Ubuntu-0ubuntu120.04)\\r\\nOpenJDK 64-Bit Server VM (build 11.0.8+10-post-Ubuntu-0ubuntu120.04, mixed mode, sharing)\\r\\n```\\r\\n\\r\\n### OS version (e.g. `uname -a`)\\r\\n\\r\\nUbuntu 20.04.1 LTS\\r\\n",
          "issue_comments": [
            {
              "comment_username": "normanmaurer",
              "comment_create_time": "2020-09-15T13:02:34Z",
              "comment_edit_time": "2020-09-15T13:02:34Z",
              "comment_text": "@ntomoya so you talk about the validation when encoding headers ?"
            },
            {
              "comment_username": "ntomoya",
              "comment_create_time": "2020-09-15T14:47:52Z",
              "comment_edit_time": "2020-09-15T14:47:52Z",
              "comment_text": "@normanmaurer yes, validation on encoding headers is insufficient for IE."
            },
            {
              "comment_username": "ntomoya",
              "comment_create_time": "2020-09-16T03:25:26Z",
              "comment_edit_time": "2020-09-16T03:29:22Z",
              "comment_text": "To be more precisely, the example below throws an error,\\r\\n```java\\r\\nhttpResponse.headers()\\r\\n    .set(\"X-Custom-Header\", \"custom\\\\r\\\\nSet-Cookie: foo=bar\");\\r\\n```\\r\\n```\\r\\njava.lang.IllegalArgumentException: only ' ' and '\\\\t' are allowed after '\\\\n': custom\\r\\nSet-Cookie: foo=bar\\r\\n\\tat io.netty.handler.codec.http.DefaultHttpHeaders$HeaderValueConverterAndValidator.validateValueChar(DefaultHttpHeaders.java:504)\\r\\n\\tat io.netty.handler.codec.http.DefaultHttpHeaders$HeaderValueConverterAndValidator.convertObject(DefaultHttpHeaders.java:453)\\r\\n\\tat io.netty.handler.codec.http.DefaultHttpHeaders$HeaderValueConverterAndValidator.convertObject(DefaultHttpHeaders.java:444)\\r\\n\\tat io.netty.handler.codec.DefaultHeaders.setObject(DefaultHeaders.java:480)\\r\\n\\tat io.netty.handler.codec.http.DefaultHttpHeaders.set(DefaultHttpHeaders.java:177)\\r\\n\\tat com.baeldung.http.server.CustomHttpServerHandler.writeResponse(CustomHttpServerHandler.java:83)\\r\\n\\tat com.baeldung.http.server.CustomHttpServerHandler.channelRead0(CustomHttpServerHandler.java:57)\\r\\n...\\r\\n```\\r\\nbut this one is executed with no error.\\r\\n```java\\r\\nhttpResponse.headers()\\r\\n    .set(\"X-Custom-Header\", \"custom\\\\r\\\\n Set-Cookie: foo=bar\");\\r\\n```"
            }
          ]
        },
        {
          "issue_url": "https://github.com/netty/netty/issues/10569",
          "issue_title": "libio_grpc_netty_shaded_netty_tcnative_osx_x86_64 not signed",
          "issue_number": 10569,
          "issue_text": "### Expected behavior\\r\\nThe file grpc-netty-shaded-1.31.1.jar/META-INF/native/libio_grpc_netty_shaded_netty_tcnative_osx_x86_64.jnilib being signed.\\r\\n\\r\\n### Actual behavior\\r\\nThat it's not signed.\\r\\n\\r\\n\\r\\n### Steps to reproduce\\r\\nSubmit a package to Apple for Notarization containing grpc-netty-shaded-1.31.1.jar. The Notarization process will fail, with the following being reported:\\r\\n\\r\\n    {\\r\\n      \"severity\": \"error\",\\r\\n      \"code\": null,\\r\\n      \"path\": \"MyApp-1.0-alpha-1.dmg/My App.app/Contents/Java/lib/grpc-netty-shaded-1.27.1.jar/META-INF/native/libio_grpc_netty_shaded_netty_tcnative_osx_x86_64.jnilib\",\\r\\n      \"message\": \"The binary is not signed.\",\\r\\n      \"docUrl\": null,\\r\\n      \"architecture\": \"x86_64\"\\r\\n    }\\r\\n\\r\\nI've worked around this by extracting the libio_grpc_netty_shaded_netty_tcnative_osx_x86_64.jnilib out of the jar, signing it, then replacing it in the jar before sending off to Apple. I think ideally the binary should be signed as part of the build phase of the \\r\\n\\r\\n### Minimal yet complete reproducer code (or URL to code)\\r\\n\\r\\n### Netty version\\r\\n1.31.1\\r\\n### JVM version (e.g. `java -version`)\\r\\nN/A\\r\\n### OS version (e.g. `uname -a`)\\r\\nmacOS",
          "issue_comments": [
            {
              "comment_username": "hyperxpro",
              "comment_create_time": "2020-09-14T09:06:26Z",
              "comment_edit_time": "2020-09-14T09:06:26Z",
              "comment_text": "That's gRPC shaded Netty. We can't do anything. Ask gRPC folks to sign the binary built from shaded Netty."
            },
            {
              "comment_username": "liamsharp",
              "comment_create_time": "2020-09-14T10:28:47Z",
              "comment_edit_time": "2020-09-14T10:28:47Z",
              "comment_text": "Hi @hyperxpro no problem, can you point me in the right direction to do that? I was sent here after raising https://github.com/grpc/grpc-java/issues/7404"
            },
            {
              "comment_username": "ejona86",
              "comment_create_time": "2020-09-14T16:06:50Z",
              "comment_edit_time": "2020-09-14T16:06:50Z",
              "comment_text": "The file is simply a renamed copy of the Netty file. If Netty signed the jnilib, then it would seem that gRPC's renaming would also be signed. From what I can tell, it appears signing is stable across renames."
            },
            {
              "comment_username": "liamsharp",
              "comment_create_time": "2020-09-15T08:42:20Z",
              "comment_edit_time": "2020-09-15T08:42:20Z",
              "comment_text": "@ejona86 Can you point me at the right project to raise the issue?"
            },
            {
              "comment_username": "normanmaurer",
              "comment_create_time": "2020-09-15T08:43:21Z",
              "comment_edit_time": "2020-09-15T08:43:21Z",
              "comment_text": "@liamsharp I think this one is the right... "
            },
            {
              "comment_username": "ejona86",
              "comment_create_time": "2020-09-15T15:23:07Z",
              "comment_edit_time": "2020-09-15T15:23:07Z",
              "comment_text": "@liamsharp, technically https://github.com/netty/netty-tcnative may be a slightly better place, but this problem probably impacts kqueue as well.\r\n\r\nI've not found a good way to check whether a jnilib is signed from Linux (since I don't have access to a Mac). But I didn't find any codesign invocation in Netty and this seems like something where if we codesigned one binary we would probably be codesigning the others."
            }
          ]
        },
        {
          "issue_url": "https://github.com/netty/netty/issues/10553",
          "issue_title": "Some new issues about FileUpload in 4.1.52.Final",
          "issue_number": 10553,
          "issue_text": "### Expected behavior\r\nI had a program to upgrade all servers. users upload file like .war, .exe, and My program will send file to targer server and restart it. \r\nNow, I found some issue in *4.1.52.Final*. When the war file upload, I called shell move it to tomcat'webapps and restart it, but it doesn't run well, the exception is `java.util.zip.ZipException: invalid LOC header (bad signature)` .\r\nI change the version to *4.1.42.Final*, it works fine!\r\nI had checked file size, it's OK, but there must have are some changes in the file, so the signature is bad.\r\nLooking forward to your answers! Best regards!\r\n### Actual behavior\r\nIt can working well...\r\n### Steps to reproduce\r\nUseing 4.1.52.Final to file upload...\r\n### Minimal yet complete reproducer code (or URL to code)\r\nhttps://github.com/github-big-cheng/upgrade.git\r\n### Netty version\r\n4.1.52.Final\r\n### JVM version (e.g. `java -version`)\r\njdk 1.7\r\n### OS version (e.g. `uname -a`)\r\nI had test it on Windows 10 and Linux CentOS 7",
          "issue_comments": [
            {
              "comment_username": "chrisvest",
              "comment_create_time": "2020-09-09T08:56:41Z",
              "comment_edit_time": "2020-09-09T08:56:41Z",
              "comment_text": "Can you narrow down the version range, like, does it work in 4.1.51 for instance? The reproducer you linked does not look minimal."
            },
            {
              "comment_username": "github-big-cheng",
              "comment_create_time": "2020-09-09T09:05:07Z",
              "comment_edit_time": "2020-09-09T09:05:07Z",
              "comment_text": "> Can you narrow down the version range, like, does it work in 4.1.51 for instance? The reproducer you linked does not look minimal.\r\n\r\nIn 4.1.51.Final also has a issue [#10516](url:https://github.com/netty/netty/issues/10516) , I will try more version lower version as soon as posible, hope can help you soon"
            },
            {
              "comment_username": "github-big-cheng",
              "comment_create_time": "2020-09-09T09:25:55Z",
              "comment_edit_time": "2020-09-09T09:26:47Z",
              "comment_text": "> Can you narrow down the version range, like, does it work in 4.1.51 for instance? The reproducer you linked does not look minimal.\r\n\r\nThe newest version is 4.1.49.Final.  4.1.51.Final, 4.1.50.Final are all not work well \r\nHope can help you..."
            },
            {
              "comment_username": "normanmaurer",
              "comment_create_time": "2020-09-09T09:29:28Z",
              "comment_edit_time": "2020-09-09T09:29:28Z",
              "comment_text": "@github-big-cheng so to clarify ... 4.1.49.Final is the newest that is working ?"
            },
            {
              "comment_username": "github-big-cheng",
              "comment_create_time": "2020-09-09T09:31:10Z",
              "comment_edit_time": "2020-09-09T09:31:10Z",
              "comment_text": "> > Can you narrow down the version range, like, does it work in 4.1.51 for instance? The reproducer you linked does not look minimal.\r\n> \r\n> The newest version is 4.1.49.Final. Hope can help you...\r\n\r\n\r\n\r\n> so to clarify\r\n\r\nyes"
            }
          ]
        },
        {
          "issue_url": "https://github.com/netty/netty/issues/10550",
          "issue_title": "Waited 10000 milliseconds for org.apache.drill.shaded.guava.com.google.common.util.concurrent.SettableFuture@6c2d4cc6[status=PENDING]",
          "issue_number": 10550,
          "issue_text": "Hi, I have upgraded Netty jar from 4.0.48.Final to 4.1.50.Final. I have made the required code changes. After running my drill project with the new Netty version Plain connection without SSL is working. While not able to connect to drillbit with enabling SSL . \r\nBelow error is coming (without stack trace)\r\nWaited 10000 milliseconds for org.apache.drill.shaded.guava.com.google.common.util.concurrent.SettableFuture@6c2d4cc6[status=PENDING]\r\n\r\nIt would be great if someone can suggest is this issue related to Netty.\r\n\r\n### Netty version\r\n4.1.50.Final\r\n\r\n### JVM version (e.g. `java -version`)\r\nJAVA 8\r\n### OS version (e.g. `uname -a`)\r\n",
          "issue_comments": [
            {
              "comment_username": "hyperxpro",
              "comment_create_time": "2020-09-09T06:11:02Z",
              "comment_edit_time": "2020-09-09T06:11:02Z",
              "comment_text": "Show `SslContext` code."
            },
            {
              "comment_username": "alkakumari42",
              "comment_create_time": "2020-09-09T07:52:06Z",
              "comment_edit_time": "2020-09-09T07:52:06Z",
              "comment_text": "Hi,\r\n SslContext code is the same as of io.netty.handler.ssl.SslContext.java. Do you want me to show this code?"
            },
            {
              "comment_username": "hyperxpro",
              "comment_create_time": "2020-09-09T07:53:30Z",
              "comment_edit_time": "2020-09-09T07:53:30Z",
              "comment_text": "The builder code I mean. Show."
            }
          ]
        },
        {
          "issue_url": "https://github.com/netty/netty/issues/10548",
          "issue_title": "Use SQPOLL mode & Kernel 5.10",
          "issue_number": 10548,
          "issue_text": "Motivation:\r\n\r\n0-syscall operation submission, if configured in SQPOLL mode, a kernel thread is created to perform submission queue polling which means that no submission is required, however the cpu usage is higher\r\n\r\nModifications:\r\n\r\n-when the kernel thread is idle, we have to wake up \r\nif the ring tail is changed\r\n-no io_uring_enter(submission) call anymore\r\n\r\nResult:\r\n\r\nbetter performance but the cpu usage is higher",
          "issue_comments": [
            {
              "comment_username": "netty-bot",
              "comment_create_time": "2020-09-08T08:19:04Z",
              "comment_edit_time": "2020-09-08T08:19:04Z",
              "comment_text": "Can one of the admins verify this patch?"
            },
            {
              "comment_username": "1Jo1",
              "comment_create_time": "2020-09-08T08:25:59Z",
              "comment_edit_time": "2020-09-08T08:34:30Z",
              "comment_text": "you cannot open/close descriptors in 5.9 as registration of files is required when SQPOLL is enabled but not 5.10, that's why I use 5.10\r\nif you want to test it you should use [io_uring-5.10](https://git.kernel.org/pub/scm/linux/kernel/git/axboe/linux-block.git/log/?h=for-5.10/io_uring)\r\n\r\n"
            },
            {
              "comment_username": "1Jo1",
              "comment_create_time": "2020-09-08T08:27:07Z",
              "comment_edit_time": "2020-09-08T08:27:07Z",
              "comment_text": "BTW I had a performance boost of 15% in echo benchmark tests when SQPOLL is active :fire:"
            },
            {
              "comment_username": "1Jo1",
              "comment_create_time": "2020-09-08T08:33:24Z",
              "comment_edit_time": "2020-09-08T08:33:24Z",
              "comment_text": "more details about SQPOLL in case if you interested [io_uring mailing list](https://lore.kernel.org/io-uring/CAAss7+pjbh2puVsQTOt7ymKSmbruBZbaOvB8tqfw0z-cMuhJYg@mail.gmail.com/T)"
            },
            {
              "comment_username": "1Jo1",
              "comment_create_time": "2020-09-08T08:44:09Z",
              "comment_edit_time": "2020-09-08T08:45:10Z",
              "comment_text": "I think it would be make sense to make  `sq_thread_cpu` configurable which is useful for servers with multiple cpus and `sq_thread_idle` default idle time is 1 Sec\r\n[io_uring_params](https://github.com/axboe/liburing/blob/master/src/include/liburing/io_uring.h#L243)"
            },
            {
              "comment_username": "hyperxpro",
              "comment_create_time": "2020-09-08T11:08:10Z",
              "comment_edit_time": "2020-09-08T11:08:10Z",
              "comment_text": "Would you mind sharing benchmark code?"
            },
            {
              "comment_username": "normanmaurer",
              "comment_create_time": "2020-09-08T11:25:45Z",
              "comment_edit_time": "2020-09-08T11:25:45Z",
              "comment_text": "@hyperxpro we will share some more numbers etc soonish. "
            },
            {
              "comment_username": "1Jo1",
              "comment_create_time": "2020-09-08T16:42:10Z",
              "comment_edit_time": "2020-09-08T16:42:10Z",
              "comment_text": "> @1Jo1 nice! I guess in this mode we may want to update the tail pointer after every SQE is added, i.e. better to not batch\r\n\r\n@njhill yeah that's true, it's more efficient especially if you want to reduce the latency, I'm gonna test both cases in terms of throughput and latency"
            },
            {
              "comment_username": "1Jo1",
              "comment_create_time": "2020-09-09T13:00:43Z",
              "comment_edit_time": "2020-09-09T13:00:43Z",
              "comment_text": "I forgot to mention that SQPOLL in 5.10 root permission is not required anymore, you just need CAP_SYS_NICE privilege"
            },
            {
              "comment_username": "normanmaurer",
              "comment_create_time": "2020-09-11T10:55:14Z",
              "comment_edit_time": "2020-09-11T10:55:14Z",
              "comment_text": "Let's hold back on this till 5.10 was cut. "
            }
          ]
        },
        {
          "issue_url": "https://github.com/netty/netty/issues/10534",
          "issue_title": "Honor `-DsocksProxyHost/socksProxyPort` and `ProxySelector.setDefault(...)` in Netty-based clients",
          "issue_number": 10534,
          "issue_text": "### Expected behavior\r\n\r\nAlmost every Java library (except Netty and Netty-based libraries) accepts those by default.\r\nThis makes it easy to modify proxy settings, even when I don't have access to all source code.\r\n\r\n\r\n### Actual behavior\r\n\r\nNetty requires manual proxy setup in code. Not all libraries using Netty are proxy-aware, and I don't have access to all source code.\r\n",
          "issue_comments": [
            {
              "comment_username": "hyperxpro",
              "comment_create_time": "2020-09-04T15:47:24Z",
              "comment_edit_time": "2020-09-04T15:47:24Z",
              "comment_text": "Ignore my previous reply regarding PR. Deleted though.\r\n\r\nWhere you want Netty to honor those configurations?"
            },
            {
              "comment_username": "iirekm",
              "comment_create_time": "2020-09-04T16:12:00Z",
              "comment_edit_time": "2020-09-04T16:12:00Z",
              "comment_text": "Netty supports client proxy setup only in `initChannel` as shown here: https://stackoverflow.com/questions/35119032/how-to-use-socks4-5-proxy-handlers-in-netty-client-4-1 (`p.addLast(new Socks4ProxyHandler(new InetSocketAddress(\"149.202.68.167\", 37678)))`)\r\n\r\nIt would be great if Netty somehow added this line automatically when `-DsocksProxyHost/Port/ProxySelector.setDefault(...)` is present.\r\nOtherwise it makes any client built on Netty work different than things built with other Java libraries (`java.net`, `OkHttp`, etc... - they all honor proxy settings)."
            },
            {
              "comment_username": "hyperxpro",
              "comment_create_time": "2020-09-04T17:04:47Z",
              "comment_edit_time": "2020-09-04T17:04:47Z",
              "comment_text": "I will tackle this"
            },
            {
              "comment_username": "hyperxpro",
              "comment_create_time": "2020-09-04T20:17:17Z",
              "comment_edit_time": "2020-09-04T20:17:17Z",
              "comment_text": "@iirekm PTAL #10535 "
            }
          ]
        },
        {
          "issue_url": "https://github.com/netty/netty/issues/10530",
          "issue_title": "Reduce DefaultAttributeMap cost",
          "issue_number": 10530,
          "issue_text": "Motivation:\r\n\r\nDefaultAttributeMap::attr has a blocking behaviour even on an existing attribute:\r\nit can be made non-blocking.\r\n\r\nModification:\r\n\r\nReplace the existing fixed bucket table with a locked intrusive linked list\r\nwith JCTools NonBlockingIdentityHashMap\r\n\r\nResult:\r\nNon blocking behaviour for the happy path, in case of hash collisions",
          "issue_comments": [
            {
              "comment_username": "franz1981",
              "comment_create_time": "2020-09-03T17:57:32Z",
              "comment_edit_time": "2020-09-03T18:03:23Z",
              "comment_text": "Some results with 3 threads:\r\n\r\nnew PR:\r\n```\r\nBenchmark                                                   (keyCount)   Mode  Cnt          Score          Error  Units\r\nDefaultAttributeMapBenchmark.hasAttributeIdentityHashMap             8  thrpt   10  367145184.163 ±  7499666.816  ops/s\r\nDefaultAttributeMapBenchmark.hasAttributeIdentityHashMap            32  thrpt   10  369148667.556 ±  3003737.630  ops/s\r\nDefaultAttributeMapBenchmark.hasAttributeIdentityHashMap           128  thrpt   10  360479094.067 ± 13524539.811  ops/s\r\nDefaultAttributeMapBenchmark.mixedAttributeAttributeMap              8  thrpt   10  229341158.761 ±  8837044.476  ops/s\r\nDefaultAttributeMapBenchmark.mixedAttributeAttributeMap             32  thrpt   10  225554730.599 ±  2452984.193  ops/s\r\nDefaultAttributeMapBenchmark.mixedAttributeAttributeMap            128  thrpt   10  210378861.870 ±  1055008.456  ops/s\r\nDefaultAttributeMapBenchmark.mixedAttributeIdentityHashMap           8  thrpt   10  289517313.268 ± 16392624.725  ops/s\r\nDefaultAttributeMapBenchmark.mixedAttributeIdentityHashMap          32  thrpt   10  295330054.501 ±  1656250.461  ops/s\r\nDefaultAttributeMapBenchmark.mixedAttributeIdentityHashMap         128  thrpt   10  277173213.813 ±  3776528.818  ops/s\r\n\r\nDefaultAttributeMapBenchmark.nextAttributeAttributeMap               8  thrpt   10  231566414.293 ± 10848753.693  ops/s\r\nDefaultAttributeMapBenchmark.nextAttributeAttributeMap              32  thrpt   10  239880800.928 ± 11002339.267  ops/s\r\nDefaultAttributeMapBenchmark.nextAttributeAttributeMap             128  thrpt   10  215727497.922 ± 10318193.502  ops/s\r\nDefaultAttributeMapBenchmark.nextAttributeIdentityHashMap            8  thrpt   10  284780317.789 ± 26697438.387  ops/s\r\nDefaultAttributeMapBenchmark.nextAttributeIdentityHashMap           32  thrpt   10  296261390.153 ± 10807951.118  ops/s\r\nDefaultAttributeMapBenchmark.nextAttributeIdentityHashMap          128  thrpt   10  297976099.812 ±  3701072.491  ops/s\r\nDefaultAttributeMapBenchmark.nextHasAttributeAttributeMap            8  thrpt   10  241590148.718 ±  4653849.298  ops/s\r\nDefaultAttributeMapBenchmark.nextHasAttributeAttributeMap           32  thrpt   10  242079507.894 ±  2528554.988  ops/s\r\nDefaultAttributeMapBenchmark.nextHasAttributeAttributeMap          128  thrpt   10  232416903.433 ± 11316480.975  ops/s\r\n```\r\n\r\npre PR:\r\n```\r\nBenchmark                                                  (keyCount)   Mode  Cnt         Score         Error  Units\r\nDefaultAttributeMapBenchmark.mixedAttributeAttributeMap             8  thrpt   10  21238289.957 ±  152632.263  ops/s\r\nDefaultAttributeMapBenchmark.mixedAttributeAttributeMap            32  thrpt   10  20579513.665 ± 1132297.498  ops/s\r\nDefaultAttributeMapBenchmark.mixedAttributeAttributeMap           128  thrpt   10  22102620.510 ±  356319.731  ops/s\r\nDefaultAttributeMapBenchmark.nextAttributeAttributeMap              8  thrpt   10  22062599.024 ±  428367.775  ops/s\r\nDefaultAttributeMapBenchmark.nextAttributeAttributeMap             32  thrpt   10  21292109.421 ±  405036.756  ops/s\r\nDefaultAttributeMapBenchmark.nextAttributeAttributeMap            128  thrpt   10  21945850.542 ± 1045899.418  ops/s\r\nDefaultAttributeMapBenchmark.nextHasAttributeAttributeMap           8  thrpt   10  21300566.677 ±  871462.820  ops/s\r\nDefaultAttributeMapBenchmark.nextHasAttributeAttributeMap          32  thrpt   10  21414965.307 ±  593072.591  ops/s\r\nDefaultAttributeMapBenchmark.nextHasAttributeAttributeMap         128  thrpt   10  21166525.325 ± 1136979.350  ops/s\r\n```\r\n\r\nStill unhappy due to the magnitude of error ( ie i'm investigating what's going on)\r\n\r\nThe benchmark is using a pure read workload on the attribute map, using an IdentityHashMap as a baseline.\r\nThe improvement is about 10X with 3 threads, but in a real case, an event loop is supposed to perform some work in addition to querying the attribute map.\r\n\r\nSome notes about the footprint for 1000 attributes:\r\n\r\n- this PR has ~ \r\n8194 references = 4 * 8194\r\n4096 ints = 4 * 4096 \r\n1000 DefaultAttribute = 24 bytes * 1000 \r\ntotal  ~ 73160 bytes\r\n\r\n- pre PR has\r\n1004 DefaultAttribute = 40 bytes * 1004 \r\ntotal  ~ 40164 bytes\r\n\r\nThere are 3 important changes:\r\n\r\n1. `Attribute::remove`, that's deprecated, is no longer reducing the capacity of the map, freeing memory\r\n2. calling `Attribute::remove` will force failing any subsequent attempt to modify the attribute value: this is a semantic that has never been defined properly so I've used it in order to reduce further the memory footprint of `DefaultAttribute`\r\n3. the map is now using the native key hashCode instead of the attribute id "
            },
            {
              "comment_username": "normanmaurer",
              "comment_create_time": "2020-09-03T17:57:47Z",
              "comment_edit_time": "2020-09-03T17:57:47Z",
              "comment_text": "@franz1981 what is the memory footprint compared to the old impl ?"
            },
            {
              "comment_username": "franz1981",
              "comment_create_time": "2020-09-03T17:59:35Z",
              "comment_edit_time": "2020-09-03T18:03:48Z",
              "comment_text": "@normanmaurer \r\n\r\n> what is the memory footprint compared to the old impl ?\r\n\r\nConsider that now is more compact then before, so the *real* benefit comes from a better heap usage and less fragmentation..\r\nsomething that cannot be measured if not by inspecting with different GCs the memory layout of both the solutions.\r\nIn the previous comment I've put some computation about the memory footprint"
            },
            {
              "comment_username": "franz1981",
              "comment_create_time": "2020-09-03T18:10:02Z",
              "comment_edit_time": "2020-09-03T18:11:30Z",
              "comment_text": "With 100 elements instead is 5472 bytes this PR vs 4160 bytes pre PR\r\nIt really depends by the JCTools map and the distribution of native hash, but that's general idea "
            },
            {
              "comment_username": "franz1981",
              "comment_create_time": "2020-09-03T19:20:44Z",
              "comment_edit_time": "2020-09-03T19:20:44Z",
              "comment_text": "The single threaded results are even more interesting (using biased locking):\r\n\r\nthis PR:\r\n```\r\nBenchmark                                               (keyCount)   Mode  Cnt         Score         Error  Units\r\nDefaultAttributeMapBenchmark.nextAttributeAttributeMap           8  thrpt   10  79519602.308 ± 1389472.881  ops/s\r\nDefaultAttributeMapBenchmark.nextAttributeAttributeMap          32  thrpt   10  81520228.714 ±  745292.328  ops/s\r\nDefaultAttributeMapBenchmark.nextAttributeAttributeMap         128  thrpt   10  79172148.769 ±  966234.464  ops/s\r\n```\r\npre PR:\r\n```\r\nBenchmark                                               (keyCount)   Mode  Cnt         Score        Error  Units\r\nDefaultAttributeMapBenchmark.nextAttributeAttributeMap           8  thrpt   10  37123644.798 ±  81277.516  ops/s\r\nDefaultAttributeMapBenchmark.nextAttributeAttributeMap          32  thrpt   10  35340734.584 ±  27993.088  ops/s\r\nDefaultAttributeMapBenchmark.nextAttributeAttributeMap         128  thrpt   10  16693696.225 ± 463195.978  ops/s\r\n```\r\nIt has a 2X -> 5X improvement although pre PR is using biased locking\r\n"
            },
            {
              "comment_username": "franz1981",
              "comment_create_time": "2020-09-03T19:43:10Z",
              "comment_edit_time": "2020-09-03T19:43:10Z",
              "comment_text": "@normanmaurer hold on...it seems that the memory footprint is not the same I've measured before, probably I'm missing some `long[]` that's making the map footprint higher then expected mmm"
            },
            {
              "comment_username": "njhill",
              "comment_create_time": "2020-09-03T19:51:00Z",
              "comment_edit_time": "2020-09-03T19:51:00Z",
              "comment_text": "Just an idea.. if changes are rare relative to reads, I wonder how doing copy-on-write of a regular `java.util.IdentityHashMap` would compare."
            },
            {
              "comment_username": "franz1981",
              "comment_create_time": "2020-09-03T20:00:42Z",
              "comment_edit_time": "2020-09-04T06:03:36Z",
              "comment_text": "@njhill \\r\\nthat's a very good idea indeed: I was going to replace the Cliff Click map with something \"similar\" now.\\r\\n\\r\\nOther information about the JCTools footprint:\\r\\n\\r\\n1 element \\r\\n\\r\\nnew PR: 1,296 B \\r\\n\\r\\nold PR: 218 B\\r\\n\\r\\n128 elements:\\r\\n\\r\\nnew PR: 11,176 B\\r\\n\\r\\nold PR: 7,628 B\\r\\n\\r\\nEffectively with a small sized number of attribute the different in footprint seems too high, wdyt?"
            },
            {
              "comment_username": "njhill",
              "comment_create_time": "2020-09-03T20:32:26Z",
              "comment_edit_time": "2020-09-03T20:32:26Z",
              "comment_text": "@franz1981 good idea, I like that even better... it can just be a `volatile AttributeKey[]` .. and on each change we just allocate a new array and copy into that, keeping them ordered by AttributeKey id for fast lookup."
            },
            {
              "comment_username": "franz1981",
              "comment_create_time": "2020-09-03T21:38:51Z",
              "comment_edit_time": "2020-09-03T21:48:46Z",
              "comment_text": "I've added an alternative version using a very expensive copy on write approach using IdentityHashMap (because the effort of building my own data structure seems too much TBH ;) )\r\n\r\nnow with the new PR I got 272 B with a single element in and 9,320 B with 128 elements.\r\n\r\nThe downside of this approach is a very slow add/removal, but a very fast get...with 3 threads as https://github.com/netty/netty/pull/10530#issuecomment-686655480):\r\n\r\n```\r\nBenchmark                                                  (keyCount)   Mode  Cnt          Score          Error  Units\r\nDefaultAttributeMapBenchmark.mixedAttributeAttributeMap             8  thrpt   10  238585811.369 ± 31934592.920  ops/s\r\nDefaultAttributeMapBenchmark.mixedAttributeAttributeMap            32  thrpt   10  243530111.418 ± 11968003.158  ops/s\r\nDefaultAttributeMapBenchmark.mixedAttributeAttributeMap           128  thrpt   10  213268432.193 ±  3327400.122  ops/s\r\nDefaultAttributeMapBenchmark.nextAttributeAttributeMap              8  thrpt   10  267212196.748 ±  4716512.964  ops/s\r\nDefaultAttributeMapBenchmark.nextAttributeAttributeMap             32  thrpt   10  265004087.206 ±  4265242.429  ops/s\r\nDefaultAttributeMapBenchmark.nextAttributeAttributeMap            128  thrpt   10  250883694.371 ±  8523950.126  ops/s\r\nDefaultAttributeMapBenchmark.nextHasAttributeAttributeMap           8  thrpt   10  276186967.502 ± 15894423.501  ops/s\r\nDefaultAttributeMapBenchmark.nextHasAttributeAttributeMap          32  thrpt   10  274964198.700 ± 10002220.257  ops/s\r\nDefaultAttributeMapBenchmark.nextHasAttributeAttributeMap         128  thrpt   10  244889594.173 ± 17344982.642  ops/s\r\n```"
            },
            {
              "comment_username": "franz1981",
              "comment_create_time": "2020-09-04T07:51:28Z",
              "comment_edit_time": "2020-09-05T16:22:16Z",
              "comment_text": "@njhill \r\n\r\n> on each change we just allocate a new array and copy into that, keeping them ordered by AttributeKey id for fast lookup.\r\n\r\nI was thinking to replace the version using the JCTools one in 2 different ways:\r\n\r\n1. using a DefaultAttribute[] array to perform copy-on-write on it, but the hottest method ie to get the DefaultAttribute with a matching AttributeKey need me to re-implement some open-addressing linear-probing hash-map-like logic, that's already well-implemented on IdentityHashMap\r\n2. implementing an optimistic blocking approach with IdentityHashMap to improve the modification cases, but need to roll my own optimistic locking thing (StampedLock isn't a thing on Netty 4)\r\n\r\nThe latter is an improvement over the existing version using CopyOnWrite, will probably create another commit with it, sharing some number \r\n"
            },
            {
              "comment_username": "franz1981",
              "comment_create_time": "2020-09-04T13:45:12Z",
              "comment_edit_time": "2020-09-04T13:45:32Z",
              "comment_text": "I've added a commit with a much lower footprint then 4.1 (!!!) and better performance, but it exploits the assumption that `AttributeKey::id` is a unique identifier and 2 `AttributeKey`s with the same id must be the same instance.\r\n\r\nSome numbers with 3 threads:\r\n```\r\nBenchmark                                                  (keyCount)   Mode  Cnt          Score          Error  Units\r\nDefaultAttributeMapBenchmark.mixedAttributeAttributeMap             8  thrpt   10  236238852.971 ± 13124401.909  ops/s\r\nDefaultAttributeMapBenchmark.mixedAttributeAttributeMap            32  thrpt   10  172406297.975 ± 10845476.359  ops/s\r\nDefaultAttributeMapBenchmark.mixedAttributeAttributeMap           128  thrpt   10  129254512.525 ± 13495237.604  ops/s\r\nDefaultAttributeMapBenchmark.nextAttributeAttributeMap              8  thrpt   10  243347578.948 ±  4626672.738  ops/s\r\nDefaultAttributeMapBenchmark.nextAttributeAttributeMap             32  thrpt   10  170921098.249 ± 13379751.240  ops/s\r\nDefaultAttributeMapBenchmark.nextAttributeAttributeMap            128  thrpt   10  126155183.136 ±  6218903.721  ops/s\r\nDefaultAttributeMapBenchmark.nextHasAttributeAttributeMap           8  thrpt   10  266955427.215 ± 21701892.061  ops/s\r\nDefaultAttributeMapBenchmark.nextHasAttributeAttributeMap          32  thrpt   10  202436066.715 ±  3386971.487  ops/s\r\nDefaultAttributeMapBenchmark.nextHasAttributeAttributeMap         128  thrpt   10  134660416.374 ±   901196.199  ops/s\r\n```\r\n1 thread:\r\n```\r\nBenchmark                                                  (keyCount)   Mode  Cnt         Score         Error  Units\r\nDefaultAttributeMapBenchmark.mixedAttributeAttributeMap             8  thrpt   10  79526576.485 ± 1278861.187  ops/s\r\nDefaultAttributeMapBenchmark.mixedAttributeAttributeMap            32  thrpt   10  56249251.719 ± 5121628.274  ops/s\r\nDefaultAttributeMapBenchmark.mixedAttributeAttributeMap           128  thrpt   10  40366700.099 ±  420161.117  ops/s\r\nDefaultAttributeMapBenchmark.nextAttributeAttributeMap              8  thrpt   10  80353600.668 ± 2049212.619  ops/s\r\nDefaultAttributeMapBenchmark.nextAttributeAttributeMap             32  thrpt   10  55393032.194 ± 1368269.740  ops/s\r\nDefaultAttributeMapBenchmark.nextAttributeAttributeMap            128  thrpt   10  46740820.732 ±  271053.408  ops/s\r\nDefaultAttributeMapBenchmark.nextHasAttributeAttributeMap           8  thrpt   10  92841001.366 ±  544530.315  ops/s\r\nDefaultAttributeMapBenchmark.nextHasAttributeAttributeMap          32  thrpt   10  67314748.684 ±  491967.820  ops/s\r\nDefaultAttributeMapBenchmark.nextHasAttributeAttributeMap         128  thrpt   10  46412138.284 ±  622929.822  ops/s\r\n```\r\nIn short I've implemented 3 versions:\r\n\r\n1. using JCTools Cliff Click IdentityHashMap: better everything (read & write), but worst footprint especially with small number of attributes\r\n2. using ad-hoc copy on write IdentityHashMap: better read, worst ever write, nice footprint (near but higher then 4.1.)\r\n3. using ad-hoc clustered copy on write ordered attribute arrays: read can be log(n), writes can cost n per attribute cluster, footprint less then 4.1  \r\n\r\nAll the 3 versions linearly scale on read and allow to read while writing too, so they are a much better choice then 4.1\r\nProbably the last version (using sorting and binary search) could be improved too"
            },
            {
              "comment_username": "franz1981",
              "comment_create_time": "2020-09-04T15:40:41Z",
              "comment_edit_time": "2020-09-04T15:40:41Z",
              "comment_text": "The last version footprint is:\r\n1 element -> 176 B vs 218 B (4.1)\r\n100 elements -> 5,000 B vs 6,032 B (4.1)\r\n\r\n\r\n"
            },
            {
              "comment_username": "njhill",
              "comment_create_time": "2020-09-04T16:14:02Z",
              "comment_edit_time": "2020-09-04T16:14:02Z",
              "comment_text": "@franz1981 this is looking great :)   let me know when it's ready to review"
            },
            {
              "comment_username": "franz1981",
              "comment_create_time": "2020-09-04T16:17:35Z",
              "comment_edit_time": "2020-09-04T16:17:35Z",
              "comment_text": "@njhill you have 3 commits and 3 different data structures to review :P pick the one you like "
            },
            {
              "comment_username": "njhill",
              "comment_create_time": "2020-09-04T16:45:11Z",
              "comment_edit_time": "2020-09-04T16:45:11Z",
              "comment_text": "@franz1981 I like number 3 but I wonder how about just a flat ordered array instead of the clustering? IIUC we're not optimizing for high write/read ratio or large numbers of attributes"
            },
            {
              "comment_username": "franz1981",
              "comment_create_time": "2020-09-04T16:49:13Z",
              "comment_edit_time": "2020-09-04T16:49:13Z",
              "comment_text": "@njhill that's correct...or maybe not...I mean, probably we need to optimize for high perf single (?) threaded read if I think about the MQTT 5 case..."
            },
            {
              "comment_username": "normanmaurer",
              "comment_create_time": "2020-09-04T19:18:49Z",
              "comment_edit_time": "2020-09-04T19:18:49Z",
              "comment_text": "I hadn't time to check this one yet... but yeah it should be optimised for \"single thread\" use with < 4 attributes. "
            },
            {
              "comment_username": "franz1981",
              "comment_create_time": "2020-09-04T19:54:24Z",
              "comment_edit_time": "2020-09-05T16:24:37Z",
              "comment_text": "@normanmaurer read mostly right? In this case...why clustering the keys identities with a 4 sized array? I am talking about the original code...\r\nI have left the 4 sized array assumption because I was thinking was there for some reason, if not, I can just use a single array (that would hurt write scalability obviously) or the copy on write IdentityHashMap approach too"
            },
            {
              "comment_username": "franz1981",
              "comment_create_time": "2020-09-09T11:49:27Z",
              "comment_edit_time": "2020-09-09T11:50:04Z",
              "comment_text": "I believe this one is already ready to be reviewed/merged as it is, unless we want to:\r\n1. drop the original idea of clustering key ids with a fixed array of 4 and using a single array (it would further lower the memory footprint, but lookup perf could be 1/4 of the clustered one)\r\n2. replace the new idea of using a binary search on attribute key by using some Open Addressing custom identity hash map (but using a single array of attrbutes like 1.): this would turn O(logn) into a O(1) ideally (assuming fair distributions of key ids)"
            },
            {
              "comment_username": "njhill",
              "comment_create_time": "2020-09-11T17:06:32Z",
              "comment_edit_time": "2020-09-11T17:06:32Z",
              "comment_text": "@franz1981 I'm ok with this but would prefer (1.) because it's simpler, less memory, and I don't think the sharding should matter given we're optimizing for n < 4 case that @normanmaurer said..."
            },
            {
              "comment_username": "franz1981",
              "comment_create_time": "2020-09-12T09:50:54Z",
              "comment_edit_time": "2020-09-12T09:50:54Z",
              "comment_text": "@njhill agree! The approach on 1 seems the way to go and would simplify the code as well ;)\r\nEarly next week I will implement and bench it to compare with the other approaches"
            },
            {
              "comment_username": "franz1981",
              "comment_create_time": "2020-09-14T11:20:44Z",
              "comment_edit_time": "2020-09-14T11:20:44Z",
              "comment_text": "Just implemented the copy-on-write single order array approach, results below:\r\nthis PR:\r\n\r\n3 threads\r\n```\r\nBenchmark                                               (keyCount)   Mode  Cnt          Score          Error  Units\r\nDefaultAttributeMapBenchmark.nextAttributeAttributeMap           2  thrpt   10  325650205.797 ± 11526663.825  ops/s\r\nDefaultAttributeMapBenchmark.nextAttributeAttributeMap           8  thrpt   10  225747059.333 ± 19824181.020  ops/s\r\nDefaultAttributeMapBenchmark.nextAttributeAttributeMap          16  thrpt   10  181140297.636 ±  2786344.073  ops/s\r\n```\r\n1 threads\r\n```\r\nBenchmark                                               (keyCount)   Mode  Cnt          Score        Error  Units\r\nDefaultAttributeMapBenchmark.nextAttributeAttributeMap           2  thrpt   10  111259701.792 ± 230536.960  ops/s\r\nDefaultAttributeMapBenchmark.nextAttributeAttributeMap           8  thrpt   10   79491273.376 ± 313734.720  ops/s\r\nDefaultAttributeMapBenchmark.nextAttributeAttributeMap          16  thrpt   10   63054500.114 ± 166193.356  ops/s\r\n```\r\n\r\n4,1 `-XX:-UseBiasedLocking`\r\n\r\n3 threads\r\n```\r\nBenchmark                                               (keyCount)   Mode  Cnt         Score        Error  Units\r\nDefaultAttributeMapBenchmark.nextAttributeAttributeMap           2  thrpt   10  14442012.527 ± 618428.625  ops/s\r\nDefaultAttributeMapBenchmark.nextAttributeAttributeMap           8  thrpt   10  21441017.685 ± 403800.528  ops/s\r\nDefaultAttributeMapBenchmark.nextAttributeAttributeMap          16  thrpt   10  21141198.068 ± 873168.213  ops/s\r\n```\r\n\r\n1 thread\r\n```\r\nBenchmark                                               (keyCount)   Mode  Cnt         Score        Error  Units\r\nDefaultAttributeMapBenchmark.nextAttributeAttributeMap           2  thrpt   10  39107346.941 ± 113486.754  ops/s\r\nDefaultAttributeMapBenchmark.nextAttributeAttributeMap           8  thrpt   10  39333390.111 ± 237693.538  ops/s\r\nDefaultAttributeMapBenchmark.nextAttributeAttributeMap          16  thrpt   10  37795765.547 ± 258702.084  ops/s\r\n```\r\n\r\n4,1 `-XX:+UseBiasedLocking`\r\n\r\n3 threads\r\n```\r\nBenchmark                                               (keyCount)   Mode  Cnt         Score         Error  Units\r\nDefaultAttributeMapBenchmark.nextAttributeAttributeMap           2  thrpt   10  14081081.878 ± 1068426.289  ops/s\r\nDefaultAttributeMapBenchmark.nextAttributeAttributeMap           8  thrpt   10  21287469.455 ± 1855659.866  ops/s\r\nDefaultAttributeMapBenchmark.nextAttributeAttributeMap          16  thrpt   10  21864656.131 ±  349921.039  ops/s\r\n```\r\n1 thread\r\n```\r\nBenchmark                                               (keyCount)   Mode  Cnt         Score       Error  Units\r\nDefaultAttributeMapBenchmark.nextAttributeAttributeMap           2  thrpt   10  37461220.507 ± 78313.574  ops/s\r\nDefaultAttributeMapBenchmark.nextAttributeAttributeMap           8  thrpt   10  37095719.140 ± 85102.342  ops/s\r\nDefaultAttributeMapBenchmark.nextAttributeAttributeMap          16  thrpt   10  37020149.395 ± 90007.037  ops/s\r\n```\r\nIn short: this PR now deliver a non blocking scalable lookup that on single threaded case is 3 or 2 times faster depending the number of attributes. The memory footprint is much lower too."
            },
            {
              "comment_username": "franz1981",
              "comment_create_time": "2020-09-14T13:55:44Z",
              "comment_edit_time": "2020-09-14T14:04:31Z",
              "comment_text": "@normanmaurer not so surprising, but a single threaded brute force linear search seems better then a binary one with <= 16 keys, see\r\n```\r\nBenchmark                                               (keyCount)   Mode  Cnt          Score         Error  Units\r\nDefaultAttributeMapBenchmark.nextAttributeAttributeMap           2  thrpt   10  126045059.852 ±  196063.855  ops/s\r\nDefaultAttributeMapBenchmark.nextAttributeAttributeMap           8  thrpt   10   93594020.735 ±  314337.683  ops/s\r\nDefaultAttributeMapBenchmark.nextAttributeAttributeMap          16  thrpt   10   66910641.578 ±  503629.078  ops/s\r\n```\r\nGiven that this behaviour is very machine (and heap size, COOPS, alignment) dependent I don't think it worths to be implemented on this PR although it seems to squeeze the very last perf from tiny/small attribute sets."
            },
            {
              "comment_username": "normanmaurer",
              "comment_create_time": "2020-09-15T14:49:49Z",
              "comment_edit_time": "2020-09-15T14:49:49Z",
              "comment_text": "@njhill @carl-mastrangelo PTAL when you have time "
            },
            {
              "comment_username": "franz1981",
              "comment_create_time": "2020-09-15T16:13:21Z",
              "comment_edit_time": "2020-09-15T16:13:38Z",
              "comment_text": "@njhill the problem is the target of such optimization: I bet that is arch dependent + depends by the alignment of the array we search into (the linear search seems to use vmovq to load batch of reference array) and when we cross the cache line on it. Given these assumptions I haven't many machines to try this and be sure it works better then binary search for a specific parameter. Maybe @nitsanw has played with stuff like these in the past (while having fun with superword optimizations?)."
            },
            {
              "comment_username": "franz1981",
              "comment_create_time": "2020-09-19T07:40:34Z",
              "comment_edit_time": "2020-09-19T07:40:34Z",
              "comment_text": "@carl-mastrangelo agree and I can add a couple of tests to enforce the current behaviour while improving doc, if needed. I just hope to not break anything due to the strengthening of behaviour of this PR :) "
            },
            {
              "comment_username": "normanmaurer",
              "comment_create_time": "2020-09-21T09:37:37Z",
              "comment_edit_time": "2020-09-21T09:37:37Z",
              "comment_text": "@franz1981 I am a bit concerned about changing the behaviour of the current implementation (even if it was not really well specified before). Can we please keep the old behaviour ?"
            },
            {
              "comment_username": "franz1981",
              "comment_create_time": "2020-09-21T13:45:47Z",
              "comment_edit_time": "2020-09-21T13:53:33Z",
              "comment_text": "@normanmaurer Thanks for the suggestion; I've found a way to preserve the weird original logic and the nice smaller new memory footprint (and helping a bit GC too :) ), I'm now squashing the new changes."
            }
          ]
        },
        {
          "issue_url": "https://github.com/netty/netty/issues/10527",
          "issue_title": "Add `null` rule check in `rules` array of RuleBasedIpFilter",
          "issue_number": 10527,
          "issue_text": "Motivation:\r\n\r\nWe can filter out `null` rules while initializing the instance of `RuleBasedIpFilter` so we don't have to keep checking for `null` rules while iterating through `rules` array in `for loop` which is just a waste of CPU cycles.\r\n\r\nModification:\r\nAdded `null` rule check inside the constructor.\r\n\r\nResult:\r\nNo more wasting CPU cycles on check the `null` rule each time in `for loop` and makes the overall operation more faster.\r\n",
          "issue_comments": [
            {
              "comment_username": "hyperxpro",
              "comment_create_time": "2020-09-02T09:27:02Z",
              "comment_edit_time": "2020-09-02T09:27:02Z",
              "comment_text": "@normanmaurer @chrisvest \r\n\r\nI was thinking about marking `RuleBasedIpFilter` as `deprecated` because we now have `IpSubnetFiler` which is way better than `RuleBasedIpFilter` in terms of performance and does the same thing. What do you think?"
            },
            {
              "comment_username": "netty-bot",
              "comment_create_time": "2020-09-02T09:29:03Z",
              "comment_edit_time": "2020-09-02T09:29:03Z",
              "comment_text": "Can one of the admins verify this patch?"
            },
            {
              "comment_username": "chrisvest",
              "comment_create_time": "2020-09-02T11:57:58Z",
              "comment_edit_time": "2020-09-02T11:57:58Z",
              "comment_text": "I agree we can't change signatures or the behaviour. Instead of the null checks, we could filter in the constructor if there are any nulls. I also don't think we should deprecate the class, since it's more general than the binary search one. The docs could call out that a specialised alternative exists, though. "
            },
            {
              "comment_username": "hyperxpro",
              "comment_create_time": "2020-09-02T12:00:11Z",
              "comment_edit_time": "2020-09-02T12:00:11Z",
              "comment_text": "We are already throwing `NullPointerException` for the `rules` array right now. I don't think throwing `NullPointerException` again for `rule` will break something.\r\n\r\nhttps://github.com/netty/netty/blob/4ececfb70d35201f84f30a480e1b5f6f87a370b9/handler/src/main/java/io/netty/handler/ipfilter/RuleBasedIpFilter.java#L40"
            },
            {
              "comment_username": "hyperxpro",
              "comment_create_time": "2020-09-02T19:05:17Z",
              "comment_edit_time": "2020-09-02T19:05:17Z",
              "comment_text": "Few security points to consider in the current code:\r\n\r\n1. We accept connection when it doesn't match any rule and there is no way to change it. There could be a use case where we only want to allow access from specific IP addresses. Let's say, Netty should accept connection from 10.1.2.0/24 which is the IP address range of XYZ Company.\r\n\r\n2. We accept the connection if we got a `null` rule.\r\n\r\nLet's say, an IP address range 192.168.1.0/24 is sending malicious requests like SQL injection, XSS, etc to the Netty server.\r\nOur list of rules looks something like this:\r\n172.16.0.0/16 -> REJECT\r\n172.20.0.0/16 -> REJECT\r\n`null` (By mistake)\r\n192.168.1.0/24 -> REJECT\r\n\r\nIn this case:\r\n```java\r\nif (rule == null) {\r\n    break;\r\n}\r\n```\r\nThis code will break the loop and we'll trigger `return true;` and accept the connection for 192.168.1.0/24.\r\n_______________\r\n\r\nConclusion:\r\n1. We need to give users the ability to modify the default action type if rule is found or not.\r\n2. Filter out `null` rules because we can't do anything with `null` rules and they don't make sense at the end."
            },
            {
              "comment_username": "chrisvest",
              "comment_create_time": "2020-09-11T13:20:53Z",
              "comment_edit_time": "2020-09-11T13:20:53Z",
              "comment_text": "@normanmaurer Now that the API compatibility is preserved, I think these changes are good.\r\n\r\nLooking at the git history, I think that the `if (rule == null) { break; }` is a bug, and that it was most likely meant to have been a `continue` originally."
            },
            {
              "comment_username": "hyperxpro",
              "comment_create_time": "2020-09-29T08:31:15Z",
              "comment_edit_time": "2020-09-29T08:31:15Z",
              "comment_text": "@normanmaurer Ping..."
            }
          ]
        },
        {
          "issue_url": "https://github.com/netty/netty/issues/10525",
          "issue_title": "Invalid Argument Exception in WeightedFairQueueByteDistributor",
          "issue_number": 10525,
          "issue_text": "I tracked down a bug which causes clients to fail several requests when visiting our Netty HTTP/2 server from Chome.  Typically the Chrome Dev Tools page shows some requests failed due to the connection being closed early.  It seemingly only happened when coming from the browser.   It additionally only seems to happen for very new connections, longer lived connections don exhibit the problem.  Lastly, this bug appears to be pretty sensitive to timing.  Slowing the rate of requests seems to make this bug disappear.   \r\n\r\nThe behavior of the bug is:\r\n\r\n1.  Client sends some streams to the server, and then adjusts the priorities and dependencies.\r\n2.  The server uses the WeightedFairQueueByteDistributor rather than the UniformStreamByteDistributor\r\n3.  The exception (shown below) is thrown, causing the Http2 handler to catch it and send a Go away.  \r\n4.  After the go away, the underlying connection is closed.\r\n\r\n```\r\nio.netty.handler.codec.http2.Http2Exception: Error flushing\r\n\tat io.netty.handler.codec.http2.Http2Exception.connectionError(Http2Exception.java:117)\r\n\tat io.netty.handler.codec.http2.Http2ConnectionHandler.flush(Http2ConnectionHandler.java:193)\r\n\tat io.netty.handler.codec.http2.Http2ConnectionHandler.channelWritabilityChanged(Http2ConnectionHandler.java:428)\r\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelWritabilityChanged(AbstractChannelHandlerContext.java:441)\r\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelWritabilityChanged(AbstractChannelHandlerContext.java:428)\r\n\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelWritabilityChanged(AbstractChannelHandlerContext.java:421)\r\n\tat io.netty.channel.DefaultChannelPipeline$HeadContext.channelWritabilityChanged(DefaultChannelPipeline.java:1433)\r\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelWritabilityChanged(AbstractChannelHandlerContext.java:441)\r\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelWritabilityChanged(AbstractChannelHandlerContext.java:428)\r\n\tat io.netty.channel.DefaultChannelPipeline.fireChannelWritabilityChanged(DefaultChannelPipeline.java:931)\r\n\tat io.netty.channel.ChannelOutboundBuffer.fireChannelWritabilityChanged(ChannelOutboundBuffer.java:628)\r\n\tat io.netty.channel.ChannelOutboundBuffer.setWritable(ChannelOutboundBuffer.java:594)\r\n\tat io.netty.channel.ChannelOutboundBuffer.decrementPendingOutboundBytes(ChannelOutboundBuffer.java:196)\r\n\tat io.netty.channel.ChannelOutboundBuffer.remove(ChannelOutboundBuffer.java:273)\r\n\tat io.netty.channel.ChannelOutboundBuffer.removeBytes(ChannelOutboundBuffer.java:352)\r\n\tat io.netty.channel.epoll.AbstractEpollStreamChannel.writeBytesMultiple(AbstractEpollStreamChannel.java:305)\r\n\tat io.netty.channel.epoll.AbstractEpollStreamChannel.doWriteMultiple(AbstractEpollStreamChannel.java:510)\r\n\tat io.netty.channel.epoll.AbstractEpollStreamChannel.doWrite(AbstractEpollStreamChannel.java:422)\r\n\tat io.netty.channel.AbstractChannel$AbstractUnsafe.flush0(AbstractChannel.java:930)\r\n\tat io.netty.channel.epoll.AbstractEpollChannel$AbstractEpollUnsafe.epollOutReady(AbstractEpollChannel.java:532)\r\n\tat io.netty.channel.epoll.EpollEventLoop.processReady(EpollEventLoop.java:465)\r\n\tat io.netty.channel.epoll.EpollEventLoop.run(EpollEventLoop.java:378)\r\n\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)\r\n\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\r\n\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\r\n\tat java.base/java.lang.Thread.run(Thread.java:834)\r\nCaused by: java.lang.IllegalArgumentException: e.priorityQueueIndex(): 0 (expected: -1) + e: {streamId 7 streamableBytes 0 activeCountForTree 6 pseudoTimeQueueIndex 0 pseudoTimeToWrite 119223 pseudoTime 0 flags 4 pseudoTimeQueue.size() 1 stateOnlyQueueIndex 0 parent.streamId 13} [{streamId 15 streamableBytes 185716 activeCountForTree 6 pseudoTimeQueueIndex 0 pseudoTimeToWrite 0 pseudoTime 0 flags 5 pseudoTimeQueue.size() 1 stateOnlyQueueIndex -1 parent.streamId 7} [{streamId 17 streamableBytes 98524 activeCountForTree 5 pseudoTimeQueueIndex 0 pseudoTimeToWrite 0 pseudoTime 0 flags 5 pseudoTimeQueue.size() 1 stateOnlyQueueIndex -1 parent.streamId 15} [{streamId 19 streamableBytes 167141 activeCountForTree 4 pseudoTimeQueueIndex 0 pseudoTimeToWrite 0 pseudoTime 0 flags 5 pseudoTimeQueue.size() 1 stateOnlyQueueIndex -1 parent.streamId 17} [{streamId 21 streamableBytes 1445060 activeCountForTree 3 pseudoTimeQueueIndex 0 pseudoTimeToWrite 0 pseudoTime 0 flags 5 pseudoTimeQueue.size() 1 stateOnlyQueueIndex -1 parent.streamId 19} [{streamId 23 streamableBytes 2920 activeCountForTree 2 pseudoTimeQueueIndex 0 pseudoTimeToWrite 0 pseudoTime 0 flags 5 pseudoTimeQueue.size() 1 stateOnlyQueueIndex -1 parent.streamId 21} [{streamId 25 streamableBytes 10 activeCountForTree 1 pseudoTimeQueueIndex 0 pseudoTimeToWrite 0 pseudoTime 0 flags 5 pseudoTimeQueue.size() 0 stateOnlyQueueIndex -1 parent.streamId 23} []]]]]]]\r\n\tat io.netty.util.internal.DefaultPriorityQueue.offer(DefaultPriorityQueue.java:88)\r\n\tat io.netty.util.internal.DefaultPriorityQueue.offer(DefaultPriorityQueue.java:31)\r\n\tat io.netty.handler.codec.http2.WeightedFairQueueByteDistributor$State.offerPseudoTimeQueue(WeightedFairQueueByteDistributor.java:671)\r\n\tat io.netty.handler.codec.http2.WeightedFairQueueByteDistributor.distributeToChildren(WeightedFairQueueByteDistributor.java:340)\r\n\tat io.netty.handler.codec.http2.WeightedFairQueueByteDistributor.distribute(WeightedFairQueueByteDistributor.java:303)\r\n\tat io.netty.handler.codec.http2.WeightedFairQueueByteDistributor.distributeToChildren(WeightedFairQueueByteDistributor.java:325)\r\n\tat io.netty.handler.codec.http2.WeightedFairQueueByteDistributor.distribute(WeightedFairQueueByteDistributor.java:303)\r\n\tat io.netty.handler.codec.http2.WeightedFairQueueByteDistributor.distributeToChildren(WeightedFairQueueByteDistributor.java:325)\r\n\tat io.netty.handler.codec.http2.WeightedFairQueueByteDistributor.distribute(WeightedFairQueueByteDistributor.java:303)\r\n\tat io.netty.handler.codec.http2.WeightedFairQueueByteDistributor.distributeToChildren(WeightedFairQueueByteDistributor.java:325)\r\n\tat io.netty.handler.codec.http2.WeightedFairQueueByteDistributor.distribute(WeightedFairQueueByteDistributor.java:303)\r\n\tat io.netty.handler.codec.http2.WeightedFairQueueByteDistributor.distributeToChildren(WeightedFairQueueByteDistributor.java:325)\r\n\tat io.netty.handler.codec.http2.WeightedFairQueueByteDistributor.distribute(WeightedFairQueueByteDistributor.java:303)\r\n\tat io.netty.handler.codec.http2.WeightedFairQueueByteDistributor.distributeToChildren(WeightedFairQueueByteDistributor.java:325)\r\n\tat io.netty.handler.codec.http2.WeightedFairQueueByteDistributor.distribute(WeightedFairQueueByteDistributor.java:273)\r\n\tat io.netty.handler.codec.http2.DefaultHttp2RemoteFlowController$WritabilityMonitor.writePendingBytes(DefaultHttp2RemoteFlowController.java:627)\r\n\tat io.netty.handler.codec.http2.DefaultHttp2RemoteFlowController.writePendingBytes(DefaultHttp2RemoteFlowController.java:267)\r\n\tat io.netty.handler.codec.http2.Http2ConnectionHandler.flush(Http2ConnectionHandler.java:188)\r\n\t... 24 more\r\n```\r\n\r\n### Steps to reproduce\r\n\r\nI don't have an exact way to reproduce it yet, but I can get it to happen about half the time after killing the connections in Chrome.   I do have the HTTP/2 state logged by Chrome for a failure though.  I can provide it if it would help.\r\n\r\n\r\n### Minimal yet complete reproducer code (or URL to code)\r\n\r\n### Netty version\r\n4.1.51\r\n\r\n### JVM version (e.g. `java -version`)\r\nJDK 11\r\n",
          "issue_comments": [
            {
              "comment_username": "carl-mastrangelo",
              "comment_create_time": "2020-09-02T01:25:15Z",
              "comment_edit_time": "2020-09-02T01:25:15Z",
              "comment_text": "A side note: It appears Http2FrameCodec doesn't offer a way to change out the stream distributor to the Uniform one, which would allow us to work around this bug.   Any objections to a PR to allow this to be set?"
            },
            {
              "comment_username": "mostroverkhov",
              "comment_create_time": "2020-09-02T05:05:45Z",
              "comment_edit_time": "2020-09-02T05:05:45Z",
              "comment_text": "For workaround with `4.1.51` one can\r\n\r\n```\r\nHttp2Connection connection = http2FrameCodec.connection();\r\n      connection\r\n          .remote()\r\n          .flowController(\r\n              new DefaultHttp2RemoteFlowController(\r\n                  connection, new UniformStreamByteDistributor(connection)));\r\n```"
            },
            {
              "comment_username": "normanmaurer",
              "comment_create_time": "2020-09-02T06:55:23Z",
              "comment_edit_time": "2020-09-02T06:55:23Z",
              "comment_text": "@carl-mastrangelo sure a \"nicer\" way of changing it would be good as well. "
            },
            {
              "comment_username": "normanmaurer",
              "comment_create_time": "2020-09-09T09:24:02Z",
              "comment_edit_time": "2020-09-09T09:24:02Z",
              "comment_text": "@carl-mastrangelo did you want to todo a PR ?"
            },
            {
              "comment_username": "carl-mastrangelo",
              "comment_create_time": "2020-09-11T22:43:23Z",
              "comment_edit_time": "2020-09-11T22:43:23Z",
              "comment_text": "@normanmaurer I don't have a fix for the IAE, it's somewhat beyond me.  I also took a stab at updating AbstractHttp2ConnectionHandlerBuilder, but it too is much more complex than I thought.   I'm going to renege for now since it turned out to be more work than expected."
            }
          ]
        },
        {
          "issue_url": "https://github.com/netty/netty/issues/10524",
          "issue_title": "CAN networking",
          "issue_number": 10524,
          "issue_text": "I'm the main developer of the [JavaCAN](https://github.com/pschichtel/JavaCAN) project. The library, in its current stable release, implements Java's Selector using a epoll via JNI, but that API is very restrictive in how it can be extended with many parts of the API hardwired to the existing TCP and UDP channels. It's hard to implement the a Selector and SelectableChannel while correctly using epoll.\r\n\r\nFor the 3.x release of the library we planned to replace the existing custom epoll implementation with a Netty integration, in order to utilize Netty's existing infrastructure. @mscheibler [asked the question about this on stackoverflow](https://stackoverflow.com/questions/63568442/using-nettys-epoll-implementation-for-linux-socketcan-communication) after looking into the code.\r\n\r\nI had a quick look at it myself and it seems there is indeed quite a few assumptions on IP communication as @normanmaurer on said stackoverflow.\r\n\r\nI'm opening this issue to get more input from the netty team and gather the issues we face while trying to get this working with JavaCAN.\r\n\r\n",
          "issue_comments": []
        },
        {
          "issue_url": "https://github.com/netty/netty/issues/10508",
          "issue_title": "HttpPostMultipartRequestDecoder performance regression",
          "issue_number": 10508,
          "issue_text": "### Expected behavior\r\n\r\n#10226 improves performance.\r\n\r\n### Actual behavior\r\n\r\nThe decoder is now at least 100x slower than before iff it is offered a direct buffer.\r\n\r\nIf the decoder is offered a direct buffer it now creates a bunch of 64b direct buffers during parsing, where before it always used heap buffers.\r\nUsing a `PooledByteBufAllocator` that prefers direct buffers did not solve the issue.\r\nUsing a `(Un)PooledByteBufAllocator` that prefers heap buffers normalizes performance again.\r\n\r\nGiven that this issue was not listed in the announcement under important, I would think that this was not intentional.\r\n\r\n### Steps to reproduce\r\n\r\nCreate a HttpPostMultipartRequestDecoder and offer around a 1000 chunks (64k); in 4.1.49 this runs in seconds, whereas in 4.1.50 it takes minutes.\r\n\r\n### Minimal yet complete reproducer code (or URL to code)\r\n\r\nWe have [this](https://github.com/apache/flink/blob/e991de19337591cde444fbede010cb8bdc7f118f/flink-runtime/src/test/java/org/apache/flink/runtime/rest/FileUploadHandlerTest.java) test in Apache Flink that exhibits the problem.\r\nI can try to write a minimal reproducer if requested; may take some time to figure out how to correctly create the requests :/\r\n\r\n### Netty version\r\n\r\n4.1.50\r\n\r\n### JVM version (e.g. `java -version`)\r\n\r\n8\r\n\r\n### OS version (e.g. `uname -a`)\r\n\r\nWindows 10, but we also saw it on ubuntu linux",
          "issue_comments": [
            {
              "comment_username": "fredericBregier",
              "comment_create_time": "2020-08-27T13:51:03Z",
              "comment_edit_time": "2020-08-27T13:51:03Z",
              "comment_text": "My 2 cents:\r\nFirst change:\r\n- Before, it was a `copy` which limits the max, \r\n- now is a `buf.alloc.buffer(...).writeByte(buf)`  => Pooled buffer using the original ByteBufAllocator from the `buf`\r\n\r\nSecond change, which is probably the one you point out:\r\n- Before (2 places): `Unpooled.buffer(64)` (Unpooled implicit)\r\n- After (same): `undecodedChunk.alloc().heapBuffer(64)`\r\n\r\nThis should use the ByteBufAllocator used for undecoded, which in turns should be (if I understand) the same allocator than original `buf` (so the one from IO) since either it is the same, or a new allocated one using the same allocator.\r\n\r\nI could suggest to replace `undecodedChunk.alloc().heapBuffer(64)` by `undecodedChunk.alloc().buffer(64, 64)` (here there should not be any more than 64), but as you noticed that putting a `PooledByteBufAllocator` to direct is not correct, but heap seems ok, then I finally could suggest: `undecodedChunk.alloc().heapBuffer(64, 64)`, which is almost what we have there...\r\n\r\n@normanmaurer Any suggestion since I do not have a clear mind on this... ?\r\n"
            },
            {
              "comment_username": "fredericBregier",
              "comment_create_time": "2020-08-28T22:37:14Z",
              "comment_edit_time": "2020-08-28T22:37:14Z",
              "comment_text": "@zentol I tried to reproduce using only Netty, but I can't get the numbers you've got.\r\n\r\nCould you have a reproducer code with Netty only (almost a Junit or simple example)?\r\nWhat kind of data are you sending (which forms and numbers)?\r\nAnd to be sure, your tests shows that Direct buffers are not ok, while Heap buffers are ?\r\nThanks"
            },
            {
              "comment_username": "fredericBregier",
              "comment_create_time": "2020-09-02T06:33:55Z",
              "comment_edit_time": "2020-09-02T06:33:55Z",
              "comment_text": "@zentol Note that I used 4.1.51 and I can't reproduce the issue you raised... "
            },
            {
              "comment_username": "fredericBregier",
              "comment_create_time": "2020-09-03T07:07:42Z",
              "comment_edit_time": "2020-09-03T07:07:42Z",
              "comment_text": "@zentol I tried another time, but the numbers I've got are almost the same whatever the allocation:\r\n- PooledHeap, PooledDirect, PooledBuffer (from undecodedChunked\r\n- UnpooledHeap, UnpooledDirect, UnpooledBuffer\r\n\r\nI've tried both 1000 chunks for big data (64KB each) so 1000 decoded vars or 1000 chunks for small data (4KB each but 16 times so still chunks of 64KB each) so 16000 decoded vars, and the results are the same : \r\n- 4KB data size: roughly 230ms each test (16000 data) (highest - lowest = 5ms)\r\n- 64KB data size: roughly 2900ms each test (1000 data), the difference being the data written on disk due to large size (> 16KB)  (highest - lowest = 310ms, but not so relevant since disk access is most of the issue as without disk access, there is no issue)\r\n- 64KB data size: roughly 190ms each test (1000 data), the difference being the data not written on disk (memory only) (highest - lowest = 8ms)\r\n\r\nSo my conclusion until further information from you is that there is no issue there... Either I'm missing something, either it was fixed in 4.1.51..."
            },
            {
              "comment_username": "zentol",
              "comment_create_time": "2020-09-14T09:07:21Z",
              "comment_edit_time": "2020-09-14T09:07:21Z",
              "comment_text": "@fredericBregier I did not test it with 4.1.51 due to #10425. I will check whether the issue persists in 4.1.52, and if so try to create a reproducer."
            },
            {
              "comment_username": "zentol",
              "comment_create_time": "2020-09-14T13:03:32Z",
              "comment_edit_time": "2020-09-14T13:03:32Z",
              "comment_text": "Ok, I think I got closer to the problem.\\r\\n\\r\\nI wrote a reproducing test, which ran fast in 4.1.49 and slow in 4.1.50, but when I moved it into a separate project it suddenly ran fast either way.\\r\\nI then noticed that the test we used so far also uses a `ResourceLeakDetector` set to `PARANOID`. If I set the level to `ADVANCED` it runs quickly with both versions, but `PARANOID` appears to slow things down way more in 4.1.50+.\\r\\n\\r\\nHere's the test in question:\\r\\n\\r\\n```\\r\\npublic class RegressionTest {\\r\\n\\r\\n\\t@Rule\\r\\n\\tpublic final TemporaryFolder temporaryFolder = new TemporaryFolder();\\r\\n\\r\\n\\tstatic {\\r\\n\\t\\tResourceLeakDetector.setLevel(ResourceLeakDetector.Level.PARANOID);\\r\\n\\t}\\r\\n\\r\\n\\t@Test\\r\\n\\tpublic void testRegression() throws Exception {\\r\\n\\t\\tFile file = temporaryFolder.newFile();\\r\\n\\t\\ttry (RandomAccessFile rw = new RandomAccessFile(file, \"rw\")) {\\r\\n\\t\\t\\trw.setLength(1024 * 1024 * 5);\\r\\n\\t\\t}\\r\\n\\t\\tDefaultHttpDataFactory httpDataFactory = new DefaultHttpDataFactory(true);\\r\\n\\r\\n\\t\\tHttpRequest httpRequest = new DefaultFullHttpRequest(HttpVersion.HTTP_1_1, HttpMethod.POST, \"\");\\r\\n\\t\\tHttpPostRequestEncoder bodyRequestEncoder = new HttpPostRequestEncoder(httpDataFactory, httpRequest, true);\\r\\n\\t\\tbodyRequestEncoder.addBodyFileUpload(\"file\", file, \"application/octet-stream\", false);\\r\\n\\r\\n\\t\\tHttpRequest firstHttpRequest = bodyRequestEncoder.finalizeRequest();\\r\\n\\t\\tHttpPostRequestDecoder httpPostRequestDecoder = new HttpPostRequestDecoder(httpDataFactory, firstHttpRequest);\\r\\n\\r\\n\\t\\tByteBufAllocator allocator = PooledByteBufAllocator.DEFAULT;\\r\\n\\t\\tfor (HttpContent httpContent = bodyRequestEncoder.readChunk(allocator); httpContent != null; httpContent = bodyRequestEncoder.readChunk(allocator)) {\\r\\n\\t\\t\\thttpPostRequestDecoder.offer(httpContent);\\r\\n\\t\\t}\\r\\n\\t}\\r\\n}\\r\\n```"
            },
            {
              "comment_username": "fredericBregier",
              "comment_create_time": "2020-09-14T13:59:19Z",
              "comment_edit_time": "2020-09-14T13:59:19Z",
              "comment_text": "@zentol Effctively, with PARANOID level set, chunks are quite high consuming time, while other levels are quite quickier. On my tests (same than before, based on 4.1.52):  whatever the pooled/unpooled heap/direct buffers\r\n- No detector: 225 to 230ms\r\n- Simple: 229 to 233ms\r\n- Advanced; 228 to 245ms\r\n- Paranoid:  147.000ms\r\nSo yes, there is a hug gap using Paranoid (almost x600). Maybe you can stay in at most Advanced mode ?"
            },
            {
              "comment_username": "fredericBregier",
              "comment_create_time": "2020-09-14T14:02:58Z",
              "comment_edit_time": "2020-09-14T14:02:58Z",
              "comment_text": "@normanmaurer However I have a question for you, do you think this kind of gap is normal ?\r\nI do know that Paranoid is not for Production, but maybe it is so huge that it could not be usable. Note however that I used Paranoid level for a long time is all my tests and did not found any issue about on my side."
            },
            {
              "comment_username": "zentol",
              "comment_create_time": "2020-09-15T09:42:38Z",
              "comment_edit_time": "2020-09-15T09:42:38Z",
              "comment_text": "Using Advanced would be workaround we would go for, but this runtime increase on Paranoid does not seem normal. It was perfectly fine to use Paranoid in earlier versions, and it would be unfortunate to weaken our tests now with never versions. :/"
            },
            {
              "comment_username": "normanmaurer",
              "comment_create_time": "2020-09-21T13:25:50Z",
              "comment_edit_time": "2020-09-21T13:25:50Z",
              "comment_text": "sorry for the late reply... The problem with \"paranoid\" is that it basically creates one stack trace per `ByteBuf` access which can be very expensive. So these kind of performance drops can happen."
            },
            {
              "comment_username": "fredericBregier",
              "comment_create_time": "2020-09-21T18:47:38Z",
              "comment_edit_time": "2020-09-21T18:47:38Z",
              "comment_text": "@normanmaurer Thanks!\\r\\nFor me, it's fine as is.\\r\\nIf I'm correct, changing the number of items tracked through `io.netty.leakDetection.targetRecords` will not change every access to be checked but just limit the number of retained items ?\\r\\n\\r\\nI believe that one could set specific values for `io.netty.leakDetection.samplingInterval` (default being 128 to a small value (still power of 2, such as 16 to reduce the PARANOID effect to a smaller part (If I'm correct, the main difference between ADVANCED and PARANOID is that this samplingInterval is ignored within PARANOID ?).\\r\\n\\r\\nI checked using level = `ADVANCED` and `io.netty.leakDetection.samplingInterval` to: (using Pooled one)\\r\\n- compared to `PARANOID`: 140.000ms whatever Direct or Heap\\r\\n- 8 : 140.000ms Direct vs 4.000ms Heap, so almost the same as PARANOID, except using Heap\\r\\n- 16 : 210ms whatever\\r\\n- 128 (default) : 200ms whatever Direct or Heap\\r\\n- 12 (*I know that the code said it should be a power of 2): 590ms Direct 420ms Heap\\r\\nSo maybe 12 could be a good tradeof ? (more trace but not as many as in PARANOID).\\r\\n\\r\\nAgain, on my side, I use PARANOID everywhere in my tests and I'm fine with it. It is more for the question of @zentol .\\r\\n\\r\\nNote: I used this in my test bench code\\r\\n\\r\\n    @BeforeClass\\r\\n    public static void setDefaultAdvancedLevel() {\\r\\n      System.setProperty(\"io.netty.leakDetection.samplingInterval\", \"12\");\\r\\n      ResourceLeakDetector.setLevel(Level.ADVANCED);\\r\\n    }\\r\\n"
            },
            {
              "comment_username": "zentol",
              "comment_create_time": "2020-09-22T08:30:26Z",
              "comment_edit_time": "2020-09-22T08:30:26Z",
              "comment_text": "Our problem isn't that PARANOID is slower than other modes, that's fine and expected, but that due to _some change that impacts leak detection_ our test is 100x slower in 4.1.50 than before.\r\n\r\nI would just like to understand what has changed; if the leak detection was made slower in general, then that's fine, but I'm worried that something else has changed that indirectly causes leak detection to be slower in our specific case."
            }
          ]
        },
        {
          "issue_url": "https://github.com/netty/netty/issues/10493",
          "issue_title": "How to tune Netty to reduce writeAndFlush time",
          "issue_number": 10493,
          "issue_text": "### Expected behavior\r\nWe are developing a Netty based HTTP2 server to replace exiting Java socket based server. \r\nWe expect Netty + HTTP2 can give us better performance/latency.\r\n\r\n### Actual behavior\r\nHowever, in a recent performance test, where multiple clients send large amount of GET requests to server, Netty+HTTP/2 behaved worse than socket implementation. \r\n\r\nWe tried one request per http/2 connection and http2 multiplexing. Both were worse than socket implementation.\r\nWe also tried 1MB for GET and 4MB for GET, both were worse than socket implementation. \r\n\r\nDuring the performance test, data is in memory and none of CPU/NIC saturated. (NIC/CPU usage lower than socket implementation. )\r\n\r\nOur metrics showed that the majority of the time is in writeAndFlush: from writeAndFlush to listener invoked. \r\n\r\nIs there any parameter we can tune to get better latency? (We have increased TCP buffer to 4MB)\r\n\r\n\r\n### Steps to reproduce\r\n\r\n### Minimal yet complete reproducer code (or URL to code)\r\n\r\n### Netty version\r\n\r\n### JVM version (e.g. `java -version`)\r\njava -version\r\njava version \"1.8.0_121\"\r\nJava(TM) SE Runtime Environment (build 1.8.0_121-b13)\r\nJava HotSpot(TM) 64-Bit Server VM (build 25.121-b13, mixed mode)\r\n\r\n### OS version (e.g. `uname -a`)\r\nLinux",
          "issue_comments": [
            {
              "comment_username": "hyperxpro",
              "comment_create_time": "2020-08-21T05:48:26Z",
              "comment_edit_time": "2020-08-21T05:48:26Z",
              "comment_text": "When you use `writeAndFlush`, it'll push data down regardless of size. So call `write` and when you've a reasonable size, call `flush`."
            },
            {
              "comment_username": "zzmao",
              "comment_create_time": "2020-08-21T05:59:33Z",
              "comment_edit_time": "2020-08-21T05:59:33Z",
              "comment_text": "@hyperxpro  Thanks!  I noticed this before from [here](http://normanmaurer.me/presentations/2014-facebook-eng-netty/slides.html) \r\n\r\nHowever, we only have one chunk of data and we want to send it out ASAP, so we called `writeAndFlush`. "
            },
            {
              "comment_username": "medusar",
              "comment_create_time": "2020-08-21T06:58:36Z",
              "comment_edit_time": "2020-08-21T06:58:36Z",
              "comment_text": "You can use `FlushConsolidationHandler` to reduce `flush` calls."
            },
            {
              "comment_username": "normanmaurer",
              "comment_create_time": "2020-08-21T06:59:57Z",
              "comment_edit_time": "2020-08-21T06:59:57Z",
              "comment_text": "Its really impossible to give any guidance without more details and profiling data. All we can do at the moment is basically guess "
            },
            {
              "comment_username": "chrisvest",
              "comment_create_time": "2020-08-21T07:10:49Z",
              "comment_edit_time": "2020-08-21T07:10:49Z",
              "comment_text": "Is the socket version also implementing HTTP2?"
            }
          ]
        },
        {
          "issue_url": "https://github.com/netty/netty/issues/10486",
          "issue_title": "DNS-over-HTTPS (DoH) Support",
          "issue_number": 10486,
          "issue_text": "Motivation:\r\n\r\n[DNS-over-HTTPS (DoH)](https://tools.ietf.org/html/rfc8484) wraps DNS queries inside HTTP request and sends it over HTTPS to make sure queries are secure in transit.\r\n\r\nModification:\r\n\r\nAdded DoHQueryEncoder, DoHResponseEncoder and DoHDecoder\r\nAdded DoHClient Example\r\n\r\nResult:\r\nEncoding DnsQuery to HTTP Request using DoHQueryEncoder.\r\n\r\nEncode DnsResponse to HTTP Response using DoHResponseEncoder.\r\n\r\nDecoding DnsQuery from HTTP Request and DnsResponse from HTTP Response using DoHDecoder.\r\n\r\nFixes #9091. ",
          "issue_comments": [
            {
              "comment_username": "netty-bot",
              "comment_create_time": "2020-08-19T06:24:03Z",
              "comment_edit_time": "2020-08-19T06:24:03Z",
              "comment_text": "Can one of the admins verify this patch?"
            },
            {
              "comment_username": "hyperxpro",
              "comment_create_time": "2020-08-19T06:27:04Z",
              "comment_edit_time": "2020-08-19T06:27:04Z",
              "comment_text": "\\cc @normanmaurer @yschimke @johnou"
            },
            {
              "comment_username": "yschimke",
              "comment_create_time": "2020-08-19T07:33:57Z",
              "comment_edit_time": "2020-08-19T08:00:23Z",
              "comment_text": "LGTM\\r\\n\\r\\nI suspect this might be a first pass requiring a second when you go to use this for real. Or follow up with fixes, and you get \"real world\" enhancement requests.\\r\\n\\r\\nIt's unclear to make from the example code how this would work in a realistic codebase, and it avoids some of the tricky topics like http response headers by not looking at them.  Obviously things like happy eyeballs are completely out of scope, but a more interesting follow up example would be how to use this to implement something like a small reverse proxy."
            },
            {
              "comment_username": "yschimke",
              "comment_create_time": "2020-08-19T08:02:52Z",
              "comment_edit_time": "2020-08-19T08:02:52Z",
              "comment_text": "@normanmaurer what does it look like to use an alternative DNS infrastructure in a Netty server?  Or is this just part of the toolbox for a team who knows what they are doing to bolt together themselves? "
            }
          ]
        },
        {
          "issue_url": "https://github.com/netty/netty/issues/10475",
          "issue_title": "Netty enforce HTTP proxy to support HTTP CONNECT method",
          "issue_number": 10475,
          "issue_text": "### Expected behavior\r\nIn case of HTTP protocol, the client request is sent as is, without sending the initial message with HTTP CONNECT method.\r\n\r\n#### Background\r\n> CONNECT\r\n> This specification reserves the method name CONNECT for use with a\r\n> **proxy that can dynamically switch to being a tunnel** (e.g. SSL\r\n> tunneling 44).\r\n\r\nSource: https://tools.ietf.org/html/rfc2616#section-9.9\r\n\r\n> CONNECT\r\n> The CONNECT method requests that the recipient establish a tunnel to\r\n> the destination origin server identified by the request-target and,\r\n> **if successful, thereafter restrict its behavior to blind forwarding\r\n> of packets, in both directions, until the tunnel is closed**. Tunnels\r\n> are commonly used to create an end-to-end virtual connection, through\r\n> one or more proxies, which can then be secured using TLS (Transport\r\n> Layer Security, RFC5246).\r\nSource: https://tools.ietf.org/html/rfc7231#section-4.3.6\r\n\r\n#### Request\r\nNo every HTTP proxy supports being a transparent tunnelling proxy. Therefore, Netty as a library shall either react differently on HTTP and HTTPS protocols, or alternatively provide an option to control this behaviour.\r\n\r\nThis issue is related to:\r\n- https://github.com/netty/netty/issues/3634\r\n- https://github.com/reactor/reactor-netty/issues/159\r\n\r\n### Actual behaviour\r\nNo matter of the concrete HTTP protocol, HTTP or HTTPS, Netty's [HttpProxyHandler](https://github.com/netty/netty/blob/0c3eae34ec25203e22d3fa6a678a11a936f9c8b4/handler-proxy/src/main/java/io/netty/handler/proxy/HttpProxyHandler.java#L165) always sends HTTP CONNECT request as initial message.\r\n\r\n### Steps to reproduce\r\nUse pure Netty (or Spring WebClient based on Netty) as plain HTTP client behind a plain HTTP proxy.\r\n\r\n### Minimal yet complete reproducer code (or URL to code)\r\nN/A\r\n\r\n### Netty version\r\nUp to 4.1.51.FINAL\r\n\r\n### JVM version (e.g. `java -version`)\r\nN/A\r\n\r\n### OS version (e.g. `uname -a`)\r\nN/A",
          "issue_comments": [
            {
              "comment_username": "normanmaurer",
              "comment_create_time": "2020-08-13T11:58:11Z",
              "comment_edit_time": "2020-08-13T11:58:11Z",
              "comment_text": "@manolvalchev do you have any proposal how to \"fix\" this ?"
            },
            {
              "comment_username": "thomas-br",
              "comment_create_time": "2020-08-14T08:45:11Z",
              "comment_edit_time": "2020-08-14T08:45:11Z",
              "comment_text": "Maybe the `ProxyProvider` on `tcpClient.proxy(proxy -> ... )` could receive options to specify whether the tunneling with HTTP CONNECT should be used or plain/normal proxying.\r\n\r\nNot sure yet what needs to be changed then in the [HttpProxyHandler](https://github.com/netty/netty/blob/0c3eae34ec25203e22d3fa6a678a11a936f9c8b4/handler-proxy/src/main/java/io/netty/handler/proxy/HttpProxyHandler.java#L165) then to also support normal proxy behavior. "
            },
            {
              "comment_username": "violetagg",
              "comment_create_time": "2020-08-14T09:04:30Z",
              "comment_edit_time": "2020-08-14T09:04:30Z",
              "comment_text": "> Maybe the `ProxyProvider` on `tcpClient.proxy(proxy -> ... )` could receive options to specify whether the tunneling with HTTP CONNECT should be used or plain/normal proxying.\r\n\r\n@thomas-br The configuration above is Reactor Netty specific and should be discussed in Reactor Netty project when/if there is an extension to the current HttpProxyHandler provided by Netty\r\n"
            },
            {
              "comment_username": "manolvalchev",
              "comment_create_time": "2020-08-14T09:08:06Z",
              "comment_edit_time": "2020-08-14T09:09:01Z",
              "comment_text": "@normanmaurer, Usually, such a decision is taken based on the URL protocol scheme, e.g. ```http``` -> no usage of CONNECT method; ```https``` -> with usage of CONNECT method. Another option would be to offer configuration parameter, so that the consumer of the library (on the fly, not static for the library) decide whether HTTP CONNECT to be used or not - at the end, the consumer of the proxy (the code that uses Netty as client connection library) knows what are the specifics and requirements of the proxy - the low level library can only help in offering this or that option but IMHO cannot take that decision."
            },
            {
              "comment_username": "thomas-br",
              "comment_create_time": "2020-08-14T09:23:43Z",
              "comment_edit_time": "2020-08-14T09:23:43Z",
              "comment_text": "> @thomas-br The configuration above is Reactor Netty specific and should be discussed in Reactor Netty project when/if there is an extension to the current HttpProxyHandler provided by Netty\r\n\r\n@violetagg Agree. But despite the configuration capabilities offered by Reactor Netty, shouldn't both proxy behaviors be offered by the `HttpProxyHandler` of netty? Because as @manolvalchev stated, the current implementations only supports the proxy tunneling."
            },
            {
              "comment_username": "kachayev",
              "comment_create_time": "2020-08-14T10:14:30Z",
              "comment_edit_time": "2020-08-14T10:19:20Z",
              "comment_text": "If there's no need to establish tunnel when connecting to the proxy, the `HttpProxyHandler` would be probably an overkill: no need to create and send initial message, wait for the response, decode it and handle issues as it goes, track timeout between req/resp, as well as no need to stash writes before CONNECT response is processed. The only significant difference compared to generic HTTP when not using CONNECT:\r\n* connection should be established to another address\r\n* DNS resolve should be disabled\r\n\r\nWhich could be done in another handler that only intercepts `connect`. Adding conditional configuration into `HttpProxyHandler` seems somewhat questionable as it basically means conditioning over which methods to overwrite. And as far as decision between tunnel/non-tunnel requires user input (there's no way to figure out if proxy supports tunneling and/or if the tunnel is necessary when it's supported) prior to establishing the connection, the same input could be used to choose between 2 different handlers. Which seems to be more suitable for application/framework rather than Netty itself (e.g. how it's done in [Aleph](https://github.com/aleph-io/aleph/blob/d3dc2e9835b58fb53b957171765e8da68e52aa84/src/aleph/http/client.clj#L325-L335) framework)."
            },
            {
              "comment_username": "slandelle",
              "comment_create_time": "2020-08-14T12:35:48Z",
              "comment_edit_time": "2020-08-14T12:35:48Z",
              "comment_text": "> connection should be established to another address\r\nDNS resolve should be disabled\r\n\r\nAlso, request url must be absolute instead of relative to domain root."
            },
            {
              "comment_username": "thomas-br",
              "comment_create_time": "2020-08-20T09:19:06Z",
              "comment_edit_time": "2020-08-20T09:19:06Z",
              "comment_text": "@kachayev What do you suggest to continue on this one to get to a suitable solution, maybe similar as in the Aleph framework you showed?\r\nWould that be offering a second non-CONNECT handler which then can be selected in [reactor-netty](https://github.com/reactor/reactor-netty/blob/fbce9325c2da531e1877b6ada84eaf74db3c40fd/reactor-netty-core/src/main/java/reactor/netty/transport/ProxyProvider.java#L126) then? Or is there a better solution we can work on?"
            },
            {
              "comment_username": "kachayev",
              "comment_create_time": "2020-08-20T18:25:07Z",
              "comment_edit_time": "2020-08-20T18:25:07Z",
              "comment_text": "@thomas-br honestly, I've never worked with `reactor-netty` :) From what I see in `ProxyProvider` code, it seems more than reasonable to add new configuration `tunnel` and when it is set to `false` and `type` = `HTTP`, choose another handler [here](https://github.com/reactor/reactor-netty/blob/fbce9325c2da531e1877b6ada84eaf74db3c40fd/reactor-netty-core/src/main/java/reactor/netty/transport/ProxyProvider.java#L124).  Maybe if it's common enough situation, it would be good to have this handler implemented in Netty (like `NonTunnelHttpProxyHandler` or something).\\r\\n\\r\\nIt's still somewhat \"messy\" and not always intuitive as non-tunnel usage invalidates other parameters, like `username`, `password`, `httpHeaders` etc. But in practice if your proxy asks for user/password, it definitely expectes tunnel."
            },
            {
              "comment_username": "kachayev",
              "comment_create_time": "2020-08-20T18:33:52Z",
              "comment_edit_time": "2020-08-20T18:33:52Z",
              "comment_text": "After some investigation, turned out I was wrong on this:\r\n\r\n* DNS resolve should be disabled\r\n\r\nIn some networks the name resolution is expected to happen on the client rather than on the proxy (typically with custom DNS resolvers or in networks like Tor). "
            }
          ]
        },
        {
          "issue_url": "https://github.com/netty/netty/issues/10436",
          "issue_title": "Http2: server rejects request with valid :protocol pseudo header",
          "issue_number": 10436,
          "issue_text": "### Expected behavior\r\n\r\nServer accepts request  with `:protocol` pseudo header (defined in [RFC8441](https://tools.ietf.org/html/rfc8441#section-4)) , if its initial settings contain SETTINGS_ENABLE_CONNECT_PROTOCOL SETTINGS=1.\r\n\r\n### Actual behavior\r\nRequest is rejected\r\n```\r\nio.netty.handler.codec.http2.Http2Exception$StreamException: Invalid HTTP/2 pseudo-header ':protocol' encountered.\r\n\tat io.netty.handler.codec.http2.Http2Exception.streamError(Http2Exception.java:147)\r\n\tat io.netty.handler.codec.http2.HpackDecoder.validate(HpackDecoder.java:393)\r\n```\r\nSuboptimal workaround is configuring server's `Http2ConnectionHandler` with `http2Builder.validateHeaders(false)`\r\n\r\n### Steps to reproduce\r\n\r\n### Minimal yet complete reproducer code (or URL to code)\r\nhttps://github.com/jauntsdn/netty-websocket-http2/tree/feature/http2-protocol-header-reject\r\n\r\nstart `./channel_server.sh`  \r\nstart `./channel_client.sh`\r\n\r\n### Netty version\r\n4.1.51.Final\r\n### JVM version (e.g. `java -version`)\r\n\r\n### OS version (e.g. `uname -a`)\r\n",
          "issue_comments": [
            {
              "comment_username": "normanmaurer",
              "comment_create_time": "2020-08-02T16:45:09Z",
              "comment_edit_time": "2020-08-02T16:45:09Z",
              "comment_text": "@ejona86 @bryce-anderson @carl-mastrangelo @slandelle  can you have a look ?"
            },
            {
              "comment_username": "carl-mastrangelo",
              "comment_create_time": "2020-08-03T21:33:25Z",
              "comment_edit_time": "2020-08-03T21:33:25Z",
              "comment_text": "This might be reasonable to support with the Http2ConnectionHandler, but not from the Multiplex handler.    Netty /is/ compliant with [RFC 7540](https://tools.ietf.org/html/rfc7540#section-8.1.2.1)  which states:\r\n\r\n> Endpoints MUST treat a request or response that contains undefined or invalid pseudo-header fields as malformed\r\n\r\n"
            },
            {
              "comment_username": "ejona86",
              "comment_create_time": "2020-08-03T21:52:45Z",
              "comment_edit_time": "2020-08-03T21:52:45Z",
              "comment_text": "It looks like the problem is that Netty does not support RFC 8441 for bootstrapping websockets. This [code in the sample](https://github.com/jauntsdn/netty-websocket-http2/blob/c7f9799608f9cb29fef93a2f6b5252cb8a2b6192/netty-websocket-http2/src/main/java/com/jauntsdn/netty/handler/codec/http2/websocketx/Http2WebSocketServerHandler.java#L64-L66) is invalid:\r\n\r\n```java\r\n    http2Builder\r\n        .initialSettings()\r\n        .put(Http2WebSocketProtocol.SETTINGS_ENABLE_CONNECT_PROTOCOL, (Long) 1L);\r\n```\r\n\r\nWhen you send SETTINGS_ENABLE_CONNECT_PROTOCOL, that means your system understands `:protocol`, which is not true in this case. And Netty should not blindly accept `:protocol` just in case someone set SETTINGS_ENABLE_CONNECT_PROTOCOL.\r\n\r\nI think part of the problem here though is that this verification is being done in the hpack decoder. That doesn't seem appropriate, as the pseudo headers are part of HTTP/2, not HPACK."
            },
            {
              "comment_username": "mostroverkhov",
              "comment_create_time": "2020-08-04T12:13:12Z",
              "comment_edit_time": "2020-08-04T12:13:12Z",
              "comment_text": "> This code in the sample is invalid\r\n\r\nIt is valid as soon as channel pipeline contains rfc8441 websocket-over-http2 handlers. Repository linked in the issue implements rfc8441, and having parameter SETTINGS_ENABLE_CONNECT_PROTOCOL SETTINGS=1 is required by that spec for server to advertise websocket-over-http2 support.\r\n\r\n> When you send SETTINGS_ENABLE_CONNECT_PROTOCOL, that means your system understands :protocol, which is not true in this case.\r\n\r\nIn this particular case system *does* understand `:protocol` because It is an [implementation](https://github.com/jauntsdn/netty-websocket-http2) of rfc8441.\r\n\r\nnetty's own http2 handlers deprive external code from supporting websocket-over-http2 - by requiring `validateHeaders = false` on the server for this use case. Requiring `validateHeaders = false` in practice means said external code cant be used on servers exposed to untrusted clients. \r\n\r\nIf having `server advertising ENABLE_CONNECT_PROTOCOL, but no appropriate handler` is not considered a bug on library user side / needs to be handled by netty itself, then one solution may be as follows:\r\n\r\n* server opts-in for websocket-over-http2 with `SETTINGS_ENABLE_CONNECT_PROTOCOL=1`. \r\n* add marker interface `Http2WebsocketHandler` that is used by  netty's http2 handlers to decide if pipeline actually supports websocket-over-http2. It helps with websocket requests incorrectly treated as CONNECT requests.\r\n* if server opted for websocket-over-http2, and there is single appropriate handler on pipeline (implements marker `Http2WebsocketHandler`), then `:protocol` pseudo header is treated as acceptable.\r\n\r\nThis should work unless some 3rd party grabbed ENABLE_CONNECT_PROTOCOL settings code (0x08) for other purposes. \r\n"
            },
            {
              "comment_username": "ejona86",
              "comment_create_time": "2020-08-04T15:03:57Z",
              "comment_edit_time": "2020-08-04T15:03:57Z",
              "comment_text": ">> This code in the sample is invalid\\r\\n\\r\\n> It is valid as soon as channel pipeline contains rfc8441 websocket-over-http2 handlers. \\r\\n\\r\\nNo, it is not. Because RFC 8441 also requires accepting the `:protocol` header, which the code does not do.\\r\\n\\r\\n> In this particular case system does understand :protocol because It is an implementation of rfc8441.\\r\\n\\r\\nThe \"system\" includes HpackDecoder, which obviously does _not_ support RFC 8441.\\r\\n\\r\\nThis isn't a case of \"I made Netty support RFC 8441\" but rather \"I want to make Netty support RFC 8441.\" This is a feature request to figure out how Netty can allow supporting RFC 8441.\\r\\n\\r\\n> by requiring validateHeaders = false on the server for this use case\\r\\n\\r\\nThat's actually _required_ by HTTP/2. You are no longer HTTP/2 (with or without RFC 8441) if you disable those checks. As I said, I think those checks are put in the wrong place as ideally you'd be able to allow an additional header to be validated properly. You should not disable the checks completely (unless potentially you reimplemented them yourself).\\r\\n\\r\\n> server opts-in for websocket-over-http2 with SETTINGS_ENABLE_CONNECT_PROTOCOL=1.\\r\\nadd marker interface Http2WebsocketHandler that is used by netty's http2 handlers to decide if pipeline actually supports websocket-over-http2...\\r\\n\\r\\nThis is a very implicit approach. I'd expect we'd want a more explicit approach. For example, providing the Http2WebsocketHandler to the HTTP/2 code to be used if websocket connect is used (similar to existing Upgrade handling). (If the HTTP/2 code has to know about Http2WebsocketHandler already, I see little point in implicit wiring.)\\r\\n\\r\\nBut it also sort of looks like there is no need for it to be Websocket-specific. SETTINGS_ENABLE_CONNECT_PROTOCOL is intended to be generic. So maybe we just need an option for enabling SETTINGS_ENABLE_CONNECT_PROTOCOL in the innards and then let your handler handle the CONNECT request completely. I'd have to look at the code more to see if that behavior should be enabled by the SETTINGS_ENABLE_CONNECT_PROTOCOL setting directly or whether we'd want an API when building the HTTP/2 classes and have the Netty internals set that setting."
            },
            {
              "comment_username": "normanmaurer",
              "comment_create_time": "2020-08-10T14:51:48Z",
              "comment_edit_time": "2020-08-10T14:51:48Z",
              "comment_text": "@ejona86 @mostroverkhov so I am a bit lost here... what do you propose to moving forward here ?"
            },
            {
              "comment_username": "ejona86",
              "comment_create_time": "2020-08-10T15:09:09Z",
              "comment_edit_time": "2020-08-10T15:09:09Z",
              "comment_text": "Netty probably needs direct support of the `:protocol` header. It is unclear whether we have an option to enable that support or whether we do it implicitly based on the SETTINGS."
            },
            {
              "comment_username": "mostroverkhov",
              "comment_create_time": "2020-08-19T06:22:58Z",
              "comment_edit_time": "2020-08-19T06:22:58Z",
              "comment_text": "@normanmaurer Looking at ejona86's suggestions \r\n> I think part of the problem here though is that this verification is being done in the hpack decoder. That doesn't seem appropriate, as the pseudo headers are part of HTTP/2, not HPACK.\r\n\r\n> As I said, I think those checks are put in the wrong place as ideally you'd be able to allow an additional header to be validated properly.\r\n\r\nI read them as a hint to have headers validation pluggable, with rfc7540 as default, and optional rfc8441 (that is also part of this project). \r\n\r\nI am happy with any solution which allows using netty's http2 handlers for websocket-over-http2, and seems there are no significant blockers other than `:protocol` pseudo-header validation issue. \r\n"
            },
            {
              "comment_username": "mostroverkhov",
              "comment_create_time": "2020-09-03T19:41:27Z",
              "comment_edit_time": "2020-09-03T19:41:27Z",
              "comment_text": "@normanmaurer @ejona86 I have made few changes related to this on https://github.com/mostroverkhov/netty/tree/header-validators just to start things going.  \r\n\r\n`AbstractHttp2ConnectionHandlerBuilder` is extended with protected `validateHeaders(Http2HeadersValidationType http2HeadersValidationType)`. Method is exposed on `Http2FrameCodecBuilder` and `Http2ConnectionBuilder`.\r\nPublic `Http2HeadersValidationType` is enum comprised of `NONE, RFC7540, RFC8441`. "
            },
            {
              "comment_username": "normanmaurer",
              "comment_create_time": "2020-09-21T13:29:04Z",
              "comment_edit_time": "2020-09-21T13:29:04Z",
              "comment_text": "@ejona86 @carl-mastrangelo can you have a look ?"
            }
          ]
        },
        {
          "issue_url": "https://github.com/netty/netty/issues/10433",
          "issue_title": "DriftNettyServerConfig with assumeClientsSupportOutOfOrderResponses=false is processing requests out of order",
          "issue_number": 10433,
          "issue_text": "### Summary\r\nWhen ```DriftNettyServerConfig``` is configured with ```assumeClientsSupportOutOfOrderResponses = false`` then writing 2 messages should cause polling for the 2nd message to return null instead of an object.\r\n\r\nThis is causing unit test failures in the Drift project of Presto, see https://github.com/prestodb/drift/issues/15, and happens after changing the Netty version to >= 4.1.38\r\n\r\nSee https://github.com/prestodb/drift/pull/22\r\n\r\n### Expected behavior\r\nNetty server should only process the second message after we are done with the first request.\r\n\r\n### Actual behavior\r\nNetty server starts processing the second message (its state is PENDING) even though the first request is not done.\r\n\r\n### Steps to reproduce\r\nSee PR https://github.com/prestodb/drift/pull/22 on repro-steps\r\nInstall JDK 8 and mvn 3.5.0\r\n```\r\ngit clone git@github.com:prestodb/drift.git\r\ncd drift/drift-transport-netty\r\nmvn clean test -Dtest=TestDriftNettyServerTransport\r\n```\r\n\r\n### Minimal yet complete reproducer code (or URL to code)\r\n\r\n### Netty version\r\n4.1.38.Final\r\n\r\n### JVM version (e.g. `java -version`)\r\n1.8.0_261\r\n\r\n### OS version (e.g. `uname -a`)\r\nMac OS Catalina 10.15.5 -  Darwin Kernel Version 19.5.0\r\n",
          "issue_comments": [
            {
              "comment_username": "afernandez",
              "comment_create_time": "2020-08-04T01:27:30Z",
              "comment_edit_time": "2020-08-04T01:27:30Z",
              "comment_text": "Hi, @trustin ,  @normanmaurer , just checking if this was on your radar to see if it's indeed a bug and introduced in v. 4.1.38. Thank you"
            },
            {
              "comment_username": "normanmaurer",
              "comment_create_time": "2020-08-10T07:20:57Z",
              "comment_edit_time": "2020-08-10T07:20:57Z",
              "comment_text": "I will have a look this week. So yes on my radar"
            },
            {
              "comment_username": "normanmaurer",
              "comment_create_time": "2020-08-13T12:00:55Z",
              "comment_edit_time": "2020-08-13T12:00:55Z",
              "comment_text": "@afernandez so basically you say you can reproduce this starting 4.1.38.Final (and also in the latest release) right ?"
            },
            {
              "comment_username": "normanmaurer",
              "comment_create_time": "2020-08-18T11:40:15Z",
              "comment_edit_time": "2020-08-18T11:40:15Z",
              "comment_text": "@afernandez ^^"
            },
            {
              "comment_username": "normanmaurer",
              "comment_create_time": "2020-08-18T12:03:27Z",
              "comment_edit_time": "2020-08-18T12:03:27Z",
              "comment_text": "I suspect the change in behaviour was introduced by https://github.com/netty/netty/commit/6b6475fb565551c36f372fe37683d98df6674161 ... But to better understand if this is a bug or not I would need to get some more details about what you try to do.  Can you give some more pointers in what you try to do with your code and how you do it ? Like for example point me to the right places in your code ? "
            },
            {
              "comment_username": "normanmaurer",
              "comment_create_time": "2020-08-18T12:07:29Z",
              "comment_edit_time": "2020-08-18T12:07:29Z",
              "comment_text": "Also I tried to run the test but it failed as the snapshot was missing:\r\n\r\n```\r\n[INFO] ------------------------------------------------------------------------\r\n[INFO] BUILD FAILURE\r\n[INFO] ------------------------------------------------------------------------\r\n[INFO] Total time:  14.758 s\r\n[INFO] Finished at: 2020-08-18T14:05:59+02:00\r\n[INFO] ------------------------------------------------------------------------\r\n[ERROR] Failed to execute goal on project drift-transport-netty: Could not resolve dependencies for project com.facebook.drift:drift-transport-netty:jar:1.28-SNAPSHOT: The following artifacts could not be resolved: com.facebook.drift:drift-api:jar:1.28-SNAPSHOT, com.facebook.drift:drift-codec:jar:1.28-SNAPSHOT, com.facebook.drift:drift-protocol:jar:1.28-SNAPSHOT, com.facebook.drift:drift-transport-spi:jar:1.28-SNAPSHOT: Could not find artifact com.facebook.drift:drift-api:jar:1.28-SNAPSHOT in sonatype-nexus-snapshots (https://oss.sonatype.org/content/repositories/snapshots) -> [Help 1]\r\n[ERROR]\r\n```\r\n\r\nDo I need to add some snapshot repository ?"
            },
            {
              "comment_username": "normanmaurer",
              "comment_create_time": "2020-08-27T06:21:30Z",
              "comment_edit_time": "2020-08-27T06:21:30Z",
              "comment_text": "@afernandez ping... sorry but I need some help here to help you :)"
            }
          ]
        },
        {
          "issue_url": "https://github.com/netty/netty/issues/10432",
          "issue_title": "Track memory attributes to avoid memory leak",
          "issue_number": 10432,
          "issue_text": "Motivation:\r\nIn commit c354fa48e10de847cf17c10083a2cbc2c0a63a36 (https://github.com/netty/netty/pull/10209) the way attributes are decoded has changed. In `HttpPostStandardRequestDecoder#decodeAttribute(ByteBuf, Charset)` a buffer is allocated but may not be released while calling `InterfaceHttpPostRequestDecoder#destroy()`.\r\n\r\n```\r\nHttpPostRequestDecoder decoder =\r\n    new HttpPostRequestDecoder(new MemoryHttpDataFactory(false), request);\r\ntry {\r\n    ...\r\n} finally {\r\n    decoder.destroy();\r\n}\r\n```\r\n\r\n`decoder.destroy()` invokes `DefaultHttpDataFactory#cleanRequestHttpData(HttpRequest)`, but as the memory attributes are not tracked, nothing is released.\r\n\r\nWorkaround:\r\n```\r\nHttpPostRequestDecoder decoder =\r\n    new HttpPostRequestDecoder(new MemoryHttpDataFactory(false), request);\r\ntry {\r\n    ...\r\n} finally {\r\n    decoder.getBodyHttpDatas()\r\n        .forEach(ReferenceCounted::release);\r\n    decoder.destroy();\r\n}\r\n```\r\n\r\nModification:\r\nTrack memory attributes so that they are released by calling `HttpDataFactory#cleanAllHttpData()` or `HttpDataFactory#cleanRequestHttpData(HttpRequest)`.\r\n\r\nResult:\r\nAvoid memory leak",
          "issue_comments": [
            {
              "comment_username": "netty-bot",
              "comment_create_time": "2020-07-28T15:44:19Z",
              "comment_edit_time": "2020-07-28T15:44:19Z",
              "comment_text": "Can one of the admins verify this patch?"
            },
            {
              "comment_username": "netty-bot",
              "comment_create_time": "2020-07-28T15:44:38Z",
              "comment_edit_time": "2020-07-28T15:44:38Z",
              "comment_text": "Can one of the admins verify this patch?"
            },
            {
              "comment_username": "netty-bot",
              "comment_create_time": "2020-07-28T15:45:04Z",
              "comment_edit_time": "2020-07-28T15:45:04Z",
              "comment_text": "Can one of the admins verify this patch?"
            },
            {
              "comment_username": "netty-bot",
              "comment_create_time": "2020-07-28T15:49:04Z",
              "comment_edit_time": "2020-07-28T15:49:04Z",
              "comment_text": "Can one of the admins verify this patch?"
            },
            {
              "comment_username": "normanmaurer",
              "comment_create_time": "2020-07-29T06:48:31Z",
              "comment_edit_time": "2020-07-29T06:48:31Z",
              "comment_text": "@mschneid can you add a unit test as well ?"
            },
            {
              "comment_username": "mschneid",
              "comment_create_time": "2020-07-30T09:37:55Z",
              "comment_edit_time": "2020-07-30T09:37:55Z",
              "comment_text": "> @mschneid can you add a unit test as well ?\r\n\r\n@normanmaurer Done"
            },
            {
              "comment_username": "normanmaurer",
              "comment_create_time": "2020-08-05T06:55:01Z",
              "comment_edit_time": "2020-08-05T06:55:01Z",
              "comment_text": "@netty-bot test this please "
            },
            {
              "comment_username": "normanmaurer",
              "comment_create_time": "2020-08-05T18:58:29Z",
              "comment_edit_time": "2020-08-05T18:58:29Z",
              "comment_text": "@netty-bot test this please "
            },
            {
              "comment_username": "normanmaurer",
              "comment_create_time": "2020-08-06T06:46:49Z",
              "comment_edit_time": "2020-08-06T06:46:49Z",
              "comment_text": "@mschneid there are unit test failures. PTAL"
            },
            {
              "comment_username": "normanmaurer",
              "comment_create_time": "2020-08-06T12:05:18Z",
              "comment_edit_time": "2020-08-06T12:05:18Z",
              "comment_text": "@netty-bot test this please "
            },
            {
              "comment_username": "mschneid",
              "comment_create_time": "2020-08-10T10:34:37Z",
              "comment_edit_time": "2020-08-10T10:34:37Z",
              "comment_text": "@normanmaurer should I squash this PR?"
            },
            {
              "comment_username": "normanmaurer",
              "comment_create_time": "2020-08-13T11:55:31Z",
              "comment_edit_time": "2020-08-13T11:55:31Z",
              "comment_text": "@mschneid let me have a look first. From a quick look I am not sure if this PR is 100 % correct. I will come back to you shortly "
            },
            {
              "comment_username": "normanmaurer",
              "comment_create_time": "2020-08-19T06:35:58Z",
              "comment_edit_time": "2020-08-19T06:35:58Z",
              "comment_text": "@mschneid so I did spent some time to look into this and the commit 9a701d523193828c180cdfff30bfe803d9b61362 seems to be not correct. Let me think about this a bit and come up with an alternative way of fixing the test-failures "
            }
          ]
        },
        {
          "issue_url": "https://github.com/netty/netty/issues/10430",
          "issue_title": "Netty-codec-http 4.1.51 still fails veracode with SID-3108",
          "issue_number": 10430,
          "issue_text": "### Expected behavior\r\nPass veracode(thought it was fixed in #10111), but looks like it was not\r\n### Actual behavior\r\nFails veracode\r\n![image](https://user-images.githubusercontent.com/1686996/88561499-7d0e8300-cffd-11ea-958e-493b31c0a808.png)\r\n\r\n### Steps to reproduce\r\nbuild a project with\r\n     <dependency>\r\n      <groupId>com.azure</groupId>\r\n      <artifactId>azure-security-keyvault-secrets</artifactId>\r\n      <version>4.1.5</version>\r\n    </dependency>\r\nthat will bring in netty 4.1.51, and when submitted to veracode it will show SID-3108\r\n### Minimal yet complete reproducer code (or URL to code)\r\nfollowing sample here should show the issue: https://docs.microsoft.com/en-us/azure/key-vault/secrets/quick-create-java#sample-code\r\n### Netty version\r\nnetty-codec-http-4.1.51.Final.jar\r\n### JVM version (e.g. `java -version`)\r\njava 1.8.0_251\r\n### OS version (e.g. `uname -a`)\r\nWindows 10 1339",
          "issue_comments": [
            {
              "comment_username": "hyperxpro",
              "comment_create_time": "2020-08-19T15:01:05Z",
              "comment_edit_time": "2020-08-19T15:01:05Z",
              "comment_text": "Can you point to specific part of code?"
            }
          ]
        },
        {
          "issue_url": "https://github.com/netty/netty/issues/10426",
          "issue_title": "Possible memory leak when set maxPendingTasks=100",
          "issue_number": 10426,
          "issue_text": "### Expected behavior\r\nI am trying to implement an echo udp server, since the request rate may be really high, so we want to limit the resource, the reading queue of _DefaultEventExecutorGroup_ which do the time-consuming task. \r\nThe expected behavior is when I set small _maxPendingTasks_, like 100, or 1000, and do stress test to send 60,000 qps, the _DefaultEventExecutorGroup_ reject most of the request, and lead no memory leak.\r\n### Actual behavior\r\ndirect memory leak. To debug it, I change the allocator using `Channel.config().setAllocator(UnpooledByteBufAllocator.DEFAULT)`, I can see the direct memory is increasing consistently, and at last, it equals to the _MaxDirectMemory_, then all things stop, the nioEventLoop is wait at epoll_wait(using strace), the DefaultEventExecutorGroup is also wait(using strace).\r\n**BUT, when not set _maxPendingTasks_, keep it as INT_MAX, program works well with no leak.**\r\n\r\nScreenshot when set maxPendingTasks=10, and send request rate is 1200 per second:\r\nFirst, start send requests, and keep a few seconds, then stop(screenshot 1), we can see unpooled direct memory is not released. The screenshot 2 is the same result.\r\n\r\n![netty-leak-1](https://user-images.githubusercontent.com/24410509/88992888-0b914600-d317-11ea-8832-cc4f5b669fa5.png)\r\n![netty-leak-2](https://user-images.githubusercontent.com/24410509/88992914-1e0b7f80-d317-11ea-97fa-2f0c3e3c92b5.png)\r\n\r\n### Steps to reproduce\r\nset maxPendingTasks=100, or other small ones.\r\n### Minimal yet complete reproducer code (or URL to code)\r\n    private EventLoopGroup bossGroup = new EpollEventLoopGroup();\r\n    // set maxPendingTasks=100\r\n    static final EventExecutorGroup threadGroup = new DefaultEventExecutorGroup(1, null, 100, MyRejectedExecutionHandler.reject());\r\n\r\n    Bootstrap bootstrap = new Bootstrap();\r\n        bootstrap.group(bossGroup)\r\n            .channel(EpollDatagramChannel.class)\r\n                .option(EpollChannelOption.SO_REUSEPORT, true)\r\n                .handler(new ChannelInitializer<Channel>() {\r\n        @Override\r\n        protected void initChannel(Channel channel) throws Exception {\r\n            // change to unpooled allocator\r\n            channel.config().setAllocator(UnpooledByteBufAllocator.DEFAULT);\r\n            channel.pipeline().addLast(threadGroup, new UdpServerHandler());\r\n        }\r\n    });\r\n\r\n    private static class UdpServerHandler extends SimpleChannelInboundHandler<DatagramPacket> {\r\n\r\n        @Override\r\n        protected void channelRead0(ChannelHandlerContext ctx, DatagramPacket datagramPacket) throws Exception {\r\n            // just echo, so there is no memory leak\r\n            ByteBuf buffer = datagramPacket.content();\r\n            ReferenceCountUtil.retain(buffer);\r\n            DatagramPacket packet = new DatagramPacket(buffer, datagramPacket.sender());\r\n            ctx.channel().writeAndFlush(packet);\r\n        }\r\n\r\n    } // end of UdpServerHandler\r\n### Netty version\r\n4.1.50.final\r\n### JVM version\r\nopenjdk version \"1.8.0_252\"\r\n### OS version\r\nLinux centos 4.19",
          "issue_comments": [
            {
              "comment_username": "hackeryard",
              "comment_create_time": "2020-07-29T11:33:03Z",
              "comment_edit_time": "2020-07-29T11:33:03Z",
              "comment_text": "Any information is welcome."
            },
            {
              "comment_username": "normanmaurer",
              "comment_create_time": "2020-08-10T07:23:27Z",
              "comment_edit_time": "2020-08-10T07:23:27Z",
              "comment_text": "Will have a look "
            },
            {
              "comment_username": "normanmaurer",
              "comment_create_time": "2020-08-10T14:38:25Z",
              "comment_edit_time": "2020-08-10T14:38:25Z",
              "comment_text": "@hackeryard can you also log the stack of when it is rejected ?"
            },
            {
              "comment_username": "hackeryard",
              "comment_create_time": "2020-08-10T22:15:05Z",
              "comment_edit_time": "2020-08-10T22:15:05Z",
              "comment_text": "@normanmaurer Thanks for reply. I overrive the reject handler of DefaultEventExecutorGroup, so there is no log, but netty report the direct memory leak problem between the task is rejected.\\r\\n\\r\\nHere is reject handler:\\r\\n```\\r\\n@Slf4j\\r\\npublic final class MyRejectedExecutionHandler {\\r\\n    private static final RejectedExecutionHandler REJECT = new RejectedExecutionHandler() {\\r\\n        public void rejected(Runnable task, SingleThreadEventExecutor executor) {\\r\\n            // do nothing\\r\\n            log.info(\"Reject request..\");\\r\\n        }\\r\\n    };\\r\\n\\r\\n    public static RejectedExecutionHandler reject() {\\r\\n        return REJECT;\\r\\n    }\\r\\n}\\r\\n\\r\\n```\\r\\n\\r\\nHere is netty leak detector:\\r\\n```\\r\\n2020-07-25 00:08:20.508 ERROR 30075 --- [ntLoopGroup-3-1] io.netty.util.ResourceLeakDetector       : LEAK: ByteBuf.release() was not called before it's garbage-collected. See https://netty.io/wiki/reference-counted-objects.html for more information.\\r\\nRecent access records: \\r\\nCreated at:\\r\\n\\tio.netty.buffer.PooledByteBufAllocator.newDirectBuffer(PooledByteBufAllocator.java:363)\\r\\n\\tio.netty.buffer.AbstractByteBufAllocator.directBuffer(AbstractByteBufAllocator.java:187)\\r\\n\\tio.netty.buffer.AbstractByteBufAllocator.directBuffer(AbstractByteBufAllocator.java:178)\\r\\n\\tio.netty.channel.unix.PreferredDirectByteBufAllocator.ioBuffer(PreferredDirectByteBufAllocator.java:53)\\r\\n\\tio.netty.channel.DefaultMaxMessagesRecvByteBufAllocator$MaxMessageHandle.allocate(DefaultMaxMessagesRecvByteBufAllocator.java:114)\\r\\n\\tio.netty.channel.epoll.EpollRecvByteAllocatorHandle.allocate(EpollRecvByteAllocatorHandle.java:75)\\r\\n\\tio.netty.channel.epoll.EpollDatagramChannel$EpollDatagramChannelUnsafe.epollInReady(EpollDatagramChannel.java:483)\\r\\n\\tio.netty.channel.epoll.AbstractEpollChannel$AbstractEpollUnsafe$1.run(AbstractEpollChannel.java:387)\\r\\n\\tio.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:164)\\r\\n\\tio.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:472)\\r\\n\\tio.netty.channel.epoll.EpollEventLoop.run(EpollEventLoop.java:384)\\r\\n\\tio.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)\\r\\n\\tio.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\\r\\n\\tio.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\\r\\n\\tjava.lang.Thread.run(Thread.java:748)\\r\\n\\r\\n```"
            }
          ]
        },
        {
          "issue_url": "https://github.com/netty/netty/issues/10422",
          "issue_title": "[Feature]Add zstd encoder and decoder",
          "issue_number": 10422,
          "issue_text": "Motivation:\r\n\r\nZstandard(https://facebook.github.io/zstd/) is a high performance, high compression ratio compression algorithm,This pr is to add netty support for the zstandard algorithm,The implementation of zstandard algorithm relies on zstd-jni (https://github.com/luben/zstd-jni), which is an openSource third-party library,Apache Kafka is also using this library for message compression processing.Please review this pr,thk\r\n\r\nModification:\r\n\r\nAdd ZstdEncoder and ZstdDecoder and test case.\r\n\r\nResult:\r\n\r\nIf there is no issue then describe the changes introduced by this PR.\r\n",
          "issue_comments": [
            {
              "comment_username": "netty-bot",
              "comment_create_time": "2020-07-22T06:09:03Z",
              "comment_edit_time": "2020-07-22T06:09:03Z",
              "comment_text": "Can one of the admins verify this patch?"
            },
            {
              "comment_username": "netty-bot",
              "comment_create_time": "2020-07-22T06:09:20Z",
              "comment_edit_time": "2020-07-22T06:09:20Z",
              "comment_text": "Can one of the admins verify this patch?"
            },
            {
              "comment_username": "netty-bot",
              "comment_create_time": "2020-07-22T06:09:37Z",
              "comment_edit_time": "2020-07-22T06:09:37Z",
              "comment_text": "Can one of the admins verify this patch?"
            },
            {
              "comment_username": "netty-bot",
              "comment_create_time": "2020-07-22T06:10:04Z",
              "comment_edit_time": "2020-07-22T06:10:04Z",
              "comment_text": "Can one of the admins verify this patch?"
            },
            {
              "comment_username": "hyperxpro",
              "comment_create_time": "2020-07-22T22:22:53Z",
              "comment_edit_time": "2020-07-22T22:22:53Z",
              "comment_text": "I was going to add Brotli support but got stuck with some projects. If you got some time, would you mind adding Brotli support too, please?\r\n\r\nSee #6899 "
            },
            {
              "comment_username": "skyguard1",
              "comment_create_time": "2020-07-23T02:32:44Z",
              "comment_edit_time": "2020-07-23T02:33:38Z",
              "comment_text": "> I was going to add Brotli support but got stuck with some projects. If you got some time, would you mind adding Brotli support too, please?\r\n> \r\n> See #6899\r\n\r\nThanks for your review, but I don't know much about this compression algorithm.sorry."
            },
            {
              "comment_username": "skyguard1",
              "comment_create_time": "2020-07-23T05:32:56Z",
              "comment_edit_time": "2020-07-23T05:33:50Z",
              "comment_text": "> Add license headers to all files.\r\n\r\nI've finished.appreciate it.\r\n"
            },
            {
              "comment_username": "skyguard1",
              "comment_create_time": "2020-07-30T02:10:19Z",
              "comment_edit_time": "2020-07-30T02:10:35Z",
              "comment_text": "Is there any update? :smiley:"
            },
            {
              "comment_username": "hyperxpro",
              "comment_create_time": "2020-07-30T09:52:59Z",
              "comment_edit_time": "2020-07-30T09:52:59Z",
              "comment_text": "\\cc @normanmaurer "
            },
            {
              "comment_username": "normanmaurer",
              "comment_create_time": "2020-08-07T07:19:28Z",
              "comment_edit_time": "2020-08-07T07:19:28Z",
              "comment_text": "I will have a look next week. "
            },
            {
              "comment_username": "normanmaurer",
              "comment_create_time": "2020-08-12T09:23:12Z",
              "comment_edit_time": "2020-08-12T09:23:12Z",
              "comment_text": "@chrisvest PTAL when you have a chance "
            },
            {
              "comment_username": "skyguard1",
              "comment_create_time": "2020-08-13T02:16:24Z",
              "comment_edit_time": "2020-08-13T02:16:24Z",
              "comment_text": "> I had some initial comments. I'll review more thoroughly tomorrow.\r\n\r\nThanks for your review"
            },
            {
              "comment_username": "normanmaurer",
              "comment_create_time": "2020-08-13T18:35:25Z",
              "comment_edit_time": "2020-08-13T18:35:25Z",
              "comment_text": "@idelpivnitskiy you had any time yet to review this one ?"
            },
            {
              "comment_username": "skyguard1",
              "comment_create_time": "2020-08-14T02:07:23Z",
              "comment_edit_time": "2020-08-14T02:07:39Z",
              "comment_text": "> Hi @skyguard1,\r\n> \r\n> Thank you for this huge effort in adding a new compression codec! This is a complex task.\r\n> I have comments for the existing implementation, but before addressing them let's discuss the format of Zstd blocks first, because it may impact the current implementation. Please, see my last comment in this review.\r\n\r\nThanks for your great effort in code review, I will take a look at it and make corresponding changes.appreciate it."
            },
            {
              "comment_username": "skyguard1",
              "comment_create_time": "2020-08-17T05:44:48Z",
              "comment_edit_time": "2020-08-17T05:50:58Z",
              "comment_text": "I made some changes, the biggest problem is to implement the standard protocol format, which will have huge changes, anyone can make their own suggestion"
            },
            {
              "comment_username": "normanmaurer",
              "comment_create_time": "2020-08-27T06:33:35Z",
              "comment_edit_time": "2020-08-27T06:33:35Z",
              "comment_text": "@skyguard1 please address comments of @idelpivnitskiy and ping me once ready for review again "
            },
            {
              "comment_username": "skyguard1",
              "comment_create_time": "2020-08-27T06:38:38Z",
              "comment_edit_time": "2020-08-27T06:38:38Z",
              "comment_text": "@normanmaurer,Sorry, I am trying to implement the rfc8478 specification, it is really a bit complicated, I will tell you when I finish, thank"
            },
            {
              "comment_username": "chrisvest",
              "comment_create_time": "2020-08-27T11:50:21Z",
              "comment_edit_time": "2020-08-27T11:50:21Z",
              "comment_text": "@skyguard1 What's the source of the complexity? I would've though the zstd library would provide most of what's needed?"
            },
            {
              "comment_username": "skyguard1",
              "comment_create_time": "2020-08-27T11:59:59Z",
              "comment_edit_time": "2020-08-27T11:59:59Z",
              "comment_text": "@chrisvest，I need to implement the rfc8478 specification, which has a huge modification to the original logic, and I have to do more tests to ensure that it can be universal"
            },
            {
              "comment_username": "idelpivnitskiy",
              "comment_create_time": "2020-08-27T17:24:09Z",
              "comment_edit_time": "2020-08-27T17:26:32Z",
              "comment_text": "@skyguard1 I would be happy to help if you have any questions or need guidance on the approach. Feel free to ask in this thread.\r\n\r\nI would propose to implement it in stages. It's fine to send multiple PRs. For example, you can split the work in this way:\r\n1. `ZstdEncoder` - it should be relatively simple, because you can just use the `Zstd` library to compress data. The only thing you need to do there is to figure out the correct size of the output `ByteBuf` to minimize redundant allocations.\r\n2. `ZstdDecoder` - this is an interesting part. You do not need to implement the full rfc8478 specification, the `Zstd` library should be used for the actual decompression. The only important bit there is a header format. Most likely (if `Zstd` library does not help with that*) you need to parse the header to understand which blocks are compressed and which are not + the decompressed size of the block. This info is helpful to allocate necessary `ByteBuf`s. \r\n\r\n\\* zstd format is similar to gzip. Take a look at existing `ZlibDecoder` and its two implementations: `JZlibDecoder` & `JdkZlibDecoder`:\r\n- `JZlibDecoder` uses `com.jcraft.jzlib` library that does not require netty to parse the header, it does all the work. \r\n- `JdkZlibDecoder` uses `java.util.zip` that is less flexible and required netty to parse the gzip header.\r\n\r\nTry to dig into the `zstd` library to understand its capabilities better and decide on the decoder approach.\r\n\r\nTesting: do not spend time on excessive testing during the development. We can decided during the review if additional testing is necessary. As a start, use existing `AbstractEncoderTest` and `AbstractDecoderTest` classes. They contain all required tests to check compatibility between netty's codec and the original implementation from the 3-party library. You can use these classes to verify that the output is compatible with `ZstdInputStream` & `ZstdOutputStream`."
            },
            {
              "comment_username": "skyguard1",
              "comment_create_time": "2020-08-28T02:22:01Z",
              "comment_edit_time": "2020-08-28T02:22:01Z",
              "comment_text": "@idelpivnitskiy，I will look at the implementation of zstd in more detail, thanks for your suggestions, appreciate it"
            }
          ]
        },
        {
          "issue_url": "https://github.com/netty/netty/issues/10411",
          "issue_title": "Epoll client channel throws NotYetConnectedException on connect failure",
          "issue_number": 10411,
          "issue_text": "I am observing a intermittent failure case where a client using the `EpollSocketChannel` transport will have a `NotYetConnectedException` raised on its pipeline as part of a connection attempt failing.\r\n\r\n### Expected behavior\r\n\r\nConnection failures will result in the `connect()` future failing and not invoke any handlers in the channel pipeline.\r\n\r\n### Actual behavior\r\n\r\n`NotYetConnectedException` is sometimes thrown on the pipeline for the failed channel:\r\n\r\n```\r\n(epollEventLoopGroup-3-16) {ChannelInboundHandler#exceptionCaught}: java.nio.channels.NotYetConnectedException: null\r\n     at io.netty.channel.unix.Errors.ioResult(Errors.java:171)\r\n     at io.netty.channel.unix.FileDescriptor.readAddress(FileDescriptor.java:143)\r\n     at io.netty.channel.epoll.AbstractEpollChannel.doReadBytes(AbstractEpollChannel.java:348)\r\n     at io.netty.channel.epoll.AbstractEpollStreamChannel$EpollStreamUnsafe.epollInReady(AbstractEpollStreamChannel.java:778)\r\n     at io.netty.channel.epoll.EpollEventLoop.processReady(EpollEventLoop.java:475)\r\n     at io.netty.channel.epoll.EpollEventLoop.run(EpollEventLoop.java:378)\r\n     at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)\r\n     at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\r\n     at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\r\n     at java.lang.Thread.run(Thread.java:834)\r\n```\r\n\r\nAccompanied by `ClosedChannelException` for the actual connect future:\r\n\r\n```\r\njava.nio.channels.ClosedChannelException: null\r\n     at io.netty.channel.AbstractChannel$AbstractUnsafe.newClosedChannelException(AbstractChannel.java:957)\r\n     at io.netty.channel.AbstractChannel$AbstractUnsafe.ensureOpen(AbstractChannel.java:976)\r\n     at io.netty.channel.epoll.AbstractEpollChannel$AbstractEpollUnsafe.connect(AbstractEpollChannel.java:552)\r\n     at io.netty.channel.DefaultChannelPipeline$HeadContext.connect(DefaultChannelPipeline.java:1342)\r\n     at io.netty.channel.AbstractChannelHandlerContext.invokeConnect(AbstractChannelHandlerContext.java:548)\r\n     at io.netty.channel.AbstractChannelHandlerContext.connect(AbstractChannelHandlerContext.java:533)\r\n     at io.netty.channel.CombinedChannelDuplexHandler$DelegatingChannelHandlerContext.connect(CombinedChannelDuplexHandler.java:495)\r\n     at io.netty.channel.ChannelOutboundHandlerAdapter.connect(ChannelOutboundHandlerAdapter.java:51)\r\n     at io.netty.channel.CombinedChannelDuplexHandler.connect(CombinedChannelDuplexHandler.java:296)\r\n     at io.netty.channel.AbstractChannelHandlerContext.invokeConnect(AbstractChannelHandlerContext.java:548)\r\n     at io.netty.channel.AbstractChannelHandlerContext.connect(AbstractChannelHandlerContext.java:533)\r\n     at io.netty.handler.logging.LoggingHandler.connect(LoggingHandler.java:231)\r\n     at io.netty.channel.AbstractChannelHandlerContext.invokeConnect(AbstractChannelHandlerContext.java:548)\r\n     at io.netty.channel.AbstractChannelHandlerContext.connect(AbstractChannelHandlerContext.java:533)\r\n     at io.netty.channel.AbstractChannelHandlerContext.connect(AbstractChannelHandlerContext.java:517)\r\n     at io.netty.channel.DefaultChannelPipeline.connect(DefaultChannelPipeline.java:978)\r\n     at io.netty.channel.AbstractChannel.connect(AbstractChannel.java:253)\r\n     at io.netty.bootstrap.Bootstrap$3.run(Bootstrap.java:250)\r\n     at io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:164)\r\n     at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:472)\r\n     at io.netty.channel.epoll.EpollEventLoop.run(EpollEventLoop.java:384)\r\n     at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)\r\n     at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\r\n     at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\r\n     at java.lang.Thread.run(Thread.java:834)\r\n```\r\n\r\n### Steps to reproduce\r\n\r\nIssue happens very intermittently and is difficult to reproduce on-demand.\r\n\r\nI am not deeply familiar w/ Epoll so I am raising this issue to see if someone with more experience here might be able to spot some type of defect. I think these lines from the stack trace are relevant:\r\n\r\n```\r\n     at io.netty.channel.epoll.AbstractEpollChannel.doReadBytes(AbstractEpollChannel.java:348)\r\n     at io.netty.channel.epoll.AbstractEpollStreamChannel$EpollStreamUnsafe.epollInReady(AbstractEpollStreamChannel.java:778)\r\n     at io.netty.channel.epoll.EpollEventLoop.processReady(EpollEventLoop.java:475)\r\n```\r\n\r\nhttps://github.com/netty/netty/blob/4e86f768b916a34d3a929251eff8428ef47be65d/transport-native-epoll/src/main/java/io/netty/channel/epoll/EpollEventLoop.java#L468-L476\r\n\r\nhttps://github.com/netty/netty/blob/4e86f768b916a34d3a929251eff8428ef47be65d/transport-native-epoll/src/main/java/io/netty/channel/epoll/AbstractEpollStreamChannel.java#L778\r\n\r\nhttps://github.com/netty/netty/blob/4e86f768b916a34d3a929251eff8428ef47be65d/transport-native-epoll/src/main/java/io/netty/channel/epoll/AbstractEpollChannel.java#L347-L348\r\n\r\nI noticed that commit https://github.com/netty/netty/commit/c7cb104dc48f179b61ed1146b6c0529c2e1115bc#diff-db3e069239a403b954e3ebc024ba9507L336 removed a `&& ch.isOpen()` check that used to previously guard the `epollInReady()` call:\r\n\r\nhttps://github.com/netty/netty/blob/dbbdbe11a6237cc08a9a6eafeeac9c7d80924305/transport-native-epoll/src/main/java/io/netty/channel/epoll/EpollEventLoop.java#L336\r\n\r\nI'm wondering if that behavior is correct. Rather than dropping `ch.isOpen()` to support half-closed, should there at least be a check to see if the channel was *ever* open? I suspect this change might make sense from a server perspective but perhaps not a client perspective?\r\n\r\n### Netty version\r\n\r\n4.1.50",
          "issue_comments": [
            {
              "comment_username": "normanmaurer",
              "comment_create_time": "2020-08-13T12:39:07Z",
              "comment_edit_time": "2020-08-13T12:39:07Z",
              "comment_text": "On my to do list for tomorrow.... "
            },
            {
              "comment_username": "normanmaurer",
              "comment_create_time": "2020-08-19T10:25:50Z",
              "comment_edit_time": "2020-08-19T10:25:50Z",
              "comment_text": "alright... it took me a bit more then expected but I am looking at this as we speak. "
            },
            {
              "comment_username": "normanmaurer",
              "comment_create_time": "2020-08-19T13:37:32Z",
              "comment_edit_time": "2020-08-19T13:37:32Z",
              "comment_text": "@Bennett-Lynch so I did look into it a bit today and honestly I still have no idea what exactly happens. I mean we could \"hide\" the error by checking `isOpen()` but this still does not feel right to me as I would like to understand why it happens in the first place. \r\n\r\nSo this can only happen if the FD is closed before the connect(...) is actually executed which means something must have called `close()` before. I wonder what this was. It may be something that happened during pipeline initialisation (like for example during `channelRegistered(....)` calls. Do you have some more details for me how the pipeline looks like in terms of handlers ?"
            },
            {
              "comment_username": "Bennett-Lynch",
              "comment_create_time": "2020-08-19T20:03:59Z",
              "comment_edit_time": "2020-08-19T20:03:59Z",
              "comment_text": "> @Bennett-Lynch so I did look into it a bit today and honestly I still have no idea what exactly happens. I mean we could \"hide\" the error by checking isOpen() but this still does not feel right to me as I would like to understand why it happens in the first place.\r\n\r\nThanks a bunch for taking a look, @normanmaurer. Agree on not just hiding it. Likewise, I have considered placing an `isOpen()` check in my own pipeline before trying to handle this exception, but that also does not feel right.\r\n\r\n> So this can only happen if the FD is closed before the connect(...) is actually executed which means something must have called close() before. I wonder what this was.\r\n\r\nAre we sure this is always the case? See [here](https://stackoverflow.com/questions/900042/what-causes-the-enotconn-error/902964#902964) for other potential explanations of `ENOTCONN`, which I believe is the error code triggering this behavior. Are we making potentially faulty assumptions about what some of the Epoll error codes might represent?\r\n\r\n> It may be something that happened during pipeline initialisation (like for example during channelRegistered(....) calls. Do you have some more details for me how the pipeline looks like in terms of handlers?\r\n\r\nSure. The only Netty handlers in the pipeline are `HttpClientCodec` and `LoggingHandler`. There are a few custom handlers in the pipeline but none of them use `channelRegistered(..)` or `handlerAdded(..)`.\r\n\r\nThere does exist logic outside of the pipeline that may try to close the client channel, but I have metrics for when this behavior is triggered and it does not appear to be triggered in this case. It looks like this though:\r\n\r\n```\r\nconnectFuture.addListener(ChannelFutureListener.CLOSE);\r\nconnectFuture.cancel(true);\r\n```\r\n\r\nAgain, I am quite confident that this behavior is not being triggered, but I will do some additional validation to ensure that."
            },
            {
              "comment_username": "normanmaurer",
              "comment_create_time": "2020-08-20T06:34:28Z",
              "comment_edit_time": "2020-08-20T06:34:28Z",
              "comment_text": "> Are we sure this is always the case? See here for other potential explanations of ENOTCONN, which I believe is the error code triggering this behavior. Are we making potentially faulty assumptions about what some of the Epoll error codes might represent?\r\n\r\nYes I am pretty sure in this case as you can see from your stack trace `connect` (sys call) is never called which means the socket was never connected in the first place.\r\n\r\n> Sure. The only Netty handlers in the pipeline are HttpClientCodec and LoggingHandler. There are a few custom handlers in the pipeline but none of them use channelRegistered(..) or handlerAdded(..).\r\n\r\n> There does exist logic outside of the pipeline that may try to close the client channel, but I have metrics for when this behavior is triggered and it does not appear to be triggered in this case. It looks like this though:\r\n\r\n> connectFuture.addListener(ChannelFutureListener.CLOSE);\r\nconnectFuture.cancel(true);\r\n\r\n> Again, I am quite confident that this behavior is not being triggered, but I will do some additional validation to ensure that.\r\n\r\nActually this is interesting as I think this could lead up to the situation where we may race and close the channel in between which in this case would just close the FD.\r\n\r\nAlso as a side note even if the channel is never connected you may see `exceptionCaught` be called if for example ?`channelRegistered(...)` or `handlerAdded(...)` throws.\r\n\r\n@Bennett-Lynch would it be possible for you to run a custom jar with some more logging enabled when this issue happens ?\r\n\r\n"
            },
            {
              "comment_username": "Bennett-Lynch",
              "comment_create_time": "2020-08-20T17:43:52Z",
              "comment_edit_time": "2020-08-20T17:43:52Z",
              "comment_text": "> Actually this is interesting as I think this could lead up to the situation where we may race and close the channel in between which in this case would just close the FD.\r\n\r\nI wonder if `cancel(true)` would make this more racy than `cancel(false)`? Again, I don't think this is being triggered, but I will try to confirm this with 100% confidence. I will need to await some recent changes to be rolled out before validating.\r\n\r\n> Also as a side note even if the channel is never connected you may see exceptionCaught be called if for example ?channelRegistered(...) or handlerAdded(...) throws.\r\n\r\nUnderstood, as far as I can tell this is not the case though.\r\n\r\n> @Bennett-Lynch would it be possible for you to run a custom jar with some more logging enabled when this issue happens ?\r\n\r\nI would probably be more likely to be able to apply custom git diffs/patches rather than an entire jar. Would also need the logging to only be during exceptional cases. If it logs extra in the happy case it will be quite noisy and expensive."
            },
            {
              "comment_username": "normanmaurer",
              "comment_create_time": "2020-08-21T12:38:20Z",
              "comment_edit_time": "2020-08-21T12:38:20Z",
              "comment_text": "@Bennett-Lynch alright I will provide a diff very soon. "
            }
          ]
        }
      ]
    }
  },
  {
    "github_url": "https://github.com/alkacon/opencms-core",
    "github_info": {
      "name": "alkacon/opencms-core",
      "language": "Java"
    },
    "github_branches": {
      "branch_datas": [
        {
          "branch_version": "branch_6_0_x",
          "branch_url": "https://github.com/alkacon/opencms-core/tree/branch_6_0_x",
          "branch_download_url": "https://github.com/alkacon/opencms-core/archive/branch_6_0_x.zip"
        },
        {
          "branch_version": "branch_6_2_x",
          "branch_url": "https://github.com/alkacon/opencms-core/tree/branch_6_2_x",
          "branch_download_url": "https://github.com/alkacon/opencms-core/archive/branch_6_2_x.zip"
        },
        {
          "branch_version": "branch_7_0_x",
          "branch_url": "https://github.com/alkacon/opencms-core/tree/branch_7_0_x",
          "branch_download_url": "https://github.com/alkacon/opencms-core/archive/branch_7_0_x.zip"
        },
        {
          "branch_version": "branch_7_5_x",
          "branch_url": "https://github.com/alkacon/opencms-core/tree/branch_7_5_x",
          "branch_download_url": "https://github.com/alkacon/opencms-core/archive/branch_7_5_x.zip"
        },
        {
          "branch_version": "branch_8_0_x",
          "branch_url": "https://github.com/alkacon/opencms-core/tree/branch_8_0_x",
          "branch_download_url": "https://github.com/alkacon/opencms-core/archive/branch_8_0_x.zip"
        },
        {
          "branch_version": "branch_8_5_x",
          "branch_url": "https://github.com/alkacon/opencms-core/tree/branch_8_5_x",
          "branch_download_url": "https://github.com/alkacon/opencms-core/archive/branch_8_5_x.zip"
        },
        {
          "branch_version": "branch_9_0_x",
          "branch_url": "https://github.com/alkacon/opencms-core/tree/branch_9_0_x",
          "branch_download_url": "https://github.com/alkacon/opencms-core/archive/branch_9_0_x.zip"
        },
        {
          "branch_version": "branch_9_5_x",
          "branch_url": "https://github.com/alkacon/opencms-core/tree/branch_9_5_x",
          "branch_download_url": "https://github.com/alkacon/opencms-core/archive/branch_9_5_x.zip"
        },
        {
          "branch_version": "branch_10_0_x",
          "branch_url": "https://github.com/alkacon/opencms-core/tree/branch_10_0_x",
          "branch_download_url": "https://github.com/alkacon/opencms-core/archive/branch_10_0_x.zip"
        },
        {
          "branch_version": "branch_10_5_x",
          "branch_url": "https://github.com/alkacon/opencms-core/tree/branch_10_5_x",
          "branch_download_url": "https://github.com/alkacon/opencms-core/archive/branch_10_5_x.zip"
        },
        {
          "branch_version": "branch_11_0_x",
          "branch_url": "https://github.com/alkacon/opencms-core/tree/branch_11_0_x",
          "branch_download_url": "https://github.com/alkacon/opencms-core/archive/branch_11_0_x.zip"
        },
        {
          "branch_version": "branch_12_0_x",
          "branch_url": "https://github.com/alkacon/opencms-core/tree/branch_12_0_x",
          "branch_download_url": "https://github.com/alkacon/opencms-core/archive/branch_12_0_x.zip"
        },
        {
          "branch_version": "master",
          "branch_url": "https://github.com/alkacon/opencms-core/tree/master",
          "branch_download_url": "https://github.com/alkacon/opencms-core/archive/master.zip"
        }
      ]
    },
    "github_pull_requests": {
      "pull_datas": [
        {
          "pull_number": 683,
          "pull_title": "Fix typo - authentication instead of authentification",
          "pull_version": "alkacon:branch_11_0_x",
          "pull_version_url": "https://github.com/alkacon/opencms-core/tree/branch_11_0_x"
        },
        {
          "pull_number": 637,
          "pull_title": "correct spelling",
          "pull_version": "alkacon:branch_11_0_x",
          "pull_version_url": "https://github.com/alkacon/opencms-core/tree/branch_11_0_x"
        },
        {
          "pull_number": 624,
          "pull_title": "Wrong PostgreSQL syntax on init OpenCMS",
          "pull_version": "alkacon:branch_11_0_x",
          "pull_version_url": "https://github.com/alkacon/opencms-core/tree/branch_11_0_x"
        },
        {
          "pull_number": 614,
          "pull_title": "(Feature Request) Override search dir from system properties",
          "pull_version": "alkacon:branch_11_0_x",
          "pull_version_url": "https://github.com/alkacon/opencms-core/tree/branch_11_0_x"
        },
        {
          "pull_number": 612,
          "pull_title": "Add feature: cms:headincludes automatically adds the version id to the URI",
          "pull_version": "alkacon:branch_11_0_x",
          "pull_version_url": "https://github.com/alkacon/opencms-core/tree/branch_11_0_x"
        },
        {
          "pull_number": 610,
          "pull_title": "Fixed an issue where CmsFlexCache is not thread safe.",
          "pull_version": "alkacon:branch_11_0_x",
          "pull_version_url": "https://github.com/alkacon/opencms-core/tree/branch_11_0_x"
        },
        {
          "pull_number": 601,
          "pull_title": "Modified Build.gradle to support Intellij-idea and Eclipse",
          "pull_version": "alkacon:branch_11_0_x",
          "pull_version_url": "https://github.com/alkacon/opencms-core/tree/branch_11_0_x"
        },
        {
          "pull_number": 600,
          "pull_title": "Russian Localization for OpenCms 10.5",
          "pull_version": "alkacon:branch_10_5_x",
          "pull_version_url": "https://github.com/alkacon/opencms-core/tree/branch_10_5_x"
        },
        {
          "pull_number": 599,
          "pull_title": "The field id of CmsSolrDocument cannot use type of CmsUUID.",
          "pull_version": "alkacon:branch_11_0_x",
          "pull_version_url": "https://github.com/alkacon/opencms-core/tree/branch_11_0_x"
        },
        {
          "pull_number": 545,
          "pull_title": "Refactor build script",
          "pull_version": "alkacon:branch_10_5_x",
          "pull_version_url": "https://github.com/alkacon/opencms-core/tree/branch_10_5_x"
        },
        {
          "pull_number": 517,
          "pull_title": "Refactor build.gradle",
          "pull_version": "alkacon:branch_10_5_x",
          "pull_version_url": "https://github.com/alkacon/opencms-core/tree/branch_10_5_x"
        },
        {
          "pull_number": 429,
          "pull_title": "Setup: hide passwords in forms",
          "pull_version": "alkacon:branch_10_5_x",
          "pull_version_url": "https://github.com/alkacon/opencms-core/tree/branch_10_5_x"
        },
        {
          "pull_number": 200,
          "pull_title": "This is one of possible fixes for #199",
          "pull_version": "alkacon:branch_8_5_x",
          "pull_version_url": "https://github.com/alkacon/opencms-core/tree/branch_8_5_x"
        }
      ]
    },
    "github_issues": {
      "issue_datas": [
        {
          "issue_url": "https://github.com/alkacon/opencms-core/issues/703",
          "issue_title": "Non-Root Organisation Unit Issue",
          "issue_number": 703,
          "issue_text": "Hi,\r\n\r\nI have several OU. I want to disable the OU during the login process. After I checked in \"Edit Organizational Unit\" > Hide from Login form, the user logs in but there is no Page Editor Toolbar. The User has the Role \"Author\".\r\n\r\nHow can I have on the one side the login process without typing the OU and the other side the user seeing the Toolbar. To me its strange behaviour.\r\n\r\nSincerly\r\nThanks.\r\n",
          "issue_comments": []
        },
        {
          "issue_url": "https://github.com/alkacon/opencms-core/issues/702",
          "issue_title": "Performance issue creating contents",
          "issue_number": 702,
          "issue_text": "We are facing the following performance issues when a folder of contents has a lot of resources in combination with permission checks.\\r\\n\\r\\n### Steps to reproduce\\r\\n\\r\\nPreconditions:\\r\\n- Folder /.content/linklist/ contains 10000 resources li_00001.xml to li_10000.xml\\r\\n- User as edit rights, but role \"VFS resource mangager\" is not assigned\\r\\n\\r\\n1) Open container page and drag&drop a linklist to container (the new content will be create in memory only)\\r\\n2) Edit the new content (resource name will be generated and resource be created in VFS)\\r\\n\\r\\nTo generate resource name, all 10000 content resouces will be read and checked for permissions.\\r\\nTakes around 30s after click on edit:\\r\\n\\r\\n![imagen](https://user-images.githubusercontent.com/2276952/90272170-e6432280-de54-11ea-98cc-942574cea536.png)\\r\\n\\r\\nIn case user clicks edit after a few seconds again (because nothing happens), OpenCms will send a second request which results in an error \"resources already exists\"\\r\\n\\r\\n(Tested in OpenCms 10.5.3)\\r\\n\\r\\nThe cause is the readResources() in CmsDefaultFileNameGenerator with does the permission check, which runs over 10000 resources to find the succeeding number for resource name po_10001.xml:\\r\\n\\r\\n```\\r\\n    public String getNewFileName(CmsObject cms, String namePattern, int defaultDigits, boolean explorerMode)\\r\\n    throws CmsException {\\r\\n\\r\\n        String checkPattern = cms.getRequestContext().removeSiteRoot(namePattern);\\r\\n        String folderName = CmsResource.getFolderPath(checkPattern);\\r\\n\\r\\n        // must check ALL resources in folder because name doesn't care for type\\r\\n        List<CmsResource> resources = cms.readResources(folderName, CmsResourceFilter.ALL, false);\\r\\n\\r\\n        // now create a list of all the file names\\r\\n        List<String> fileNames = new ArrayList<String>(resources.size());\\r\\n        for (CmsResource res : resources) {\\r\\n            fileNames.add(cms.getSitePath(res));\\r\\n        }\\r\\n\\r\\n        return getNewFileNameFromList(fileNames, checkPattern, defaultDigits, explorerMode);\\r\\n    }\\r\\n```\\r\\n\\t\\r\\n\\t\\r\\n\\t\\r\\nThe problem seems to get worse when using OpenCms 11.0.1, as readResources() in addition is called for Online project:\\r\\n\\r\\n```\\r\\n    public String getNewFileName(CmsObject userCms, String namePattern, int defaultDigits, boolean explorerMode)\\r\\n    throws CmsException {\\r\\n\\r\\n        CmsObject cms = OpenCms.initCmsObject(m_adminCms);\\r\\n        cms.getRequestContext().setSiteRoot(userCms.getRequestContext().getSiteRoot());\\r\\n        cms.getRequestContext().setCurrentProject(userCms.getRequestContext().getCurrentProject());\\r\\n        String checkPattern = cms.getRequestContext().removeSiteRoot(namePattern);\\r\\n        String folderName = CmsResource.getFolderPath(checkPattern);\\r\\n\\r\\n        // must check ALL resources in folder because name doesn't care for type\\r\\n        List<CmsResource> resources = cms.readResources(folderName, CmsResourceFilter.ALL, false);\\r\\n        CmsObject onlineCms = OpenCms.initCmsObject(cms);\\r\\n        onlineCms.getRequestContext().setCurrentProject(cms.readProject(CmsProject.ONLINE_PROJECT_ID));\\r\\n\\r\\n        // now create a list of all the file names\\r\\n        Set<String> fileNames = new HashSet<>();\\r\\n        for (CmsResource res : resources) {\\r\\n            fileNames.add(cms.getSitePath(res));\\r\\n        }\\r\\n\\r\\n        try {\\r\\n            CmsResource offlineFolder = cms.readResource(folderName);\\r\\n            CmsResource onlineFolder = onlineCms.readResource(offlineFolder.getStructureId());\\r\\n            String onlinePath = onlineCms.getSitePath(onlineFolder);\\r\\n            List<CmsResource> onlineContents = onlineCms.readResources(onlinePath, CmsResourceFilter.ALL, false);\\r\\n            for (CmsResource res : onlineContents) {\\r\\n                fileNames.add(cms.getSitePath(res));\\r\\n            }\\r\\n\\r\\n        } catch (CmsException e) {\\r\\n            LOG.warn(e.getLocalizedMessage(), e);\\r\\n        }\\r\\n\\r\\n        return getNewFileNameFromList(fileNames, checkPattern, defaultDigits, explorerMode);\\r\\n    }\\r\\n```\\r\\n\\r\\nPossible solutions could be not to do permission check when \"just\" creating resource name.\\r\\n\\r\\n\\r\\nAdd least the edit button could be disabled until request to create resource in VFS was return.\\r\\nSo it would still be quite slow, but not resulting in an error.",
          "issue_comments": [
            {
              "comment_username": "dSeidel",
              "comment_create_time": "2020-08-18T08:24:07Z",
              "comment_edit_time": "2020-08-18T08:24:07Z",
              "comment_text": "Hi @kartobi ,\r\nyou already copied the code snippet from the head of branch_11_0_x. And this is the version as is in OpenCms 11.0.2. It uses an \"admin\"-CmsObject, which means that permission checks are skipped, since for root administrators have always all permissions granted.\r\nIn release 11.0.1 the code is different and does not use an \"admin-CmsObject and hence might check permissions. Consequently, with OpenCms 11.0.2 performance should be better."
            },
            {
              "comment_username": "tobias-karrer",
              "comment_create_time": "2020-08-18T09:13:46Z",
              "comment_edit_time": "2020-08-18T09:13:46Z",
              "comment_text": "Hi @dSeidel,\r\n\r\nyou're didn't notice admin object was initialized. Maybe because I checked 12_0_x as well, were it wasn't changed yet.\r\nI'll try to patch it in 10.5.2 we currently use."
            },
            {
              "comment_username": "tobias-karrer",
              "comment_create_time": "2020-09-16T14:40:47Z",
              "comment_edit_time": "2020-09-16T14:40:47Z",
              "comment_text": "Patching by the 11.0.2 version helped to speed up.\r\n\r\nWhat about \r\n> Add least the edit button could be disabled until request to create resource in VFS was return.\r\n> So it would still be quite slow, but not resulting in an error.\r\nWould that be possible?\r\n\r\nWe often get user feedback about this error."
            }
          ]
        },
        {
          "issue_url": "https://github.com/alkacon/opencms-core/issues/701",
          "issue_title": "opencms.properties values empty when using oCMS with Amazon RDS DB",
          "issue_number": 701,
          "issue_text": "Hello,\\r\\n\\r\\nWe are facing a problem: when using OpenCms with an RDS Postgres DB instance on Amazon, opencms.properties has wrong values. (see below)\\r\\n\\r\\n#\\r\\n# Configuration of the driver manager*\\r\\n#################################################################################\\r\\ndriver.vfs=db\\r\\ndriver.project=db\\r\\ndriver.user=db\\r\\ndriver.history=db\\r\\ndriver.subscription=db\\r\\n#\\r\\n# Declaration of database pools\\r\\n#################################################################################\\r\\ndb.pools=default\\r\\n#\\r\\n# Configuration of the default database pool\\r\\n#################################################################################\\r\\n# name of the JDBC driver\\r\\ndb.pool.default.jdbcDriver=1\\r\\n# URL of the JDBC driver\\r\\ndb.pool.default.jdbcUrl=\\r\\n# optional parameters for the URL of the JDBC driver\\r\\ndb.pool.default.jdbcUrl.params=\\r\\n# user name to connect to the database\\r\\ndb.pool.default.user=\\r\\n# password to connect to the database\\r\\ndb.pool.default.password=\\r\\n# the URL to make the JDBC DriverManager return connections from the DBCP pool\\r\\ndb.pool.default.poolUrl=opencms:default\\r\\n# Maximum pool size\\r\\ndb.pool.default.v11.maximumPoolSize=20\\r\\n# Only needs to be set if JDBC driver does not support JDBC4\\r\\ndb.pool.default.v11.connectionTestQuery=\\r\\nadditional.dbprops=\\r\\n#\\r\\n# Configuration of the database driver manager\\r\\n#################################################################################\\r\\ndb.name=\\r\\ndb.vfs.driver=\\r\\ndb.vfs.pool=opencms:default\\r\\ndb.vfs.sqlmanager=\\r\\ndb.project.driver=\\r\\ndb.project.pool=opencms:default\\r\\ndb.project.sqlmanager=\\r\\ndb.user.driver=\\r\\ndb.user.pool=opencms:default\\r\\ndb.user.sqlmanager=\\r\\ndb.history.driver=\\r\\ndb.history.pool=opencms:default\\r\\ndb.history.sqlmanager=\\r\\ndb.subscription.driver=\\r\\ndb.subscription.pool=opencms:default\\r\\ndb.subscription.sqlmanager=\\r\\n#\\r\\n# Ethernet address used for UUID generation\\r\\n# Server name used for various messages\\r\\n#################################################################################\\r\\nserver.ethernet.address=\\r\\nserver.name=OpenCmsServer\\r\\n#\\r\\n# Enable/Disable OpenCms Setup Wizard\\r\\n# The wizard sets the flag to false after the setup.\\r\\n# To use the wizard again, reset it manually to true.\\r\\n# By setting no value, wizard can always be used.\\r\\n#################################################################################\\r\\nwizard.enabled=true\\r\\n#\\r\\n# Enable/Disable exception thrown during servlet initialization.\\r\\n# If disabled there is no attempt to reinitialize the servlet, so the servlet\\r\\n# container has to be restarted, as required by some servlet containers that\\r\\n# does not like servlets throwing exceptions during initialization.\\r\\n# valid values are true, false and auto. default is auto\\r\\n#################################################################################\\r\\nbut in setup.properties, there are correct values.\\r\\n\\r\\nThis leads to a \"org.opencms.main.CmsInitException: Critical error during OpenCms initialization: Unable to initialize connection pool \"default\". Is the database up and running?\" exception when starting the Docker image with oCMS that tries to do the setup.\\r\\n \\r\\nOn the internet, we found this https://github.com/alkacon/opencms-docker/issues/12 on Alkacon's OpenCms repo, with the same problem, but no solution listed just mentioned solved. \\r\\n\\r\\nCould you please advise on what we could do?\\r\\n\\r\\nBest regards,\\r\\nGabriel-Alin",
          "issue_comments": []
        },
        {
          "issue_url": "https://github.com/alkacon/opencms-core/issues/698",
          "issue_title": "GWT error - I_CmsContainerpageService_Proxy",
          "issue_number": 698,
          "issue_text": "Hello, we have observed two errors in log file (first one also pops up in the UI) in GWT.\r\n\r\n**I_CmsContainerpageService_Proxy.checkContainerpageOrElementsChanged**\r\n`14 May 2020 13:23:32,399 ERROR [ org.opencms.gwt.CmsLogService:  66] Client LOG (Host 31.30.4.34, Address 31.30.4.34, Ticket 1589455410703):  from I_CmsContainerpageService_Proxy.checkContainerpageOrElementsChanged\r\nUnknown.De(https://xxx/handleStatic/v1449498988v/gwt/opencms/878871B8F95ECAF1B8F8D3C46B6337B1.cache.html)\r\nUnknown.Ne(https://xxx/handleStatic/v1449498988v/gwt/opencms/878871B8F95ECAF1B8F8D3C46B6337B1.cache.html)\r\nUnknown.Qe(https://xxx/handleStatic/v1449498988v/gwt/opencms/878871B8F95ECAF1B8F8D3C46B6337B1.cache.html)\r\nUnknown.F4b(https://xxx/handleStatic/v1449498988v/gwt/opencms/878871B8F95ECAF1B8F8D3C46B6337B1.cache.html)\r\nUnknown.e9b(https://xxx/handleStatic/v1449498988v/gwt/opencms/878871B8F95ECAF1B8F8D3C46B6337B1.cache.html)\r\nUnknown.eT(https://xxx/handleStatic/v1449498988v/gwt/opencms/878871B8F95ECAF1B8F8D3C46B6337B1.cache.html)\r\nUnknown.sT(https://xxx/handleStatic/v1449498988v/gwt/opencms/878871B8F95ECAF1B8F8D3C46B6337B1.cache.html)\r\nUnknown.onreadystatechange<(https://xxx/handleStatic/v1449498988v/gwt/opencms/878871B8F95ECAF1B8F8D3C46B6337B1.cache.html)\r\nUnknown.TD(https://xxx/handleStatic/v1449498988v/gwt/opencms/878871B8F95ECAF1B8F8D3C46B6337B1.cache.html)\r\nUnknown.WD(https://xxx/handleStatic/v1449498988v/gwt/opencms/878871B8F95ECAF1B8F8D3C46B6337B1.cache.html)\r\nUnknown.VD/<(https://xxx/handleStatic/v1449498988v/gwt/opencms/878871B8F95ECAF1B8F8D3C46B6337B1.cache.html)\r\nUnknown.anonymous(Unknown)`\r\n\r\n**I_CmsContainerpageService_Proxy.getGalleryDataForPage**\r\n`14 May 2020 13:24:29,554 ERROR [ org.opencms.gwt.CmsLogService:  66] Client LOG (Host 31.30.4.34, Address 31.30.4.34, Ticket 1589455468241):  from I_CmsContainerpageService_Proxy.getGalleryDataForPage\r\nUnknown.De(https://xxx/handleStatic/v1449498988v/gwt/opencms/878871B8F95ECAF1B8F8D3C46B6337B1.cache.html)\r\nUnknown.Ne(https://xxx/handleStatic/v1449498988v/gwt/opencms/878871B8F95ECAF1B8F8D3C46B6337B1.cache.html)\r\nUnknown.Qe(https://xxx/handleStatic/v1449498988v/gwt/opencms/878871B8F95ECAF1B8F8D3C46B6337B1.cache.html)\r\nUnknown.F4b(https://xxx/handleStatic/v1449498988v/gwt/opencms/878871B8F95ECAF1B8F8D3C46B6337B1.cache.html)\r\nUnknown.e9b(https://xxx/handleStatic/v1449498988v/gwt/opencms/878871B8F95ECAF1B8F8D3C46B6337B1.cache.html)\r\nUnknown.eT(https://xxx/handleStatic/v1449498988v/gwt/opencms/878871B8F95ECAF1B8F8D3C46B6337B1.cache.html)\r\nUnknown.sT(https://xxx/handleStatic/v1449498988v/gwt/opencms/878871B8F95ECAF1B8F8D3C46B6337B1.cache.html)\r\nUnknown.onreadystatechange<(https://xxx/handleStatic/v1449498988v/gwt/opencms/878871B8F95ECAF1B8F8D3C46B6337B1.cache.html)\r\nUnknown.TD(https://xxx/handleStatic/v1449498988v/gwt/opencms/878871B8F95ECAF1B8F8D3C46B6337B1.cache.html)\r\nUnknown.WD(https://xxx/handleStatic/v1449498988v/gwt/opencms/878871B8F95ECAF1B8F8D3C46B6337B1.cache.html)\r\nUnknown.VD/<(https://xxx/handleStatic/v1449498988v/gwt/opencms/878871B8F95ECAF1B8F8D3C46B6337B1.cache.html)\r\nUnknown.anonymous(Unknown)`\r\n\r\nProblem occurs only in Firefox, we tested versions 68 ESR, 75 and 76.0.1\r\n\r\nOther browsers looks OK.",
          "issue_comments": []
        },
        {
          "issue_url": "https://github.com/alkacon/opencms-core/issues/697",
          "issue_title": "Problems with Sortfield.Type.LONG",
          "issue_number": 697,
          "issue_text": "When i'm using SortField.Type.LONG to sort the lists, this isn't having any results. When I switch the value LONG to STRING, the research get some results. Is this a problem with SortField.Type.LONG?\r\n",
          "issue_comments": []
        },
        {
          "issue_url": "https://github.com/alkacon/opencms-core/issues/696",
          "issue_title": "Translation problems in the image editor",
          "issue_number": 696,
          "issue_text": "When I was editing structured content that contains an image gallery, I was able to translate everything to Brazilian Portuguese, except \"title\". How can I translate this item?\r\nSee the image below:\r\n![Alemao](https://user-images.githubusercontent.com/62144960/76628760-04120180-651c-11ea-9c6a-6395d6690264.JPG)\r\n",
          "issue_comments": []
        },
        {
          "issue_url": "https://github.com/alkacon/opencms-core/issues/695",
          "issue_title": "Translation problems on tinymce",
          "issue_number": 695,
          "issue_text": "When I was editing structured content that contains an HtmlWidget, I was able to translate everything to Brazilian Portuguese, except \"Fonts\" and \"System Font\". How can I translate this item? \r\nSee the image below:\r\n\r\n![Fonts](https://user-images.githubusercontent.com/62144960/76627791-67029900-651a-11ea-9379-8b3353e4ec9f.png)\r\n",
          "issue_comments": []
        },
        {
          "issue_url": "https://github.com/alkacon/opencms-core/issues/694",
          "issue_title": "Tomcat startup Failing in \"opencms\"  version 10.5.4 web application , says \"ERROR org.apache.commons.digester.Digester Begin event threw exception\"",
          "issue_number": 694,
          "issue_text": "**** looking for help/guide to resolve the same, Getting the following log trace:**\r\n-----------**\r\n\r\nERROR org.apache.commons.digester.Digester Begin event threw exception\r\njava.lang.ClassNotFoundException: com.abc.opencms.repository.WebdavCmsRepository\r\n\tat org.apache.catalina.loader.WebappClassLoaderBase.loadClass(WebappClassLoaderBase.java:1892)\r\n\tat org.apache.catalina.loader.WebappClassLoaderBase.loadClass(WebappClassLoaderBase.java:1735)\r\n\tat org.apache.commons.digester.ObjectCreateRule.begin(ObjectCreateRule.java:210)\r\n\tat org.apache.commons.digester.Rule.begin(Rule.java:177)\r\n\tat org.apache.commons.digester.Digester.startElement(Digester.java:1583)\r\n\tat org.apache.xerces.parsers.AbstractSAXParser.startElement(Unknown Source)\r\n\tat org.apache.xerces.impl.dtd.XMLDTDValidator.startElement(Unknown Source)\r\n\tat org.apache.xerces.impl.XMLDocumentFragmentScannerImpl.scanStartElement(Unknown Source)\r\n\tat org.apache.xerces.impl.XMLDocumentFragmentScannerImpl$FragmentContentDispatcher.dispatch(Unknown Source)\r\n\tat org.apache.xerces.impl.XMLDocumentFragmentScannerImpl.scanDocument(Unknown Source)\r\n\tat org.apache.xerces.parsers.XML11Configuration.parse(Unknown Source)\r\n\tat org.apache.xerces.parsers.XML11Configuration.parse(Unknown Source)\r\n\tat org.apache.xerces.parsers.XMLParser.parse(Unknown Source)\r\n\tat org.apache.xerces.parsers.AbstractSAXParser.parse(Unknown Source)\r\n\tat org.apache.xerces.jaxp.SAXParserImpl$JAXPSAXParser.parse(Unknown Source)\r\n\tat org.apache.commons.digester.Digester.parse(Digester.java:1892)\r\n\tat org.opencms.configuration.CmsConfigurationManager.loadXmlConfiguration(CmsConfigurationManager.java:657)\r\n\tat org.opencms.configuration.CmsConfigurationManager.loadXmlConfiguration(CmsConfigurationManager.java:379)\r\n\tat org.opencms.main.OpenCmsCore.initConfiguration(OpenCmsCore.java:1189)\r\n\tat org.opencms.main.OpenCmsCore.initContext(OpenCmsCore.java:1541)\r\n\tat org.opencms.main.OpenCmsCore.upgradeRunlevel(OpenCmsCore.java:2097)\r\n\tat org.opencms.main.OpenCmsListener.contextInitialized(OpenCmsListener.java:85)\r\n\tat org.apache.catalina.core.StandardContext.listenerStart(StandardContext.java:5110)\r\n\tat org.apache.catalina.core.StandardContext.startInternal(StandardContext.java:5633)\r\n\tat org.apache.catalina.util.LifecycleBase.start(LifecycleBase.java:145)\r\n\tat org.apache.catalina.core.ContainerBase$StartChild.call(ContainerBase.java:1694)\r\n\tat org.apache.catalina.core.ContainerBase$StartChild.call(ContainerBase.java:1684)\r\n\tat java.util.concurrent.FutureTask.run(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\tat java.lang.Thread.run(Unknown Source)\r\n\r\n--------------------\r\nThe following critical error occurred:\r\nCritical error during OpenCms initialization: Unable to read the OpenCms XML configuration.\r\nGiving up, unable to start OpenCms.\r\n--------------------\r\nERROR org.opencms.main.OpenCmsCore Critical error during OpenCms initialization: Unable to read the OpenCms XML configuration.\r\norg.opencms.main.CmsException: Critical error during OpenCms initialization: Unable to read the OpenCms XML configuration.\r\n\tat org.opencms.main.OpenCmsCore.setErrorCondition(OpenCmsCore.java:370)\r\n\tat org.opencms.main.CmsInitException.setErrorCondition(CmsInitException.java:113)\r\n\tat org.opencms.main.CmsInitException.<init>(CmsInitException.java:84)\r\n\tat org.opencms.main.OpenCmsCore.initConfiguration(OpenCmsCore.java:1191)\r\n\tat org.opencms.main.OpenCmsCore.initContext(OpenCmsCore.java:1541)\r\n\tat org.opencms.main.OpenCmsCore.upgradeRunlevel(OpenCmsCore.java:2097)\r\n\tat org.opencms.main.OpenCmsListener.contextInitialized(OpenCmsListener.java:85)\r\n\tat org.apache.catalina.core.StandardContext.listenerStart(StandardContext.java:5110)\r\n\tat org.apache.catalina.core.StandardContext.startInternal(StandardContext.java:5633)\r\n\tat org.apache.catalina.util.LifecycleBase.start(LifecycleBase.java:145)\r\n\tat org.apache.catalina.core.ContainerBase$StartChild.call(ContainerBase.java:1694)\r\n\tat org.apache.catalina.core.ContainerBase$StartChild.call(ContainerBase.java:1684)\r\n\tat java.util.concurrent.FutureTask.run(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nERROR org.opencms.main.OpenCmsListener Critical error during OpenCms initialization: Unable to read the OpenCms XML configuration.\r\norg.opencms.main.CmsInitException: Critical error during OpenCms initialization: Unable to read the OpenCms XML configuration.\r\n\tat org.opencms.main.OpenCmsCore.initConfiguration(OpenCmsCore.java:1191)\r\n\tat org.opencms.main.OpenCmsCore.initContext(OpenCmsCore.java:1541)\r\n\tat org.opencms.main.OpenCmsCore.upgradeRunlevel(OpenCmsCore.java:2097)\r\n\tat org.opencms.main.OpenCmsListener.contextInitialized(OpenCmsListener.java:85)\r\n\tat org.apache.catalina.core.StandardContext.listenerStart(StandardContext.java:5110)\r\n\tat org.apache.catalina.core.StandardContext.startInternal(StandardContext.java:5633)\r\n\tat org.apache.catalina.util.LifecycleBase.start(LifecycleBase.java:145)\r\n\tat org.apache.catalina.core.ContainerBase$StartChild.call(ContainerBase.java:1694)\r\n\tat org.apache.catalina.core.ContainerBase$StartChild.call(ContainerBase.java:1684)\r\n\tat java.util.concurrent.FutureTask.run(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: org.xml.sax.SAXParseException; lineNumber: 148; columnNumber: 90; Error at line 148 char 90: com.abc.opencms.repository.WebdavCmsRepository\r\n\tat org.apache.commons.digester.Digester.createSAXException(Digester.java:3363)\r\n\tat org.apache.commons.digester.Digester.createSAXException(Digester.java:3389)\r\n\tat org.apache.commons.digester.Digester.startElement(Digester.java:1586)\r\n\tat org.apache.xerces.parsers.AbstractSAXParser.startElement(Unknown Source)\r\n\tat org.apache.xerces.impl.dtd.XMLDTDValidator.startElement(Unknown Source)\r\n\tat org.apache.xerces.impl.XMLDocumentFragmentScannerImpl.scanStartElement(Unknown Source)\r\n\tat org.apache.xerces.impl.XMLDocumentFragmentScannerImpl$FragmentContentDispatcher.dispatch(Unknown Source)\r\n\tat org.apache.xerces.impl.XMLDocumentFragmentScannerImpl.scanDocument(Unknown Source)\r\n\tat org.apache.xerces.parsers.XML11Configuration.parse(Unknown Source)\r\n\tat org.apache.xerces.parsers.XML11Configuration.parse(Unknown Source)\r\n\tat org.apache.xerces.parsers.XMLParser.parse(Unknown Source)\r\n\tat org.apache.xerces.parsers.AbstractSAXParser.parse(Unknown Source)\r\n\tat org.apache.xerces.jaxp.SAXParserImpl$JAXPSAXParser.parse(Unknown Source)\r\n\tat org.apache.commons.digester.Digester.parse(Digester.java:1892)\r\n\tat org.opencms.configuration.CmsConfigurationManager.loadXmlConfiguration(CmsConfigurationManager.java:657)\r\n\tat org.opencms.configuration.CmsConfigurationManager.loadXmlConfiguration(CmsConfigurationManager.java:379)\r\n\tat org.opencms.main.OpenCmsCore.initConfiguration(OpenCmsCore.java:1189)\r\n\t... 12 more\r\nCaused by: java.lang.ClassNotFoundException: com.abc.opencms.repository.WebdavCmsRepository\r\n\tat org.apache.catalina.loader.WebappClassLoaderBase.loadClass(WebappClassLoaderBase.java:1892)\r\n\tat org.apache.catalina.loader.WebappClassLoaderBase.loadClass(WebappClassLoaderBase.java:1735)\r\n\tat org.apache.commons.digester.ObjectCreateRule.begin(ObjectCreateRule.java:210)\r\n\tat org.apache.commons.digester.Rule.begin(Rule.java:177)\r\n\tat org.apache.commons.digester.Digester.startElement(Digester.java:1583)\r\n\t... 26 more",
          "issue_comments": []
        },
        {
          "issue_url": "https://github.com/alkacon/opencms-core/issues/691",
          "issue_title": "xsd schema change - Unable to unmarshal XML content definition schema opencms 11",
          "issue_number": 691,
          "issue_text": "First of all, thank you for the possible help you offer me.\\r\\n\\r\\nThe point is that when I am trying to add a new field in the definition of an .xsd schema (a new resource created by me) the following error appears:\\r\\n\\r\\n`An error occurred. <br /> Unable to unmarshal XML content definition schema \"opencms: //system/modules/com.opencms.elements/schemas/video-FT.xsd\".`\\r\\n\\r\\nIn previous versions of opencms (i.e. Opencms 9.0), there was the option in (Administration View> Content Tools) to restore content schema. And if the new field of the xsd schema didn't affect the others fields, this option allowed to restore the schema of all project content. Solving in this way the error mentioned at the beginning.\\r\\n\\r\\nDo you know if this option to restore content is available in Opencms 11?",
          "issue_comments": []
        },
        {
          "issue_url": "https://github.com/alkacon/opencms-core/issues/690",
          "issue_title": "the export property is not working while using CmsOnDemandStaticExportHandler for V11.0.1",
          "issue_number": 690,
          "issue_text": "For the type of HTML redirect HTML file,  I set the export proerty false. But will export the file still exports and shows exception.  The other export handler will work fine.",
          "issue_comments": [
            {
              "comment_username": "gWestenberger",
              "comment_create_time": "2020-01-14T07:00:50Z",
              "comment_edit_time": "2020-01-14T07:00:50Z",
              "comment_text": "Maybe you forgot to publish the file? The system reads the property in the Online project for static export. "
            }
          ]
        },
        {
          "issue_url": "https://github.com/alkacon/opencms-core/issues/689",
          "issue_title": "the Permission of Direct Publish is not working properly for v11.0.1",
          "issue_number": 689,
          "issue_text": "For example, I have setting permissions of a html file that a user called userA( users group) cannot publish this file.\r\nI used another user account modify the html file.  The userA should not publish the html file, but the truth is the html file is published.\r\nPlease fix it.\r\n",
          "issue_comments": [
            {
              "comment_username": "gWestenberger",
              "comment_create_time": "2020-01-13T08:17:34Z",
              "comment_edit_time": "2020-01-13T08:17:34Z",
              "comment_text": "The Users group is set as the project manager group for the Offline project by default, so this is probably what overrides the direct publish permission - users in a project manager group for a project can always publish stuff in that project. "
            }
          ]
        },
        {
          "issue_url": "https://github.com/alkacon/opencms-core/issues/687",
          "issue_title": "Fix Bugs for macro \"filename\".",
          "issue_number": 687,
          "issue_text": "1, when using %(filename) macros in Schema, it does not work when creating xml files, which is a BUG because when you first try to parse the macro, no new file name string is passed in when you try to resolve the macro.\r\nBecause when you create a file, you set the path to the folder only through cms.getRequestContext().setUri(CmsResource.getParentFolder(resourcename)), not the file name that is passed in.\r\n\r\n\r\nAdd two macros if you need:\r\n2, Add a macro %(currentdate) to represent the time at 0 o'clock on the day, because sometimes the time required is not or does not need to be very precise, at this time if you use %(currenttime) directly, but also manually adjust the time.\r\n\r\n3, add a macro %(filename.base), allowing the basic file name of the xml file (without suffix), which can be used as a property in xml, for example, use as friendly-url-name.\r\n\r\nThe code change is here : https://github.com/Yuanuo/opencms-core/commit/dceb063a09aa3b300fca792367862cc0a00ee1ec and https://github.com/Yuanuo/opencms-core/commit/14206f0abf8e52403f31952f2f00e549cedf7764",
          "issue_comments": []
        },
        {
          "issue_url": "https://github.com/alkacon/opencms-core/issues/686",
          "issue_title": "Fixes a few issues about FilterQuery in listconfig,",
          "issue_number": 686,
          "issue_text": "1, if FilterQuery (such as q) is configured in the Listconfig, it will not be replaced with a real search string if it contains %(query).\\r\\n\\r\\n2, The new Listconfig is forced to specify Locale as en, and the configuration of en is also read first in hard-coded, and if you want to use a List configuration in a multilingual environment, it always read en's configuration (although there may be other implementation methods, such as creating multiple List configurations). So add a macro \"%(locale)\" and you can configure it in this way with q={!qf=\"content_%(locale)\"}%(query).\\r\\n\\r\\nMade such a code change: https://github.com/Yuanuo/opencms-core/commit/9d7513c6340ab739730e10b53fb8d2d4667122d0",
          "issue_comments": []
        },
        {
          "issue_url": "https://github.com/alkacon/opencms-core/issues/685",
          "issue_title": "HTTP authentication BASIC-realm should with meaningful info.",
          "issue_number": 685,
          "issue_text": "You should return a meaningful name instead of the default getOpenCmsContext(), because this method always returns \"/opencms\". This is a really bad experience for users.\r\n\r\nI submitted a code change, suggested,\r\nhttps://github.com/Yuanuo/opencms-core/commit/c6bee343bc23b32516abbc98bb1e1ba90dcecf8c",
          "issue_comments": []
        },
        {
          "issue_url": "https://github.com/alkacon/opencms-core/issues/683",
          "issue_title": "Fix typo - authentication instead of authentification",
          "issue_number": 683,
          "issue_text": "Authentification is not a real word :)\r\n\r\nThis PR only affects a label, but in general we should also consider refactoring some classes to also have a proper name.",
          "issue_comments": []
        },
        {
          "issue_url": "https://github.com/alkacon/opencms-core/issues/681",
          "issue_title": "Where storeds Property Name's i18n data?",
          "issue_number": 681,
          "issue_text": "For 11.0.1\r\nIn the Property Editor, there are a lot of properties, such as Title, NavText and so on. I don't find the way to i18n those property names. So It is possible to i18n those fields ?",
          "issue_comments": []
        },
        {
          "issue_url": "https://github.com/alkacon/opencms-core/issues/680",
          "issue_title": "A List bug for Chinses files. ",
          "issue_number": 680,
          "issue_text": "I am using 11.0.1 with the Mercury demo.\\r\\n\\r\\nI have added zh in the opencms-system.xml.\\r\\n` \\r\\n<localesconfigured> \\r\\n                <locale>zh</locale>\\r\\n                <locale>en</locale>\\r\\n </localesconfigured>\\r\\n`\\r\\n\\r\\nIn the List app, view Downloads(list_00012.xml).  Display the result. And Change the Locale to Chinese. \\r\\n\\r\\nAn error occured. Say \"Query Field 'content_zh' is not a valid field name\".\\r\\n\\r\\nThe detail Message is below.\\r\\n\\r\\n> Query “q={!tag=q type=edismax qf=\"content_zh Title_prop Description_dprop Description.html_dprop spell\"}*&fl=path,instancedate_zh_dt,instancedateend_zh_dt,instancedatecurrenttill_zh_dt,id,solr_id,disptitle_zh_sort,link,type&qt=edismax&rows=400&fq=released:[* TO *]&fq=expired:[* TO *]&fq=parent-folders:(\"/sites/default/mercury-demo/.galleries/Downloads/\")&fq=type:(\"binary\" OR \"image\" OR \"plain\")&fq=search_exclude:\"false\"&TZ=Asia/Shanghai&start=0&sort=disptitle_zh_sort asc&facet=true&facet.field={!key=parent-folders facet.mincount=1 facet.limit=200 facet.sort=index ex=parent-folders,category_exact,instancedate,q}parent-folders&facet.field={!key=category_exact facet.mincount=1 facet.limit=200 facet.sort=index ex=parent-folders,category_exact,instancedate,q}category_exact&facet.range={!key=instancedate facet.range.start=NOW/YEAR-20YEARS facet.range.end=NOW/MONTH 2YEARS facet.range.gap= 1MONTHS facet.range.hardend=false facet.mincount=1 ex=parent-folders,category_exact,instancedate,q}instancedate_zh_dt” failed。\\r\\n\\r\\n>org.opencms.search.CmsSearchException: query “q={!tag=q type=edismax qf=\"content_zh Title_prop Description_dprop Description.html_dprop spell\"}*&fl=path,instancedate_zh_dt,instancedateend_zh_dt,instancedatecurrenttill_zh_dt,id,solr_id,disptitle_zh_sort,link,type&qt=edismax&rows=400&fq=released:[* TO *]&fq=expired:[* TO *]&fq=parent-folders:(\"/sites/default/mercury-demo/.galleries/Downloads/\")&fq=type:(\"binary\" OR \"image\" OR \"plain\")&fq=search_exclude:\"false\"&TZ=Asia/Shanghai&start=0&sort=disptitle_zh_sort asc&facet=true&facet.field={!key=parent-folders facet.mincount=1 facet.limit=200 facet.sort=index ex=parent-folders,category_exact,instancedate,q}parent-folders&facet.field={!key=category_exact facet.mincount=1 facet.limit=200 facet.sort=index ex=parent-folders,category_exact,instancedate,q}category_exact&facet.range={!key=instancedate facet.range.start=NOW/YEAR-20YEARS facet.range.end=NOW/MONTH 2YEARS facet.range.gap= 1MONTHS facet.range.hardend=false facet.mincount=1 ex=parent-folders,category_exact,instancedate,q}instancedate_zh_dt” Failed。\\r\\n\\tat org.opencms.search.solr.CmsSolrIndex.search(CmsSolrIndex.java:1265)\\r\\n\\tat org.opencms.search.solr.CmsSolrIndex.search(CmsSolrIndex.java:800)\\r\\n\\tat org.opencms.search.solr.CmsSolrIndex.search(CmsSolrIndex.java:772)\\r\\n\\tat org.opencms.ui.apps.lists.CmsListManager.executeSearch(CmsListManager.java:2261)\\r\\n\\tat org.opencms.ui.apps.lists.CmsListManager.search(CmsListManager.java:1744)\\r\\n\\tat org.opencms.ui.apps.lists.CmsListManager.changeContentLocale(CmsListManager.java:1843)\\r\\n\\tat org.opencms.ui.apps.lists.CmsListManager$11.valueChange(CmsListManager.java:1533)\\r\\n\\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\\r\\n\\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\\r\\n\\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\\r\\n\\tat java.lang.reflect.Method.invoke(Unknown Source)\\r\\n\\tat com.vaadin.event.ListenerMethod.receiveEvent(ListenerMethod.java:499)\\r\\n\\tat com.vaadin.event.EventRouter.fireEvent(EventRouter.java:273)\\r\\n\\tat com.vaadin.event.EventRouter.fireEvent(EventRouter.java:237)\\r\\n\\tat com.vaadin.server.AbstractClientConnector.fireEvent(AbstractClientConnector.java:1014)\\r\\n\\tat com.vaadin.v7.ui.AbstractField.fireValueChange(AbstractField.java:1134)\\r\\n\\tat com.vaadin.v7.ui.AbstractField.setValue(AbstractField.java:545)\\r\\n\\tat com.vaadin.v7.ui.AbstractSelect.setValue(AbstractSelect.java:742)\\r\\n\\tat com.vaadin.v7.ui.AbstractField.setValue(AbstractField.java:442)\\r\\n\\tat com.vaadin.v7.ui.ComboBox.changeVariables(ComboBox.java:743)\\r\\n\\tat com.vaadin.server.communication.ServerRpcHandler.changeVariables(ServerRpcHandler.java:625)\\r\\n\\tat com.vaadin.server.communication.ServerRpcHandler.handleInvocation(ServerRpcHandler.java:471)\\r\\n\\tat com.vaadin.server.communication.ServerRpcHandler.handleInvocations(ServerRpcHandler.java:414)\\r\\n\\tat com.vaadin.server.communication.ServerRpcHandler.handleRpc(ServerRpcHandler.java:274)\\r\\n\\tat com.vaadin.server.communication.UidlRequestHandler.synchronizedHandleRequest(UidlRequestHandler.java:90)\\r\\n\\tat com.vaadin.server.SynchronizedRequestHandler.handleRequest(SynchronizedRequestHandler.java:40)\\r\\n\\tat com.vaadin.server.VaadinService.handleRequest(VaadinService.java:1602)\\r\\n\\tat com.vaadin.server.VaadinServlet.service(VaadinServlet.java:445)\\r\\n\\tat org.opencms.main.CmsUIServlet.service(CmsUIServlet.java:346)\\r\\n\\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:741)\\r\\n\\tat org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:231)\\r\\n\\tat org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:166)\\r\\n\\tat org.apache.tomcat.websocket.server.WsFilter.doFilter(WsFilter.java:52)\\r\\n\\tat org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:193)\\r\\n\\tat org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:166)\\r\\n\\tat org.opencms.main.OpenCmsUrlServletFilter.doFilter(OpenCmsUrlServletFilter.java:132)\\r\\n\\tat org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:193)\\r\\n\\tat org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:166)\\r\\n\\tat org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:199)\\r\\n\\tat org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:96)\\r\\n\\tat org.apache.catalina.authenticator.AuthenticatorBase.invoke(AuthenticatorBase.java:528)\\r\\n\\tat org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:139)\\r\\n\\tat org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:81)\\r\\n\\tat org.apache.catalina.valves.AbstractAccessLogValve.invoke(AbstractAccessLogValve.java:678)\\r\\n\\tat org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:87)\\r\\n\\tat org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:343)\\r\\n\\tat org.apache.coyote.http11.Http11Processor.service(Http11Processor.java:798)\\r\\n\\tat org.apache.coyote.AbstractProcessorLight.process(AbstractProcessorLight.java:66)\\r\\n\\tat org.apache.coyote.AbstractProtocol$ConnectionHandler.process(AbstractProtocol.java:810)\\r\\n\\tat org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.doRun(NioEndpoint.java:1498)\\r\\n\\tat org.apache.tomcat.util.net.SocketProcessorBase.run(SocketProcessorBase.java:49)\\r\\n\\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\\r\\n\\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\\r\\n\\tat org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run(TaskThread.java:61)\\r\\n\\tat java.lang.Thread.run(Unknown Source)\\r\\nCaused by: org.apache.solr.common.SolrException: org.apache.solr.search.SyntaxError: Query Field 'content_zh' is not a valid field name\\r\\n\\tat org.apache.solr.handler.component.QueryComponent.prepare(QueryComponent.java:216)\\r\\n\\tat org.apache.solr.handler.component.SearchHandler.handleRequestBody(SearchHandler.java:272)\\r\\n\\tat org.apache.solr.handler.RequestHandlerBase.handleRequest(RequestHandlerBase.java:199)\\r\\n\\tat org.apache.solr.core.SolrCore.execute(SolrCore.java:2559)\\r\\n\\tat org.apache.solr.client.solrj.embedded.EmbeddedSolrServer.request(EmbeddedSolrServer.java:191)\\r\\n\\tat org.apache.solr.client.solrj.SolrRequest.process(SolrRequest.java:207)\\r\\n\\tat org.apache.solr.client.solrj.SolrClient.query(SolrClient.java:987)\\r\\n\\tat org.apache.solr.client.solrj.SolrClient.query(SolrClient.java:1002)\\r\\n\\tat org.opencms.search.solr.CmsSolrIndex.search(CmsSolrIndex.java:996)\\r\\n\\t... 54 more\\r\\nCaused by: org.apache.solr.search.SyntaxError: Query Field 'content_zh' is not a valid field name\\r\\n\\tat org.apache.solr.search.ExtendedDismaxQParser.checkFieldInSchema(ExtendedDismaxQParser.java:275)\\r\\n\\tat org.apache.solr.search.ExtendedDismaxQParser.checkFieldsInSchema(ExtendedDismaxQParser.java:287)\\r\\n\\tat org.apache.solr.search.ExtendedDismaxQParser.validateQueryFields(ExtendedDismaxQParser.java:226)\\r\\n\\tat org.apache.solr.search.ExtendedDismaxQParser.parse(ExtendedDismaxQParser.java:151)\\r\\n\\tat org.apache.solr.search.QParser.getQuery(QParser.java:173)\\r\\n\\tat org.apache.solr.handler.component.QueryComponent.prepare(QueryComponent.java:158)\\r\\n\\t... 62 more\\r\\nCaused by: org.apache.solr.common.SolrException: undefined field: \"content_zh\"\\r\\n\\tat org.apache.solr.schema.IndexSchema.getField(IndexSchema.java:1234)\\r\\n\\tat org.apache.solr.search.ExtendedDismaxQParser.checkFieldInSchema(ExtendedDismaxQParser.java:273)\\r\\n\\t... 67 more\\r\\n",
          "issue_comments": []
        },
        {
          "issue_url": "https://github.com/alkacon/opencms-core/issues/679",
          "issue_title": "i18n text missing for module",
          "issue_number": 679,
          "issue_text": "for the file **org\\opencms\\ui\\apps\\modules\\CmsModuleTable.java**\r\n\r\nchange code\r\n`m_counter.setWindowCaption(\"Module statistics\");\r\n m_counter.setDescription(\"Module statistics\");`\r\n\r\nto\r\n`m_counter.setWindowCaption( CmsVaadinUtils.getMessageText(Messages.GUI_MODULES_STATISTICS_0));\r\n m_counter.setDescription( CmsVaadinUtils.getMessageText(Messages.GUI_MODULES_STATISTICS_0));`\r\n",
          "issue_comments": []
        },
        {
          "issue_url": "https://github.com/alkacon/opencms-core/issues/678",
          "issue_title": "i18n properties missing in messages.properties",
          "issue_number": 678,
          "issue_text": "For version 11.0.1\r\norg\\opencms\\xml\\containerpage\\messages.properties\r\n\r\n5 Properties are missing for the XSD file.\r\n\r\n```\r\nlabel.FormatterSetting.Visibility=Visibility\r\nlabel.DynamicFunctionSettingConfig.Visibility=Visibility\r\nlabel.ADEField.Visibility=Visibility\r\nlabel.ADEModuleField.Visibility=Visibility\r\nlabel.NewFormatter.Parameter=Parameter\r\n```",
          "issue_comments": []
        },
        {
          "issue_url": "https://github.com/alkacon/opencms-core/issues/677",
          "issue_title": "Undo changes not working properly",
          "issue_number": 677,
          "issue_text": "I am using 11.0.1 with Mercury Templates.\r\n\r\nI have dragged a m-section in a page. After adding some text in it, I publish it.\r\n\r\nFor Scene 1:\r\nI change the text.  The \"Undo changes\" can not be activate.\r\n\r\nFor Scene 2:\r\nI delete the m-section which is the first one I have created. The \"Undo changes\" can not undo the deleted element.",
          "issue_comments": [
            {
              "comment_username": "tHerrmann",
              "comment_create_time": "2019-10-07T04:52:43Z",
              "comment_edit_time": "2019-10-07T04:52:43Z",
              "comment_text": "The 'Undo changes' action will only undo the changes on the selected resource. So for Scene 1 you need to call it for the m-section element and not for the containerpage including the element. You can do this either  by opening the info dialog for the element and accessing the context menu from there or from the file explorer.\r\nIn Scene 2 you would need to undelete the m-section element first and apply 'Undo changes' on the containerpage afterwards."
            },
            {
              "comment_username": "veggie4ever",
              "comment_create_time": "2019-10-07T12:58:51Z",
              "comment_edit_time": "2019-10-07T12:59:06Z",
              "comment_text": "Hi Tobias,\r\n\r\nI understand what you are saying.\r\nFunnily one of our customer asked the same question last week. \r\nDo you think this behaviour is intuitive? As a user I would expect to be able to undo the changes I made on this page. The question would be, how to implement this. Especially, when more elements on a page are changed. I would like to see that as feature request for an upcoming version.\r\nBest regards\r\nKai"
            }
          ]
        },
        {
          "issue_url": "https://github.com/alkacon/opencms-core/issues/676",
          "issue_title": "one i18n text missing",
          "issue_number": 676,
          "issue_text": "In the file CmsGalleryDialog.java\r\n\r\nThe text is hard coded.\r\n`m_tabbedPanel.add(m_sitemapTab, \"Sitemap\");`\r\n\r\nTo fix it, change the code to\r\n`m_tabbedPanel.add(m_sitemapTab, Messages.get().key(Messages.GUI_TAB_TITLE_SITEMAP_0));`",
          "issue_comments": []
        },
        {
          "issue_url": "https://github.com/alkacon/opencms-core/issues/675",
          "issue_title": "User preferences language is different from opencms-system.xml",
          "issue_number": 675,
          "issue_text": "For 11.0.1 the Locale can be defined in the opencms-system.xml, But the User preferences language\r\nis different. It still use the old way.\r\n\r\nSo it can be fix by changing the code in src/org/opencms/workplace/CmsWorkplaceManager.java\r\n\r\nchange \r\n` private List<Locale> initWorkplaceLocales(CmsObject cms) {    **** }`\r\n\r\nto\r\n `\r\nprivate List<Locale> initWorkplaceLocales(CmsObject cms) { \r\n       ArrayList<Locale> result = new ArrayList<>(OpenCms.getLocaleManager().getAvailableLocales());\r\n        result.sort(CmsLocaleComparator.getComparator());\r\n        return result;\r\n}\r\n`",
          "issue_comments": [
            {
              "comment_username": "tHerrmann",
              "comment_create_time": "2019-10-07T04:40:43Z",
              "comment_edit_time": "2019-10-07T04:40:43Z",
              "comment_text": "The locales defined in the opencms-system.xml are not related to the locales available as workplace locales in the user preferences. The first are used for the contents of the websites you manage with OpenCms the later is the user interface language for your editor users.\r\nIn case you want to restrict the available workplace locales, you can remove the JARs org.opencms.locale.[locale] from the WEB-INF/lib folder in the real file system and restart your servlet-container. The english locale can not be removed, it is always available."
            },
            {
              "comment_username": "arrowwind",
              "comment_create_time": "2019-10-07T13:09:42Z",
              "comment_edit_time": "2019-10-07T13:09:42Z",
              "comment_text": "Yes, you are right. I agree that the contents locale is different from the editor interface. \r\n\r\nNow there is english and other 6 locales right now which the end user can choose in the user perferences. When the admin user adds a new user or edits the exist user, the languages the admin user can choose the ones which exist in the opencms-system.xml. For the end user view, that is confusing. I can choose 6 language in my user preferences but only 2 in the User Management.  And if all of them are update to date that will be fine, for en and de they are fully translated for others I don't think so.\r\n\r\nAs the website developer,  to deliver the untranslated content is not a professional way. So before deliver the whole thing to the end user, he will delete the unused locales if he knows to delete the jar in the lib folder or to debug the whole program to find out how to do it. Why not make the two settings the same. So the new opencms developer will not consider this thing.\r\n\r\nThanks for reading."
            }
          ]
        },
        {
          "issue_url": "https://github.com/alkacon/opencms-core/issues/673",
          "issue_title": "On Creating New Resource Types",
          "issue_number": 673,
          "issue_text": "Create a custom file type, the file can be created in the background, but the page display is disabled, the container is set to element, what is the problem?\r\n\r\n![@G9M4$VET(FAS F X4U9S_4](https://user-images.githubusercontent.com/26370760/65816560-bb95ae80-e22f-11e9-8754-67dd37b2e79a.png)\r\n![GW)3L_F{)V`G040P4 {MX2](https://user-images.githubusercontent.com/26370760/65816561-bb95ae80-e22f-11e9-961f-8b8be5365847.png)\r\n![4FR6SBD I)27FY_)HV_FZ0R](https://user-images.githubusercontent.com/26370760/65816562-bc2e4500-e22f-11e9-96e6-e7e2351727fc.png)\r\n![RJJECH80P{C5)C8TW9J6Z%T](https://user-images.githubusercontent.com/26370760/65816563-bd5f7200-e22f-11e9-84de-f4afe443b227.png)\r\n![VDIK_~3Z)79CLR8O RN67P](https://user-images.githubusercontent.com/26370760/65816565-bdf80880-e22f-11e9-9302-4af819c8513c.png)\r\n",
          "issue_comments": []
        },
        {
          "issue_url": "https://github.com/alkacon/opencms-core/issues/672",
          "issue_title": "3 texts are missing  i18n.",
          "issue_number": 672,
          "issue_text": "**The First:**\r\norg/opencms/ui/apps/sessions/CmsUserInfoDialog.java\r\n`res.add(\"\");`\r\n`res.add(\"<p>Session info:</p>\"); `\r\n\r\n**The Second**\r\norg/opencms/ui/apps/search/CmsSourceSearchForm.html\r\n`<vaadin7-combo-box _id=\"m_siteSelect\" immediate=\"true\" width=\"100%\" caption=\"Site\" />`\r\n\r\n**The Third**\r\norg/opencms/ade/sitemap/CmsVfsSitemapService.java\r\n` defaultPageInfo = new CmsNewResourceInfo(\r\n                modelResource.getTypeId(),\r\n                CmsADEManager.DEFAULT_DETAILPAGE_TYPE,\r\n                \"Default\",\r\n                \"The default detail page will be used to display detail contents or functions.\",\r\n                modelResource.getStructureId(),\r\n                false,\r\n                \"The default detail page will be used to display detail contents or functions.\");\r\n`\r\n",
          "issue_comments": [
            {
              "comment_username": "arrowwind",
              "comment_create_time": "2019-09-26T06:20:24Z",
              "comment_edit_time": "2019-10-05T15:23:40Z",
              "comment_text": "These can be fixed by the below\r\n\r\n**The First**\r\norg/opencms/ui/apps/sessions/CmsUserInfoDialog.java\r\nchange to:\r\n`res.add(\"<p>\"+ CmsVaadinUtils.getMessageText(Messages.GUI_MESSAGES_SESSION_INFO_0)+\"</p>\");`\r\n\r\norg\\opencms\\ui\\apps\\messages.properties\r\nadd:\r\n`GUI_MESSAGES_SESSION_INFO_0\t\t\t\t\t\t    =Session info:`\r\n\r\norg\\opencms\\ui\\apps\\Messages.java\r\nadd:\r\n`public static final String GUI_MESSAGES_SESSION_INFO_0 = \"GUI_MESSAGES_SESSION_INFO_0\";`\r\n\r\n**The Second**\r\norg/opencms/ui/apps/search/CmsSourceSearchForm.html\r\nchange to:\r\n`<vaadin7-combo-box _id=\"m_siteSelect\" immediate=\"true\" width=\"100%\" caption=\"%(key.GUI_SOURCESEARCH_SITE_0)\" />`\r\n\r\norg\\opencms\\ui\\apps\\messages.properties\r\nadd:\r\n`GUI_SOURCESEARCH_SITE_0=Site`\r\n\r\norg\\opencms\\ui\\apps\\Messages.java\r\nadd:\r\n`public static final String GUI_SOURCESEARCH_SITE_0= \"GUI_SOURCESEARCH_SITE_0\";`\r\n\r\n**The Third**\r\norg/opencms/ade/sitemap/CmsVfsSitemapService.java\r\nchange\r\n`      if (modelResource != null) {\r\n            defaultPageInfo = new CmsNewResourceInfo(\r\n                modelResource.getTypeId(),\r\n                CmsADEManager.DEFAULT_DETAILPAGE_TYPE,\r\n                \"Default\",\r\n                \"The default detail page will be used to display detail contents or functions.\",\r\n                modelResource.getStructureId(),\r\n                false,\r\n                \"The default detail page will be used to display detail contents or functions.\");\r\n        } else {\r\n            defaultPageInfo = new CmsNewResourceInfo(\r\n                CmsResourceTypeXmlContainerPage.getContainerPageTypeIdSafely(),\r\n                CmsADEManager.DEFAULT_DETAILPAGE_TYPE,\r\n                \"Default\",\r\n                \"The default detail page will be used to display detail contents or functions.\",\r\n                null,\r\n                false,\r\n                \"The default detail page will be used to display detail contents or functions.\");\r\n        }`\r\n\r\nto\r\n`      if (modelResource != null) {\r\n            defaultPageInfo = new CmsNewResourceInfo(\r\n                modelResource.getTypeId(),\r\n                CmsADEManager.DEFAULT_DETAILPAGE_TYPE, \r\n  \r\n Messages.get().getBundle(getWorkplaceLocale()).key(Messages.GUI_DEFAULT_DETAIL_PAGE_TITLE_0),                  \r\n \r\n Messages.get().getBundle(getWorkplaceLocale()).key(Messages.GUI_DEFAULT_DETAIL_PAGE_SUBTITLE_0),\r\n                modelResource.getStructureId(),\r\n                false,                    Messages.get().getBundle(getWorkplaceLocale()).key(Messages.GUI_DEFAULT_DETAIL_PAGE_SUBTITLE_0));\r\n        } else {\r\n            defaultPageInfo = new CmsNewResourceInfo(\r\n                CmsResourceTypeXmlContainerPage.getContainerPageTypeIdSafely(),\r\n                CmsADEManager.DEFAULT_DETAILPAGE_TYPE,                    Messages.get().getBundle(getWorkplaceLocale()).key(Messages.GUI_DEFAULT_DETAIL_PAGE_TITLE_0),\r\n                   Messages.get().getBundle(getWorkplaceLocale()).key(Messages.GUI_DEFAULT_DETAIL_PAGE_SUBTITLE_0),\r\n                null,\r\n                false,                    Messages.get().getBundle(getWorkplaceLocale()).key(Messages.GUI_DEFAULT_DETAIL_PAGE_SUBTITLE_0));\r\n        }`\r\n\r\nMessages.java\r\nadd\r\n` public static final String GUI_DEFAULT_DETAIL_PAGE_TITLE_0 = \"GUI_DEFAULT_DETAIL_PAGE_TITLE_0\";\r\npublic static final String GUI_DEFAULT_DETAIL_PAGE_SUBTITLE_0 = \"GUI_DEFAULT_DETAIL_PAGE_SUBTITLE_0\";\r\n`\r\nmessages.properties\r\nadd\r\n`GUI_DEFAULT_DETAIL_PAGE_TITLE_0=Default\r\nGUI_DEFAULT_DETAIL_PAGE_SUBTITLE_0=The default detail page will be used to display detail contents or functions.`"
            }
          ]
        },
        {
          "issue_url": "https://github.com/alkacon/opencms-core/issues/671",
          "issue_title": "How can I add a new group item admin menu in workpalce ?",
          "issue_number": 671,
          "issue_text": "Current, I'm using OpenCMS 9.5. And I want to add a new group item admin menu in workplace (\"Help\" adn \"Administrations\")\\r\\nDoes anyone know how to do that? \\r\\n\\r\\n![image](https://user-images.githubusercontent.com/34368848/65594638-694d5700-dfbd-11e9-859f-4bd1d3a1346e.png)\\r\\n",
          "issue_comments": [
            {
              "comment_username": "nguyendung94dev",
              "comment_create_time": "2020-01-14T09:29:27Z",
              "comment_edit_time": "2020-01-14T09:30:39Z",
              "comment_text": "I resolved it. Someone can delete this issue for me @aKandzior Kandzior  @tHerrmann because it always present on my dashboard github"
            }
          ]
        },
        {
          "issue_url": "https://github.com/alkacon/opencms-core/issues/670",
          "issue_title": "Errors when creating/editing some VFS configuration files",
          "issue_number": 670,
          "issue_text": "Hello,\\r\\nwe found some errors in log file when we create following vfs configuration files:\\r\\n- Fomatter configuration (type: formatter_config, id: 308)\\r\\n- Property resource bundle (type: propertyvfsbundle, id: 307)\\r\\n\\r\\nIt's not a big problem because it's working (probably) and older \"property resource bundle\" is not widely used anymore.\\r\\n\\r\\nOpenCms 11.0.1 (upgraded from OpenCms 11.0.0).\\r\\n\\r\\n**Formatter configuration**\\r\\n`10 Sep 2019 11:02:02,094 ERROR [CmsFormatterConfigurationCache: 350] Error while trying to read formatter configuration /sites/default/new_formatter_config:    Neither container types nor c\\r\\nontainer widths defined!\\r\\norg.opencms.ade.configuration.formatters.CmsFormatterBeanParser$ParseException: Neither container types nor container widths defined!\\r\\n        at org.opencms.ade.configuration.formatters.CmsFormatterBeanParser.parseMatch(CmsFormatterBeanParser.java:773) ~[opencms.jar:11.0.1]\\r\\n        at org.opencms.ade.configuration.formatters.CmsFormatterBeanParser.parse(CmsFormatterBeanParser.java:464) ~[opencms.jar:11.0.1]\\r\\n        at org.opencms.ade.configuration.formatters.CmsFormatterConfigurationCache.readFormatter(CmsFormatterConfigurationCache.java:343) [opencms.jar:11.0.1]\\r\\n        at org.opencms.ade.configuration.formatters.CmsFormatterConfigurationCache.performUpdate(CmsFormatterConfigurationCache.java:217) [opencms.jar:11.0.1]\\r\\n        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515) [?:?]\\r\\n        at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305) [?:?]\\r\\n        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305) [?:?]\\r\\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]\\r\\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]\\r\\n        at java.lang.Thread.run(Thread.java:834) [?:?]`\\r\\n\\r\\nProblem occurs :\\r\\n- when i create new file\\r\\n- when i'll add container type/width to the existing document, than save, edit, remove and again save document\\r\\n\\r\\n**Property resource bundle**\\r\\n`10 Sep 2019 11:02:15,167 ERROR [le.CmsMessageBundleEditorTypes: 990] Searching for the bundle descriptor failed.\\r\\norg.opencms.search.CmsSearchException: Execution of query \"q=*:*&fl=*,score&fl=path&qt=edismax&rows=10&fq=filename:\"new_propertyvfsbundle_desc\"\" failed.\\r\\n        at org.opencms.search.solr.CmsSolrIndex.search(CmsSolrIndex.java:1265) ~[opencms.jar:11.0.1]\\r\\n        at org.opencms.search.solr.CmsSolrIndex.search(CmsSolrIndex.java:800) ~[opencms.jar:11.0.1]\\r\\n        at org.opencms.ui.editors.messagebundle.CmsMessageBundleEditorTypes.getDescriptor(CmsMessageBundleEditorTypes.java:988) [opencms.jar:11.0.1]\\r\\n        at org.opencms.ui.editors.messagebundle.CmsMessageBundleEditorModel.initDescriptor(CmsMessageBundleEditorModel.java:1332) [opencms.jar:11.0.1]\\r\\n        at org.opencms.ui.editors.messagebundle.CmsMessageBundleEditorModel.<init>(CmsMessageBundleEditorModel.java:546) [opencms.jar:11.0.1]\\r\\n        at org.opencms.ui.editors.messagebundle.CmsMessageBundleEditor.initUI(CmsMessageBundleEditor.java:403) [opencms.jar:11.0.1]\\r\\n        at org.opencms.ui.apps.CmsEditor.onStateChange(CmsEditor.java:295) [opencms.jar:11.0.1]\\r\\n        at org.opencms.ui.apps.CmsAppView.enter(CmsAppView.java:240) [opencms.jar:11.0.1]\\r\\n        at org.opencms.ui.apps.CmsAppView.enter(CmsAppView.java:253) [opencms.jar:11.0.1]`\\r\\n\\r\\nProblem occurs:\\r\\n- when i edit file",
          "issue_comments": []
        },
        {
          "issue_url": "https://github.com/alkacon/opencms-core/issues/667",
          "issue_title": "During login, username and passwords are sent via GET method",
          "issue_number": 667,
          "issue_text": "Standard procedure should be to send them via POST",
          "issue_comments": []
        },
        {
          "issue_url": "https://github.com/alkacon/opencms-core/issues/665",
          "issue_title": "Gradle - unavailable dependencies when building OpenCms 11",
          "issue_number": 665,
          "issue_text": "Hello,\r\nI have a problem with building OCMS 11. When I try to build it with\r\n`./gradlew war`\r\nfrom the branch 'branch_11_0_x' I always get a message about missing some dependencies:\r\n```\r\n> Task :compileJava FAILED\r\n\r\nFAILURE: Build failed with an exception.\r\n\r\n* What went wrong:\r\nCould not resolve all files for configuration ':compileClasspath'.\r\n> Could not resolve org.jpedal.jpedal-lgpl:jpedal-lgpl:4.92b23.\r\n  Required by:\r\n      project :\r\n   > Could not resolve org.jpedal.jpedal-lgpl:jpedal-lgpl:4.92b23.\r\n      > Could not get resource 'http://software.rescarta.org/nexus/content/repositories/thirdparty/org/jpedal/jpedal-lgpl/jpedal-lgpl/4.92b23/jpedal-lgpl-4.92b23.pom'.\r\n         > Could not GET 'http://software.rescarta.org/nexus/content/repositories/thirdparty/org/jpedal/jpedal-lgpl/jpedal-lgpl/4.92b23/jpedal-lgpl-4.92b23.pom'.\r\n            > Connect to software.rescarta.org:80 [software.rescarta.org/192.159.83.79] failed: connect timed out\r\n> Could not resolve org.tepi.filtertable:filteringtable:1.0.1.v8.\r\n  Required by:\r\n      project :\r\n   > Could not resolve org.tepi.filtertable:filteringtable:1.0.1.v8.\r\n      > Could not get resource 'http://software.rescarta.org/nexus/content/repositories/thirdparty/org/tepi/filtertable/filteringtable/1.0.1.v8/filteringtable-1.0.1.v8.pom'.\r\n         > Could not HEAD 'http://software.rescarta.org/nexus/content/repositories/thirdparty/org/tepi/filtertable/filteringtable/1.0.1.v8/filteringtable-1.0.1.v8.pom'.\r\n            > Connect to software.rescarta.org:80 [software.rescarta.org/192.159.83.79] failed: connect timed out\r\n> Could not resolve org.vaadin.addons:popupbutton:3.0.0.\r\n  Required by:\r\n      project :\r\n   > Could not resolve org.vaadin.addons:popupbutton:3.0.0.\r\n      > Could not get resource 'http://software.rescarta.org/nexus/content/repositories/thirdparty/org/vaadin/addons/popupbutton/3.0.0/popupbutton-3.0.0.pom'.\r\n         > Could not HEAD 'http://software.rescarta.org/nexus/content/repositories/thirdparty/org/vaadin/addons/popupbutton/3.0.0/popupbutton-3.0.0.pom'.\r\n            > Connect to software.rescarta.org:80 [software.rescarta.org/192.159.83.79] failed: connect timed out\r\n```\r\nIt looks like whatever was located in 'http://software.rescarta.org' is now not available (at least for me).\r\n\r\nIs there anything I can do to be able to build the WAR without these dependencies?",
          "issue_comments": [
            {
              "comment_username": "gWestenberger",
              "comment_create_time": "2019-08-28T06:37:55Z",
              "comment_edit_time": "2019-08-28T06:37:55Z",
              "comment_text": "This seems to have been a temporary error on the Maven repository side. Try the --refresh-dependencies option for gradle."
            },
            {
              "comment_username": "Morwud",
              "comment_create_time": "2019-08-28T08:01:43Z",
              "comment_edit_time": "2019-08-28T08:01:43Z",
              "comment_text": "Tried:\r\n- running with --refresh-dependencies\r\n- clean my local cache with rm -rf $HOME/.gradle/caches/\r\n- clean .m2/.repository folder\r\n\r\nUnfortunately nothing helped. I was hoping to get the libs from another resource and add them locally to get the build working but I can't find any other source of the jpedal lib."
            },
            {
              "comment_username": "Yuanuo",
              "comment_create_time": "2019-08-30T02:32:06Z",
              "comment_edit_time": "2019-08-30T02:32:06Z",
              "comment_text": "I had the same problem with jpedal lib.\r\nThen I used this version to solve the problem:（in the **dependencies.gradle** file）\r\n\r\n    // distribution group: 'org.jpedal.jpedal-lgpl', name: 'jpedal-lgpl', version: '4.92b23'\r\n    // https://mvnrepository.com/artifact/org.jpedal/jpedal-lgpl\r\n    distribution group: 'org.jpedal', name: 'jpedal-lgpl', version: '4.74b27'"
            },
            {
              "comment_username": "tHerrmann",
              "comment_create_time": "2019-08-30T07:38:31Z",
              "comment_edit_time": "2019-08-30T07:38:31Z",
              "comment_text": "You could also use the JAR file from the OpenCms distribution and reference it from the dependencies.gradle file.\r\nCan you access the repository server at all? Is it possible that access is blocked based on your location?"
            },
            {
              "comment_username": "Morwud",
              "comment_create_time": "2019-08-30T08:18:57Z",
              "comment_edit_time": "2019-08-30T08:18:57Z",
              "comment_text": "Ok, I finally managed to build OCMS 11.\r\n\r\nI used suggestion from @Yuanuo to get older version of the library from other source available to me. Then I commented out the maven repository software.rescarta.org which solved the other two missing dependencies, because they can be found elsewhere (I checked that on mvnrepository.com).\r\n\r\nI did forgot about the jar in OpenCms distribution, that could be usable as well. And no, I can't access the repo server at all (although I don't know how to check that other than try the links mentioned above). I tried to access it from 2 different locations.\r\n\r\ntl;dr: To fix it I\r\n1. Changed JPedal dependency to older one located on different server\r\n2. Commented out the `software.rescarta.org` repository defined in `build.gradle`"
            }
          ]
        },
        {
          "issue_url": "https://github.com/alkacon/opencms-core/issues/660",
          "issue_title": "OpenCms 11 - write permission on site folder",
          "issue_number": 660,
          "issue_text": "I have set on a /sites/mysite \"read - view\" permissions to \"mygroup\" group (\"write\" is not checked). When I log in with a user of that group (role \"account manager\", groups \"mygroup\" + \"users\"), and go in that site, I can see the \"create resource\" button, and if if try to add a basic text file, I have an error : org.opencms.security.CmsPermissionViolationException: Error creating the resource \"/sites/mysite/new_plain\".\r\n\r\nI should not see the \"create resource\" button, no?\r\n\r\nWhen I display Permissions on a file in that site, I can see on \"User permission\" the read, view, and (strange?) direct publish checked. But direct publish is only set on Root admin group, I should not have it!",
          "issue_comments": []
        },
        {
          "issue_url": "https://github.com/alkacon/opencms-core/issues/658",
          "issue_title": "OpenCms 11.0.0: Sync function to folder should be [Close] or [Done] instead of [Cancel]",
          "issue_number": 658,
          "issue_text": "![image](https://user-images.githubusercontent.com/12636585/60080111-786fff00-9759-11e9-86f4-f81ac9f927e6.png)\r\n",
          "issue_comments": []
        }
      ]
    }
  },
  {
    "github_url": "https://github.com/orientechnologies/orientdb",
    "github_info": {
      "name": "orientechnologies/orientdb",
      "language": "Java"
    },
    "github_branches": {
      "branch_datas": [
        {
          "branch_version": "2.0.x",
          "branch_url": "https://github.com/orientechnologies/orientdb/tree/2.0.x",
          "branch_download_url": "https://github.com/orientechnologies/orientdb/archive/2.0.x.zip"
        },
        {
          "branch_version": "2.1.x",
          "branch_url": "https://github.com/orientechnologies/orientdb/tree/2.1.x",
          "branch_download_url": "https://github.com/orientechnologies/orientdb/archive/2.1.x.zip"
        },
        {
          "branch_version": "2.2.x",
          "branch_url": "https://github.com/orientechnologies/orientdb/tree/2.2.x",
          "branch_download_url": "https://github.com/orientechnologies/orientdb/archive/2.2.x.zip"
        },
        {
          "branch_version": "2.2.5",
          "branch_url": "https://github.com/orientechnologies/orientdb/tree/2.2.5",
          "branch_download_url": "https://github.com/orientechnologies/orientdb/archive/2.2.5.zip"
        },
        {
          "branch_version": "3.0.x_dump_on_error",
          "branch_url": "https://github.com/orientechnologies/orientdb/tree/3.0.x_dump_on_error",
          "branch_download_url": "https://github.com/orientechnologies/orientdb/archive/3.0.x_dump_on_error.zip"
        },
        {
          "branch_version": "3.0.x",
          "branch_url": "https://github.com/orientechnologies/orientdb/tree/3.0.x",
          "branch_download_url": "https://github.com/orientechnologies/orientdb/archive/3.0.x.zip"
        },
        {
          "branch_version": "3.1.x",
          "branch_url": "https://github.com/orientechnologies/orientdb/tree/3.1.x",
          "branch_download_url": "https://github.com/orientechnologies/orientdb/archive/3.1.x.zip"
        },
        {
          "branch_version": "8488FixForV2",
          "branch_url": "https://github.com/orientechnologies/orientdb/tree/8488FixForV2",
          "branch_download_url": "https://github.com/orientechnologies/orientdb/archive/8488FixForV2.zip"
        },
        {
          "branch_version": "NioToIo",
          "branch_url": "https://github.com/orientechnologies/orientdb/tree/NioToIo",
          "branch_download_url": "https://github.com/orientechnologies/orientdb/archive/NioToIo.zip"
        },
        {
          "branch_version": "WIP_ddl_in_transaction",
          "branch_url": "https://github.com/orientechnologies/orientdb/tree/WIP_ddl_in_transaction",
          "branch_download_url": "https://github.com/orientechnologies/orientdb/archive/WIP_ddl_in_transaction.zip"
        },
        {
          "branch_version": "WIP_distributed_tx_executor_imp",
          "branch_url": "https://github.com/orientechnologies/orientdb/tree/WIP_distributed_tx_executor_imp",
          "branch_download_url": "https://github.com/orientechnologies/orientdb/archive/WIP_distributed_tx_executor_imp.zip"
        },
        {
          "branch_version": "WIP_enable_new_unique_index",
          "branch_url": "https://github.com/orientechnologies/orientdb/tree/WIP_enable_new_unique_index",
          "branch_download_url": "https://github.com/orientechnologies/orientdb/archive/WIP_enable_new_unique_index.zip"
        },
        {
          "branch_version": "WIP_ha_locks",
          "branch_url": "https://github.com/orientechnologies/orientdb/tree/WIP_ha_locks",
          "branch_download_url": "https://github.com/orientechnologies/orientdb/archive/WIP_ha_locks.zip"
        },
        {
          "branch_version": "WIP_http_refactor",
          "branch_url": "https://github.com/orientechnologies/orientdb/tree/WIP_http_refactor",
          "branch_download_url": "https://github.com/orientechnologies/orientdb/archive/WIP_http_refactor.zip"
        },
        {
          "branch_version": "WIP_index_rebuild_without_docs",
          "branch_url": "https://github.com/orientechnologies/orientdb/tree/WIP_index_rebuild_without_docs",
          "branch_download_url": "https://github.com/orientechnologies/orientdb/archive/WIP_index_rebuild_without_docs.zip"
        },
        {
          "branch_version": "WIP_js_sandbox",
          "branch_url": "https://github.com/orientechnologies/orientdb/tree/WIP_js_sandbox",
          "branch_download_url": "https://github.com/orientechnologies/orientdb/archive/WIP_js_sandbox.zip"
        },
        {
          "branch_version": "WIP_locks_allocation",
          "branch_url": "https://github.com/orientechnologies/orientdb/tree/WIP_locks_allocation",
          "branch_download_url": "https://github.com/orientechnologies/orientdb/archive/WIP_locks_allocation.zip"
        },
        {
          "branch_version": "WIP_safe_distributed_3.0.x",
          "branch_url": "https://github.com/orientechnologies/orientdb/tree/WIP_safe_distributed_3.0.x",
          "branch_download_url": "https://github.com/orientechnologies/orientdb/archive/WIP_safe_distributed_3.0.x.zip"
        },
        {
          "branch_version": "WIP_scheduler_locking",
          "branch_url": "https://github.com/orientechnologies/orientdb/tree/WIP_scheduler_locking",
          "branch_download_url": "https://github.com/orientechnologies/orientdb/archive/WIP_scheduler_locking.zip"
        },
        {
          "branch_version": "WIP_server_listener_refactor",
          "branch_url": "https://github.com/orientechnologies/orientdb/tree/WIP_server_listener_refactor",
          "branch_download_url": "https://github.com/orientechnologies/orientdb/archive/WIP_server_listener_refactor.zip"
        },
        {
          "branch_version": "async_security_cache",
          "branch_url": "https://github.com/orientechnologies/orientdb/tree/async_security_cache",
          "branch_download_url": "https://github.com/orientechnologies/orientdb/archive/async_security_cache.zip"
        },
        {
          "branch_version": "autoshardedclasses",
          "branch_url": "https://github.com/orientechnologies/orientdb/tree/autoshardedclasses",
          "branch_download_url": "https://github.com/orientechnologies/orientdb/archive/autoshardedclasses.zip"
        },
        {
          "branch_version": "autosharding",
          "branch_url": "https://github.com/orientechnologies/orientdb/tree/autosharding",
          "branch_download_url": "https://github.com/orientechnologies/orientdb/archive/autosharding.zip"
        },
        {
          "branch_version": "bind_to_cache",
          "branch_url": "https://github.com/orientechnologies/orientdb/tree/bind_to_cache",
          "branch_download_url": "https://github.com/orientechnologies/orientdb/archive/bind_to_cache.zip"
        },
        {
          "branch_version": "breadthFirstFix",
          "branch_url": "https://github.com/orientechnologies/orientdb/tree/breadthFirstFix",
          "branch_download_url": "https://github.com/orientechnologies/orientdb/archive/breadthFirstFix.zip"
        },
        {
          "branch_version": "case_sensitive_schema_refactoring",
          "branch_url": "https://github.com/orientechnologies/orientdb/tree/case_sensitive_schema_refactoring",
          "branch_download_url": "https://github.com/orientechnologies/orientdb/archive/case_sensitive_schema_refactoring.zip"
        },
        {
          "branch_version": "connectionDebug",
          "branch_url": "https://github.com/orientechnologies/orientdb/tree/connectionDebug",
          "branch_download_url": "https://github.com/orientechnologies/orientdb/archive/connectionDebug.zip"
        },
        {
          "branch_version": "cure_v",
          "branch_url": "https://github.com/orientechnologies/orientdb/tree/cure_v",
          "branch_download_url": "https://github.com/orientechnologies/orientdb/archive/cure_v.zip"
        },
        {
          "branch_version": "delay_security_cache_update",
          "branch_url": "https://github.com/orientechnologies/orientdb/tree/delay_security_cache_update",
          "branch_download_url": "https://github.com/orientechnologies/orientdb/archive/delay_security_cache_update.zip"
        },
        {
          "branch_version": "deleteEntireDirOnDrop",
          "branch_url": "https://github.com/orientechnologies/orientdb/tree/deleteEntireDirOnDrop",
          "branch_download_url": "https://github.com/orientechnologies/orientdb/archive/deleteEntireDirOnDrop.zip"
        }
      ]
    },
    "github_pull_requests": {
      "pull_datas": [
        {
          "pull_number": 9397,
          "pull_title": "Replace locks used between distributed transaction phases",
          "pull_version": "orientechnologies:develop",
          "pull_version_url": "https://github.com/orientechnologies/orientdb/tree/develop"
        },
        {
          "pull_number": 9375,
          "pull_title": "[WIP] unique index for distributed API part 3",
          "pull_version": "orientechnologies:develop",
          "pull_version_url": "https://github.com/orientechnologies/orientdb/tree/develop"
        },
        {
          "pull_number": 9269,
          "pull_title": "Fix eval with result and variable",
          "pull_version": "orientechnologies:develop",
          "pull_version_url": "https://github.com/orientechnologies/orientdb/tree/develop"
        },
        {
          "pull_number": 9032,
          "pull_title": "Fix OETL Console command block",
          "pull_version": "orientechnologies:develop",
          "pull_version_url": "https://github.com/orientechnologies/orientdb/tree/develop"
        },
        {
          "pull_number": 8962,
          "pull_title": "Wip index rebuild without docs",
          "pull_version": "orientechnologies:develop",
          "pull_version_url": "https://github.com/orientechnologies/orientdb/tree/develop"
        },
        {
          "pull_number": 8948,
          "pull_title": "Add unit tests for com.orientechnologies.orient.core.sql.parser.JavaCharStream",
          "pull_version": "orientechnologies:develop",
          "pull_version_url": "https://github.com/orientechnologies/orientdb/tree/develop"
        },
        {
          "pull_number": 8804,
          "pull_title": "Update OrientJdbcConnection.java",
          "pull_version": "orientechnologies:develop",
          "pull_version_url": "https://github.com/orientechnologies/orientdb/tree/develop"
        },
        {
          "pull_number": 8638,
          "pull_title": "Deletion SB-Tree RidBags",
          "pull_version": "orientechnologies:3.0.x",
          "pull_version_url": "https://github.com/orientechnologies/orientdb/tree/3.0.x"
        },
        {
          "pull_number": 8623,
          "pull_title": "Avoid therad local for sequences",
          "pull_version": "orientechnologies:develop",
          "pull_version_url": "https://github.com/orientechnologies/orientdb/tree/develop"
        },
        {
          "pull_number": 8568,
          "pull_title": "Provide 'fluent interface' on OElement #8564",
          "pull_version": "orientechnologies:develop",
          "pull_version_url": "https://github.com/orientechnologies/orientdb/tree/develop"
        },
        {
          "pull_number": 8293,
          "pull_title": "For geoserver",
          "pull_version": "orientechnologies:develop",
          "pull_version_url": "https://github.com/orientechnologies/orientdb/tree/develop"
        },
        {
          "pull_number": 8119,
          "pull_title": "Pluggable storage",
          "pull_version": "orientechnologies:develop",
          "pull_version_url": "https://github.com/orientechnologies/orientdb/tree/develop"
        },
        {
          "pull_number": 8101,
          "pull_title": "Tree index code cleanup",
          "pull_version": "orientechnologies:develop",
          "pull_version_url": "https://github.com/orientechnologies/orientdb/tree/develop"
        },
        {
          "pull_number": 7759,
          "pull_title": "Comments regarding tests problems",
          "pull_version": "orientechnologies:develop",
          "pull_version_url": "https://github.com/orientechnologies/orientdb/tree/develop"
        },
        {
          "pull_number": 7648,
          "pull_title": "Added method to log in WAL custom WALRecords",
          "pull_version": "orientechnologies:develop",
          "pull_version_url": "https://github.com/orientechnologies/orientdb/tree/develop"
        },
        {
          "pull_number": 7502,
          "pull_title": "Remove unnecessary code",
          "pull_version": "orientechnologies:develop",
          "pull_version_url": "https://github.com/orientechnologies/orientdb/tree/develop"
        },
        {
          "pull_number": 7415,
          "pull_title": "(refactor) some opportunities to use multi-catch",
          "pull_version": "orientechnologies:develop",
          "pull_version_url": "https://github.com/orientechnologies/orientdb/tree/develop"
        },
        {
          "pull_number": 7360,
          "pull_title": "Case sensitive schema refactoring",
          "pull_version": "orientechnologies:develop",
          "pull_version_url": "https://github.com/orientechnologies/orientdb/tree/develop"
        },
        {
          "pull_number": 7341,
          "pull_title": "Avoid extra piping with grep",
          "pull_version": "orientechnologies:master",
          "pull_version_url": "https://github.com/orientechnologies/orientdb/tree/master"
        },
        {
          "pull_number": 7174,
          "pull_title": "Change ODistributedRedirectException to extend ONeedRetryException",
          "pull_version": "orientechnologies:develop",
          "pull_version_url": "https://github.com/orientechnologies/orientdb/tree/develop"
        },
        {
          "pull_number": 7093,
          "pull_title": "Support for indexes on fields of embedded properties type like EMBEDDED/EMBEDDEDLIST/EMBEDDEDMAP/EMBEDDEDSET",
          "pull_version": "orientechnologies:develop",
          "pull_version_url": "https://github.com/orientechnologies/orientdb/tree/develop"
        },
        {
          "pull_number": 7022,
          "pull_title": "Add TestCode",
          "pull_version": "orientechnologies:develop",
          "pull_version_url": "https://github.com/orientechnologies/orientdb/tree/develop"
        },
        {
          "pull_number": 7019,
          "pull_title": "Fix merge method",
          "pull_version": "orientechnologies:develop",
          "pull_version_url": "https://github.com/orientechnologies/orientdb/tree/develop"
        },
        {
          "pull_number": 7003,
          "pull_title": "server.bat - Add a pause at the end",
          "pull_version": "orientechnologies:master",
          "pull_version_url": "https://github.com/orientechnologies/orientdb/tree/master"
        },
        {
          "pull_number": 6993,
          "pull_title": "Added missing constructors",
          "pull_version": "orientechnologies:develop",
          "pull_version_url": "https://github.com/orientechnologies/orientdb/tree/develop"
        },
        {
          "pull_number": 6752,
          "pull_title": "fix for #6751",
          "pull_version": "orientechnologies:master",
          "pull_version_url": "https://github.com/orientechnologies/orientdb/tree/master"
        },
        {
          "pull_number": 6602,
          "pull_title": "Suggestion: add tattletale report...",
          "pull_version": "orientechnologies:develop",
          "pull_version_url": "https://github.com/orientechnologies/orientdb/tree/develop"
        },
        {
          "pull_number": 6600,
          "pull_title": "Current Live query works only  final matched datas after crud operation",
          "pull_version": "orientechnologies:develop",
          "pull_version_url": "https://github.com/orientechnologies/orientdb/tree/develop"
        },
        {
          "pull_number": 6430,
          "pull_title": "Update readme.txt",
          "pull_version": "orientechnologies:master",
          "pull_version_url": "https://github.com/orientechnologies/orientdb/tree/master"
        },
        {
          "pull_number": 6208,
          "pull_title": "Allow ORecordHook define default hook position by itself",
          "pull_version": "orientechnologies:develop",
          "pull_version_url": "https://github.com/orientechnologies/orientdb/tree/develop"
        }
      ]
    },
    "github_issues": {
      "issue_datas": [
        {
          "issue_url": "https://github.com/orientechnologies/orientdb/issues/9399",
          "issue_title": "[OVariableParser.resolveVariables] Error on resolving property",
          "issue_number": 9399,
          "issue_text": " \"command\": \"create egde '${className}' from  (select from course where name='${input.course}') to (select from label where name='${input.label}') set name='${input.name}'\"\\r\\n\\r\\nshow me :\\r\\n\"create '' from  (select from course where name=objiet) to (select from label where name='JAVA') set name=contain\"\\r\\nError on resolving property: classNameException\\r\\n",
          "issue_comments": [
            {
              "comment_username": "luigidellaquila",
              "comment_create_time": "2020-09-29T09:17:37Z",
              "comment_edit_time": "2020-09-29T09:17:37Z",
              "comment_text": "Hi @HuaC-Z \r\n\r\nOVariableParser is an old API, almost unused nowadays.\r\nAnyway, probably the problem here is just a missing `input.`, ie. `create '${input.className}` instead of `create '${className}`\r\n\r\nThis said, probably you should use query parameters instead of variables:\r\n\r\n```\r\nSELECT FROM ... WHERE name = ?\r\n```\r\n\r\nAnd the `CREATE` syntax is wrong, you probably mean `CREATE EDGE \r\n\r\nI hope it helps\r\n\r\nThanks\r\n\r\nLuigi"
            },
            {
              "comment_username": "HuaC-Z",
              "comment_create_time": "2020-09-29T09:46:22Z",
              "comment_edit_time": "2020-09-29T09:56:04Z",
              "comment_text": "\r\n>hi @Luigi\r\nThank you for helping me solve the problem.\r\nI want to get variables from the configuration file instead of from the CSV. Do you have any suggestions on this point."
            }
          ]
        },
        {
          "issue_url": "https://github.com/orientechnologies/orientdb/issues/9398",
          "issue_title": "List connections command doesn't show database and user information",
          "issue_number": 9398,
          "issue_text": "### OrientDB Version: 3.0.34\r\n### Java Version: 1.8\r\n### OS: Linux\r\n\r\n## Expected behavior  \r\n\"list connections\" commands shows database and user for binary connections\r\n\r\n## Actual behavior  \r\nin the output of list connections commands the database and user columns appears as \"-\" for binary connections\r\n\r\n## Steps to reproduce  \r\nexecute console.sh script\r\nconnect with \"CONNECT remote:localhost user password\"\r\nexecute \"list connections\" \r\n",
          "issue_comments": []
        },
        {
          "issue_url": "https://github.com/orientechnologies/orientdb/issues/9397",
          "issue_title": "Replace locks used between distributed transaction phases",
          "issue_number": 9397,
          "issue_text": "**Do not merge yet!**",
          "issue_comments": [
            {
              "comment_username": "codecov-commenter",
              "comment_create_time": "2020-09-29T08:52:46Z",
              "comment_edit_time": "2020-09-29T08:56:16Z",
              "comment_text": "# [Codecov](https://codecov.io/gh/orientechnologies/orientdb/pull/9397?src=pr&el=h1) Report\n> Merging [#9397](https://codecov.io/gh/orientechnologies/orientdb/pull/9397?src=pr&el=desc) into [develop](https://codecov.io/gh/orientechnologies/orientdb/commit/f8cbba15933a0dad8bd31172246d39d7a4752068?el=desc) will **increase** coverage by `6.28%`.\n> The diff coverage is `65.84%`.\n\n[![Impacted file tree graph](https://codecov.io/gh/orientechnologies/orientdb/pull/9397/graphs/tree.svg?width=650&height=150&src=pr&token=JVej89Tdc3)](https://codecov.io/gh/orientechnologies/orientdb/pull/9397?src=pr&el=tree)\n\n```diff\n@@              Coverage Diff               @@\n##             develop    #9397       +/-   ##\n==============================================\n+ Coverage      47.81%   54.10%    +6.28%     \n- Complexity      1615    35582    +33967     \n==============================================\n  Files            192     2436     +2244     \n  Lines          11408   192601   +181193     \n  Branches        1903    35115    +33212     \n==============================================\n+ Hits            5455   104208    +98753     \n- Misses          5166    73467    +68301     \n- Partials         787    14926    +14139     \n```\n\n\n| [Impacted Files](https://codecov.io/gh/orientechnologies/orientdb/pull/9397?src=pr&el=tree) | Coverage Δ | Complexity Δ | |\n|---|---|---|---|\n| [...on/ODistributedTxPromiseRequestIsOldException.java](https://codecov.io/gh/orientechnologies/orientdb/pull/9397/diff?src=pr&el=tree#diff-ZGlzdHJpYnV0ZWQvc3JjL21haW4vamF2YS9jb20vb3JpZW50ZWNobm9sb2dpZXMvb3JpZW50L3NlcnZlci9kaXN0cmlidXRlZC9leGNlcHRpb24vT0Rpc3RyaWJ1dGVkVHhQcm9taXNlUmVxdWVzdElzT2xkRXhjZXB0aW9uLmphdmE=) | `0.00% <0.00%> (ø)` | `0.00 <0.00> (?)` | |\n| [...nt/server/distributed/impl/ODistributedOutput.java](https://codecov.io/gh/orientechnologies/orientdb/pull/9397/diff?src=pr&el=tree#diff-ZGlzdHJpYnV0ZWQvc3JjL21haW4vamF2YS9jb20vb3JpZW50ZWNobm9sb2dpZXMvb3JpZW50L3NlcnZlci9kaXN0cmlidXRlZC9pbXBsL09EaXN0cmlidXRlZE91dHB1dC5qYXZh) | `35.96% <0.00%> (ø)` | `29.00 <0.00> (?)` | |\n| [...er/distributed/impl/ODistributedTxCoordinator.java](https://codecov.io/gh/orientechnologies/orientdb/pull/9397/diff?src=pr&el=tree#diff-ZGlzdHJpYnV0ZWQvc3JjL21haW4vamF2YS9jb20vb3JpZW50ZWNobm9sb2dpZXMvb3JpZW50L3NlcnZlci9kaXN0cmlidXRlZC9pbXBsL09EaXN0cmlidXRlZFR4Q29vcmRpbmF0b3IuamF2YQ==) | `36.99% <0.00%> (ø)` | `16.00 <1.00> (?)` | |\n| [...istributed/impl/ODistributedTxResponseManager.java](https://codecov.io/gh/orientechnologies/orientdb/pull/9397/diff?src=pr&el=tree#diff-ZGlzdHJpYnV0ZWQvc3JjL21haW4vamF2YS9jb20vb3JpZW50ZWNobm9sb2dpZXMvb3JpZW50L3NlcnZlci9kaXN0cmlidXRlZC9pbXBsL09EaXN0cmlidXRlZFR4UmVzcG9uc2VNYW5hZ2VyLmphdmE=) | `59.00% <ø> (ø)` | `20.00 <0.00> (?)` | |\n| [.../distributed/impl/task/OTransactionPhase2Task.java](https://codecov.io/gh/orientechnologies/orientdb/pull/9397/diff?src=pr&el=tree#diff-ZGlzdHJpYnV0ZWQvc3JjL21haW4vamF2YS9jb20vb3JpZW50ZWNobm9sb2dpZXMvb3JpZW50L3NlcnZlci9kaXN0cmlidXRlZC9pbXBsL3Rhc2svT1RyYW5zYWN0aW9uUGhhc2UyVGFzay5qYXZh) | `45.63% <ø> (ø)` | `15.00 <0.00> (?)` | |\n| [...stributed/task/ODistributedKeyLockedException.java](https://codecov.io/gh/orientechnologies/orientdb/pull/9397/diff?src=pr&el=tree#diff-c2VydmVyL3NyYy9tYWluL2phdmEvY29tL29yaWVudGVjaG5vbG9naWVzL29yaWVudC9zZXJ2ZXIvZGlzdHJpYnV0ZWQvdGFzay9PRGlzdHJpYnV0ZWRLZXlMb2NrZWRFeGNlcHRpb24uamF2YQ==) | `0.00% <0.00%> (ø)` | `0.00 <0.00> (ø)` | |\n| [...ibuted/task/ODistributedRecordLockedException.java](https://codecov.io/gh/orientechnologies/orientdb/pull/9397/diff?src=pr&el=tree#diff-c2VydmVyL3NyYy9tYWluL2phdmEvY29tL29yaWVudGVjaG5vbG9naWVzL29yaWVudC9zZXJ2ZXIvZGlzdHJpYnV0ZWQvdGFzay9PRGlzdHJpYnV0ZWRSZWNvcmRMb2NrZWRFeGNlcHRpb24uamF2YQ==) | `0.00% <0.00%> (ø)` | `0.00 <0.00> (ø)` | |\n| [.../distributed/impl/task/OTransactionPhase1Task.java](https://codecov.io/gh/orientechnologies/orientdb/pull/9397/diff?src=pr&el=tree#diff-ZGlzdHJpYnV0ZWQvc3JjL21haW4vamF2YS9jb20vb3JpZW50ZWNobm9sb2dpZXMvb3JpZW50L3NlcnZlci9kaXN0cmlidXRlZC9pbXBsL3Rhc2svT1RyYW5zYWN0aW9uUGhhc2UxVGFzay5qYXZh) | `61.50% <50.00%> (ø)` | `39.00 <2.00> (?)` | |\n| [...distributed/impl/ODatabaseDocumentDistributed.java](https://codecov.io/gh/orientechnologies/orientdb/pull/9397/diff?src=pr&el=tree#diff-ZGlzdHJpYnV0ZWQvc3JjL21haW4vamF2YS9jb20vb3JpZW50ZWNobm9sb2dpZXMvb3JpZW50L3NlcnZlci9kaXN0cmlidXRlZC9pbXBsL09EYXRhYmFzZURvY3VtZW50RGlzdHJpYnV0ZWQuamF2YQ==) | `43.61% <65.00%> (ø)` | `68.00 <10.00> (?)` | |\n| [...ver/distributed/exception/OTxPromiseException.java](https://codecov.io/gh/orientechnologies/orientdb/pull/9397/diff?src=pr&el=tree#diff-ZGlzdHJpYnV0ZWQvc3JjL21haW4vamF2YS9jb20vb3JpZW50ZWNobm9sb2dpZXMvb3JpZW50L3NlcnZlci9kaXN0cmlidXRlZC9leGNlcHRpb24vT1R4UHJvbWlzZUV4Y2VwdGlvbi5qYXZh) | `66.66% <66.66%> (ø)` | `1.00 <1.00> (?)` | |\n| ... and [2261 more](https://codecov.io/gh/orientechnologies/orientdb/pull/9397/diff?src=pr&el=tree-more) | |\n\n------\n\n[Continue to review full report at Codecov](https://codecov.io/gh/orientechnologies/orientdb/pull/9397?src=pr&el=continue).\n> **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta)\n> `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`\n> Powered by [Codecov](https://codecov.io/gh/orientechnologies/orientdb/pull/9397?src=pr&el=footer). Last update [f8cbba1...9753794](https://codecov.io/gh/orientechnologies/orientdb/pull/9397?src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).\n"
            }
          ]
        },
        {
          "issue_url": "https://github.com/orientechnologies/orientdb/issues/9396",
          "issue_title": "OrientDB live lock thread in collection.size",
          "issue_number": 9396,
          "issue_text": "### OrientDB Version: 3.0.34\\r\\n### Java Version: 1.8\\r\\n### OS: Linux \\r\\n\\r\\n## Expected behavior  \\r\\nThreads don't live lock and consume all cpu\\r\\n\\r\\n## Actual behavior  \\r\\nThis thread is live locked consuming 100% CPU with this stack. I've also attached the full thread dump\\r\\n[orient-server-boot.txt](https://github.com/orientechnologies/orientdb/files/5268055/orient-server-boot.txt)\\r\\n\\r\\n\\r\\n\\r\\n\"OrientDB (/172.25.1.18:2424) <- BinaryClient (/172.25.5.171:54870)\" #1663 daemon prio=5 os_prio=0 tid=0x00007f8de40a4800 nid=0x53e8 runnable [0x00007f8dee3dd000]\\r\\n   java.lang.Thread.State: RUNNABLE\\r\\n        at java.util.Collections$UnmodifiableCollection.size(Collections.java:1032)\\r\\n        at com.orientechnologies.orient.core.tx.OTransactionRealAbstract.isIndexKeyMayDependOnRid(OTransactionRealAbstract.java:503)\\r\\n        at com.orientechnologies.orient.core.tx.OTransactionRealAbstract.updateIdentityAfterCommit(OTransactionRealAbstract.java:338)\\r\\n        at com.orientechnologies.orient.core.storage.impl.local.OAbstractPaginatedStorage.commit(OAbstractPaginatedStorage.java:2651)\\r\\n        at com.orientechnologies.orient.core.storage.impl.local.OAbstractPaginatedStorage.commit(OAbstractPaginatedStorage.java:2489)\\r\\n        at com.orientechnologies.orient.core.db.document.ODatabaseDocumentAbstract.internalCommit(ODatabaseDocumentAbstract.java:2757)\\r\\n        at com.orientechnologies.orient.core.tx.OTransactionOptimistic.doCommit(OTransactionOptimistic.java:533)\\r\\n        at com.orientechnologies.orient.core.tx.OTransactionOptimistic.commit(OTransactionOptimistic.java:103)\\r\\n        at com.orientechnologies.orient.core.db.document.ODatabaseDocumentAbstract.commit(ODatabaseDocumentAbstract.java:2238)\\r\\n        at com.orientechnologies.orient.core.db.document.ODatabaseDocumentAbstract.commit(ODatabaseDocumentAbstract.java:2208)\\r\\n        at com.orientechnologies.orient.core.sql.parser.OCommitStatement.executeSimple(OCommitStatement.java:30)\\r\\n        at com.orientechnologies.orient.core.sql.executor.OSingleOpExecutionPlan.executeInternal(OSingleOpExecutionPlan.java:99)\\r\\n        at com.orientechnologies.orient.core.sql.executor.ScriptLineStep.syncPull(ScriptLineStep.java:37)\\r\\n        at com.orientechnologies.orient.core.sql.executor.OScriptExecutionPlan.doExecute(OScriptExecutionPlan.java:89)\\r\\n        at com.orientechnologies.orient.core.sql.executor.OScriptExecutionPlan.fetchNext(OScriptExecutionPlan.java:50)\\r\\n        at com.orientechnologies.orient.core.sql.parser.OLocalResultSet.fetchNext(OLocalResultSet.java:39)\\r\\n        at com.orientechnologies.orient.core.sql.parser.OLocalResultSet.<init>(OLocalResultSet.java:30)\\r\\n        at com.orientechnologies.orient.core.command.OSqlScriptExecutor.executeInternal(OSqlScriptExecutor.java:114)\\r\\n        at com.orientechnologies.orient.core.command.OSqlScriptExecutor.execute(OSqlScriptExecutor.java:60)\\r\\n        at com.orientechnologies.orient.core.db.document.ODatabaseDocumentEmbedded.execute(ODatabaseDocumentEmbedded.java:638)\\r\\n        at com.orientechnologies.orient.server.OConnectionBinaryExecutor.executeQuery(OConnectionBinaryExecutor.java:1192)\\r\\n        at com.orientechnologies.orient.client.remote.message.OQueryRequest.execute(OQueryRequest.java:136)\\r\\n        at com.orientechnologies.orient.server.network.protocol.binary.ONetworkProtocolBinary.sessionRequest(ONetworkProtocolBinary.java:320)\\r\\n        at com.orientechnologies.orient.server.network.protocol.binary.ONetworkProtocolBinary.execute(ONetworkProtocolBinary.java:215)\\r\\n        at com.orientechnologies.common.thread.OSoftThread.run(OSoftThread.java:69)\\r\\n\\r\\n\\r\\n## Steps to reproduce  \\r\\nI'm upserting in OrientDB using multiple threads with remote binary connections.",
          "issue_comments": [
            {
              "comment_username": "madmac2501",
              "comment_create_time": "2020-09-28T13:11:37Z",
              "comment_edit_time": "2020-09-28T13:11:37Z",
              "comment_text": "Hi,\\r\\n\\r\\nnow I'm seeing this behavior with this thread, in this case I'm not sure if it's caused by a query or by a defect. How can I identify the query that is running this?\\r\\n\\r\\nThanks\\r\\n\\r\\n\"OrientDB (/172.25.1.18:2424) <- BinaryClient (/172.25.5.106:48914)\" #2025 daemon prio=5 os_prio=0 tid=0x00007fd0d4292800 nid=0x73fe runnable [0x00007fcd6be70000]\\r\\n   java.lang.Thread.State: RUNNABLE\\r\\n        at com.orientechnologies.orient.core.tx.OTransactionRealAbstract.isIndexKeyMayDependOnRid(OTransactionRealAbstract.java:517)\\r\\n        at com.orientechnologies.orient.core.tx.OTransactionRealAbstract.isIndexKeyMayDependOnRid(OTransactionRealAbstract.java:504)\\r\\n        at com.orientechnologies.orient.core.tx.OTransactionRealAbstract.updateIdentityAfterCommit(OTransactionRealAbstract.java:338)\\r\\n        at com.orientechnologies.orient.core.storage.impl.local.OAbstractPaginatedStorage.commit(OAbstractPaginatedStorage.java:2651)\\r\\n        at com.orientechnologies.orient.core.storage.impl.local.OAbstractPaginatedStorage.commit(OAbstractPaginatedStorage.java:2489)\\r\\n        at com.orientechnologies.orient.core.db.document.ODatabaseDocumentAbstract.internalCommit(ODatabaseDocumentAbstract.java:2757)\\r\\n        at com.orientechnologies.orient.core.tx.OTransactionOptimistic.doCommit(OTransactionOptimistic.java:533)\\r\\n        at com.orientechnologies.orient.core.tx.OTransactionOptimistic.commit(OTransactionOptimistic.java:103)\\r\\n        at com.orientechnologies.orient.core.db.document.ODatabaseDocumentAbstract.commit(ODatabaseDocumentAbstract.java:2238)\\r\\n        at com.orientechnologies.orient.core.db.document.ODatabaseDocumentAbstract.commit(ODatabaseDocumentAbstract.java:2208)\\r\\n        at com.orientechnologies.orient.core.sql.parser.OCommitStatement.executeSimple(OCommitStatement.java:30)\\r\\n        at com.orientechnologies.orient.core.sql.executor.OSingleOpExecutionPlan.executeInternal(OSingleOpExecutionPlan.java:99)\\r\\n        at com.orientechnologies.orient.core.sql.executor.ScriptLineStep.syncPull(ScriptLineStep.java:37)\\r\\n        at com.orientechnologies.orient.core.sql.executor.OScriptExecutionPlan.doExecute(OScriptExecutionPlan.java:89)\\r\\n        at com.orientechnologies.orient.core.sql.executor.OScriptExecutionPlan.fetchNext(OScriptExecutionPlan.java:50)\\r\\n        at com.orientechnologies.orient.core.sql.parser.OLocalResultSet.fetchNext(OLocalResultSet.java:39)\\r\\n        at com.orientechnologies.orient.core.sql.parser.OLocalResultSet.<init>(OLocalResultSet.java:30)\\r\\n        at com.orientechnologies.orient.core.command.OSqlScriptExecutor.executeInternal(OSqlScriptExecutor.java:114)\\r\\n        at com.orientechnologies.orient.core.command.OSqlScriptExecutor.execute(OSqlScriptExecutor.java:60)\\r\\n        at com.orientechnologies.orient.core.db.document.ODatabaseDocumentEmbedded.execute(ODatabaseDocumentEmbedded.java:638)\\r\\n        at com.orientechnologies.orient.server.OConnectionBinaryExecutor.executeQuery(OConnectionBinaryExecutor.java:1192)\\r\\n        at com.orientechnologies.orient.client.remote.message.OQueryRequest.execute(OQueryRequest.java:136)\\r\\n        at com.orientechnologies.orient.server.network.protocol.binary.ONetworkProtocolBinary.sessionRequest(ONetworkProtocolBinary.java:320)\\r\\n        at com.orientechnologies.orient.server.network.protocol.binary.ONetworkProtocolBinary.execute(ONetworkProtocolBinary.java:215)\\r\\n        at com.orientechnologies.common.thread.OSoftThread.run(OSoftThread.java:69)\\r\\n"
            }
          ]
        },
        {
          "issue_url": "https://github.com/orientechnologies/orientdb/issues/9395",
          "issue_title": "server.sh: Could not initialize class com.kenai.jffi.MemoryIO$SingletonHolder at com.kenai.jffi.MemoryIO.getInstance(MemoryIO.java:64)",
          "issue_number": 9395,
          "issue_text": "### OrientDB Version: 3.1.2\\r\\n### Java Version: openjdk version \"1.8.0_262\"  \\r\\n### OS: CentOS Linux release 7.8.2003 (Core)\\r\\n\\r\\n## Expected behavior  \\r\\nOrientdb server should start when I run ./server.sh from the bin directory.\\r\\n\\r\\n## Actual behavior  \\r\\nremoving old pid file /ai/orientdb-3.1.2/bin/orient.pid\\r\\n\\r\\n2020-09-21 14:56:42:830 INFO  Can not detect value of limit of open files. [ONative]\\r\\n2020-09-21 14:56:42:838 INFO  Default limit of open files (512) will be used. [ONative]\\r\\n2020-09-21 14:56:42:856 INFO  Loading configuration from: /ai/orientdb-3.1.2/config/orientdb-server-config.xml... [OServerConfigurationLoaderXml]\\r\\n2020-09-21 14:56:43:252 INFO  OrientDB Server v3.1.2 - Veloce (build 555d4a1824fdc980c2c2a680a840d68fc03f3b5e, branch 3.1.x) is starting up... [OServer]\\r\\n2020-09-21 14:56:43:827 INFO  405541326848 B/386754 MB/377 GB of physical memory were detected on machine [ONative]\\r\\n2020-09-21 14:56:43:828 INFO  Can not detect memory limit value. [ONative]\\r\\n2020-09-21 14:56:43:829 INFO  Path to 'memory' cgroup is '/' [ONative]\\r\\n2020-09-21 14:56:43:835 INFO  Mounting path for memory cgroup controller is '/sys/fs/cgroup/memory' [ONative]\\r\\n2020-09-21 14:56:43:836 INFO  cgroup soft memory limit is 9223372036854771712 B/8796093022207 MB/8589934591 GB [ONative]\\r\\n2020-09-21 14:56:43:837 INFO  cgroup hard memory limit is 9223372036854771712 B/8796093022207 MB/8589934591 GB [ONative]\\r\\n2020-09-21 14:56:43:838 INFO  Detected memory limit for current process is 405541326848 B/386754 MB/377 GB [ONative]\\r\\n2020-09-21 14:56:43:840 INFO  JVM can use maximum 1963MB of heap memory [OMemoryAndLocalPaginatedEnginesInitializer]\\r\\n2020-09-21 14:56:43:841 INFO  Because OrientDB is running outside a container 2g of memory will be left unallocated according to the setting 'memory.leftToOS' not taking into account heap memory [OMemoryAndLocalPaginatedEnginesInitializer]\\r\\n2020-09-21 14:56:43:841 INFO  OrientDB auto-config DISKCACHE=382,743MB (heap=1,963MB os=386,754MB) [orientechnologies]\\r\\n2020-09-21 14:56:43:847 INFO  System is started under an effective user : `user` \\r\\n[OEngineLocalPaginated]\\r\\n2020-09-21 14:56:43:869 INFO  Allocation of 5817693 pages. [OEngineLocalPaginated]\\r\\n2020-09-21 14:56:44:038 INFO  WAL maximum segment size is set to 7,530 MB [OrientDBEmbedded]\\r\\n2020-09-21 14:56:44:062 INFO  Databases directory: /ai/orientdb-3.1.2/databases [OServer]\\r\\n2020-09-21 14:56:44:110 INFO  Creating the system database 'OSystem' for current server [OSystemDatabase]\\r\\n2020-09-21 14:56:44:172 INFO  Page size for WAL located in /ai/orientdb-3.1.2/databases/OSystem is set to 4096 bytes. [CASDiskWriteAheadLog]Exception `480B7195` in storage `plocal:/ai/orientdb-3.1.2/databases/OSystem`: 3.1.2 - Veloce (build 555d4a1824fdc980c2c2a680a840d68fc03f3b5e, branch 3.1.x)\\r\\njava.lang.NoClassDefFoundError: Could not initialize class com.kenai.jffi.MemoryIO$SingletonHolder\\r\\n        at com.kenai.jffi.MemoryIO.getInstance(MemoryIO.java:64)\\r\\n        at com.orientechnologies.common.directmemory.ODirectMemoryAllocator.allocate(ODirectMemoryAllocator.java:123)\\r\\n        at com.orientechnologies.orient.core.storage.impl.local.paginated.wal.cas.CASDiskWriteAheadLog.<init>(CASDiskWriteAheadLog.java:393)\\r\\n        at com.orientechnologies.orient.core.storage.disk.OLocalPaginatedStorage.initWalAndDiskCache(OLocalPaginatedStorage.java:786)\\r\\n        at com.orientechnologies.orient.core.storage.impl.local.OAbstractPaginatedStorage.create(OAbstractPaginatedStorage.java:731)\\r\\n        at com.orientechnologies.orient.core.storage.disk.OLocalPaginatedStorage.create(OLocalPaginatedStorage.java:207)\\r\\n        at com.orientechnologies.orient.core.db.OrientDBEmbedded.internalCreate(OrientDBEmbedded.java:743)\\r\\n        at com.orientechnologies.orient.core.db.OrientDBEmbedded.create(OrientDBEmbedded.java:635)\\r\\n        at com.orientechnologies.orient.server.OSystemDatabase.init(OSystemDatabase.java:155)\\r\\n        at com.orientechnologies.orient.server.OSystemDatabase.<init>(OSystemDatabase.java:50)\\r\\n        at com.orientechnologies.orient.server.OServer.initSystemDatabase(OServer.java:1276)\\r\\n        at com.orientechnologies.orient.server.OServer.activate(OServer.java:444)\\r\\n        at com.orientechnologies.orient.server.OServerMain$1.run(OServerMain.java:49)\\r\\nJVM error was thrown\\r\\njava.lang.NoClassDefFoundError: Could not initialize class com.kenai.jffi.MemoryIO$SingletonHolder\\r\\n        at com.kenai.jffi.MemoryIO.getInstance(MemoryIO.java:64)\\r\\n        at com.orientechnologies.common.directmemory.ODirectMemoryAllocator.allocate(ODirectMemoryAllocator.java:123)\\r\\n        at com.orientechnologies.orient.core.storage.impl.local.paginated.wal.cas.CASDiskWriteAheadLog.<init>(CASDiskWriteAheadLog.java:393)\\r\\n        at com.orientechnologies.orient.core.storage.disk.OLocalPaginatedStorage.initWalAndDiskCache(OLocalPaginatedStorage.java:786)\\r\\n        at com.orientechnologies.orient.core.storage.impl.local.OAbstractPaginatedStorage.create(OAbstractPaginatedStorage.java:731)\\r\\n        at com.orientechnologies.orient.core.storage.disk.OLocalPaginatedStorage.create(OLocalPaginatedStorage.java:207)\\r\\n        at com.orientechnologies.orient.core.db.OrientDBEmbedded.internalCreate(OrientDBEmbedded.java:743)\\r\\n        at com.orientechnologies.orient.core.db.OrientDBEmbedded.create(OrientDBEmbedded.java:635)\\r\\n        at com.orientechnologies.orient.server.OSystemDatabase.init(OSystemDatabase.java:155)\\r\\n        at com.orientechnologies.orient.server.OSystemDatabase.<init>(OSystemDatabase.java:50)\\r\\n        at com.orientechnologies.orient.server.OServer.initSystemDatabase(OServer.java:1276)\\r\\n        at com.orientechnologies.orient.server.OServer.activate(OServer.java:444)\\r\\n        at com.orientechnologies.orient.server.OServerMain$1.run(OServerMain.java:49)\\r\\nException `480B7195` in storage `plocal:/ai/orientdb-3.1.2/databases/OSystem`: 3.1.2 - Veloce (build 555d4a1824fdc980c2c2a680a840d68fc03f3b5e, branch 3.1.x)\\r\\njava.lang.NoClassDefFoundError: Could not initialize class com.kenai.jffi.MemoryIO$SingletonHolder\\r\\n        at com.kenai.jffi.MemoryIO.getInstance(MemoryIO.java:64)\\r\\n        at com.orientechnologies.common.directmemory.ODirectMemoryAllocator.allocate(ODirectMemoryAllocator.java:123)\\r\\n        at com.orientechnologies.orient.core.storage.impl.local.paginated.wal.cas.CASDiskWriteAheadLog.<init>(CASDiskWriteAheadLog.java:393)\\r\\n        at com.orientechnologies.orient.core.storage.disk.OLocalPaginatedStorage.initWalAndDiskCache(OLocalPaginatedStorage.java:786)\\r\\n        at com.orientechnologies.orient.core.storage.impl.local.OAbstractPaginatedStorage.create(OAbstractPaginatedStorage.java:731)\\r\\n        at com.orientechnologies.orient.core.storage.disk.OLocalPaginatedStorage.create(OLocalPaginatedStorage.java:207)\\r\\n        at com.orientechnologies.orient.core.db.OrientDBEmbedded.internalCreate(OrientDBEmbedded.java:743)\\r\\n        at com.orientechnologies.orient.core.db.OrientDBEmbedded.create(OrientDBEmbedded.java:635)\\r\\n        at com.orientechnologies.orient.server.OSystemDatabase.init(OSystemDatabase.java:155)\\r\\n        at com.orientechnologies.orient.server.OSystemDatabase.<init>(OSystemDatabase.java:50)\\r\\n        at com.orientechnologies.orient.server.OServer.initSystemDatabase(OServer.java:1276)\\r\\n        at com.orientechnologies.orient.server.OServer.activate(OServer.java:444)\\r\\n        at com.orientechnologies.orient.server.OServerMain$1.run(OServerMain.java:49)\\r\\nException in thread \"Thread-0\" java.lang.NoClassDefFoundError: Could not initialize class com.kenai.jffi.MemoryIO$SingletonHolder\\r\\n        at com.kenai.jffi.MemoryIO.getInstance(MemoryIO.java:64)\\r\\n        at com.orientechnologies.common.directmemory.ODirectMemoryAllocator.allocate(ODirectMemoryAllocator.java:123)\\r\\n        at com.orientechnologies.orient.core.storage.impl.local.paginated.wal.cas.CASDiskWriteAheadLog.<init>(CASDiskWriteAheadLog.java:393)\\r\\n        at com.orientechnologies.orient.core.storage.disk.OLocalPaginatedStorage.initWalAndDiskCache(OLocalPaginatedStorage.java:786)\\r\\n        at com.orientechnologies.orient.core.storage.impl.local.OAbstractPaginatedStorage.create(OAbstractPaginatedStorage.java:731)\\r\\n        at com.orientechnologies.orient.core.storage.disk.OLocalPaginatedStorage.create(OLocalPaginatedStorage.java:207)\\r\\n        at com.orientechnologies.orient.core.db.OrientDBEmbedded.internalCreate(OrientDBEmbedded.java:743)\\r\\n        at com.orientechnologies.orient.core.db.OrientDBEmbedded.create(OrientDBEmbedded.java:635)\\r\\n        at com.orientechnologies.orient.server.OSystemDatabase.init(OSystemDatabase.java:155)\\r\\n        at com.orientechnologies.orient.server.OSystemDatabase.<init>(OSystemDatabase.java:50)\\r\\n        at com.orientechnologies.orient.server.OServer.initSystemDatabase(OServer.java:1276)\\r\\n        at com.orientechnologies.orient.server.OServer.activate(OServer.java:444)\\r\\n        at com.orientechnologies.orient.server.OServerMain$1.run(OServerMain.java:49)\\r\\n\\r\\n## Steps to reproduce  \\r\\ncd {orientdb}/bin\\r\\n./server.sh\\r\\n\\r\\n",
          "issue_comments": []
        },
        {
          "issue_url": "https://github.com/orientechnologies/orientdb/issues/9394",
          "issue_title": "Error on reconnecting to distributed database",
          "issue_number": 9394,
          "issue_text": "### OrientDB Version: 3.1.2, running in Docker based on official Docker image\r\n### Java Version: 1.8.0_265\r\n### OS: Docker host is SLES-12\r\n\r\n## Expected behavior  \r\nThe clients can reconnect to a distributed database with a connection string specifying all the hostsnames.\r\n\r\n## Actual behavior  \r\nWe have three Docker hosts, and on each one there is a container running OrientDB. These are all part of the same cluster, which works fine.\r\nThe client connects to the servers using the code:\r\n`new OrientGraphFactory(\"remote:docker01;docker02;docker03/portal \"user\", \"pw\").setupPool(1, 50)`\r\nThis also works fine.\r\n\r\nHowever, after a while of inactivity, the connection from the client to the server goes stale or times out. And the client is not able to reconnect to the server.\r\n\r\nWe see the following error message in the client log:\r\n`2020-09-16 18:18:43.169  INFO 16 --- [nio-8080-exec-2] c.o.orient.client.remote.OStorageRemote  : Caught Network I/O errors on docker03:2424/portal, trying an automatic reconnection... (error: null)\r\n2020-09-16 18:18:43.209 ERROR 16 --- [nio-8080-exec-2] n.k.b.p.rest.controller.BildeController  : Feilmelding: null\r\ncom.orientechnologies.common.io.OIOException: null\r\n\tat com.orientechnologies.orient.client.remote.OStorageRemote.baseNetworkOperation(OStorageRemote.java:497) ~[orientdb-client-3.1.2.jar!/:3.1.2]\r\n\tat com.orientechnologies.orient.client.remote.OStorageRemote.networkOperationRetryTimeout(OStorageRemote.java:365) ~[orientdb-client-3.1.2.jar!/:3.1.2]\r\n\tat com.orientechnologies.orient.client.remote.OStorageRemote.networkOperation(OStorageRemote.java:405) ~[orientdb-client-3.1.2.jar!/:3.1.2]\r\n\tat com.orientechnologies.orient.client.remote.OStorageRemote.command(OStorageRemote.java:1095) ~[orientdb-client-3.1.2.jar!/:3.1.2]\r\n\tat com.orientechnologies.orient.core.sql.query.OSQLQuery.run(OSQLQuery.java:72) ~[orientdb-core-3.1.2.jar!/:3.1.2]\r\n\tat com.orientechnologies.orient.core.sql.query.OSQLAsynchQuery.run(OSQLAsynchQuery.java:76) ~[orientdb-core-3.1.2.jar!/:3.1.2]\r\n\tat com.orientechnologies.orient.core.sql.query.OSQLSynchQuery.run(OSQLSynchQuery.java:84) ~[orientdb-core-3.1.2.jar!/:3.1.2]\r\n\tat com.orientechnologies.orient.core.query.OQueryAbstract.execute(OQueryAbstract.java:34) ~[orientdb-core-3.1.2.jar!/:3.1.2]\r\n\tat com.orientechnologies.orient.core.db.document.ODatabaseDocumentAbstract.query(ODatabaseDocumentAbstract.java:358) ~[orientdb-core-3.1.2.jar!/:3.1.2]\r\n\tat com.orientechnologies.orient.core.db.document.ODatabaseDocumentTx.query(ODatabaseDocumentTx.java:769) ~[orientdb-core-3.1.2.jar!/:3.1.2]\r\n\tat com.tinkerpop.blueprints.impls.orient.OrientGraphQuery.vertices(OrientGraphQuery.java:206) ~[orientdb-graphdb-3.1.2.jar!/:3.1.2]\r\n\tat com.tinkerpop.blueprints.impls.orient.OrientBaseGraph.getVertices(OrientBaseGraph.java:860) ~[orientdb-graphdb-3.1.2.jar!/:3.1.2]\r\n\tat no.kommune.bergen.plattform.transaksjon.TransactionWrapper.findByProperty(TransactionWrapper.java:130) ~[domeneplattform-12.0-SNAPSHOT.jar!/:na]\r\n\tat no.kommune.bergen.plattform.transaksjon.TransactionWrapper.findById(TransactionWrapper.java:138) ~[domeneplattform-12.0-SNAPSHOT.jar!/:na]\r\n\tat no.kommune.bergen.portal.rest.controller.BildeController.get(BildeController.java:53) ~[classes!/:na]\r\n\tat sun.reflect.GeneratedMethodAccessor46.invoke(Unknown Source) ~[na:na]\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[na:1.8.0_212]\r\n\tat java.lang.reflect.Method.invoke(Method.java:498) ~[na:1.8.0_212]\r\n\tat org.springframework.web.method.support.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:190) ~[spring-web-5.1.7.RELEASE.jar!/:5.1.7.RELEASE]\r\n\tat org.springframework.web.method.support.InvocableHandlerMethod.invokeForRequest(InvocableHandlerMethod.java:138) ~[spring-web-5.1.7.RELEASE.jar!/:5.1.7.RELEASE]\r\n\tat org.springframework.web.servlet.mvc.method.annotation.ServletInvocableHandlerMethod.invokeAndHandle(ServletInvocableHandlerMethod.java:104) ~[spring-webmvc-5.1.7.RELEASE.jar!/:5.1.7.RELEASE]\r\n\tat org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.invokeHandlerMethod(RequestMappingHandlerAdapter.java:892) ~[spring-webmvc-5.1.7.RELEASE.jar!/:5.1.7.RELEASE]\r\n\tat org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.handleInternal(RequestMappingHandlerAdapter.java:797) ~[spring-webmvc-5.1.7.RELEASE.jar!/:5.1.7.RELEASE]\r\n\tat org.springframework.web.servlet.mvc.method.AbstractHandlerMethodAdapter.handle(AbstractHandlerMethodAdapter.java:87) ~[spring-webmvc-5.1.7.RELEASE.jar!/:5.1.7.RELEASE]\r\n\tat org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:1039) ~[spring-webmvc-5.1.7.RELEASE.jar!/:5.1.7.RELEASE]\r\n\tat org.springframework.web.servlet.DispatcherServlet.doService(DispatcherServlet.java:942) ~[spring-webmvc-5.1.7.RELEASE.jar!/:5.1.7.RELEASE]\r\n\tat org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:1005) ~[spring-webmvc-5.1.7.RELEASE.jar!/:5.1.7.RELEASE]\r\n\tat org.springframework.web.servlet.FrameworkServlet.doGet(FrameworkServlet.java:897) ~[spring-webmvc-5.1.7.RELEASE.jar!/:5.1.7.RELEASE]\r\n\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:634) [tomcat-embed-core-9.0.19.jar!/:9.0.19]\r\n\tat org.springframework.web.servlet.FrameworkServlet.service(FrameworkServlet.java:882) ~[spring-webmvc-5.1.7.RELEASE.jar!/:5.1.7.RELEASE]\r\n\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:741) [tomcat-embed-core-9.0.19.jar!/:9.0.19]\r\n\tat org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:231) [tomcat-embed-core-9.0.19.jar!/:9.0.19]\r\n\tat org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:166) [tomcat-embed-core-9.0.19.jar!/:9.0.19]\r\n\tat org.apache.tomcat.websocket.server.WsFilter.doFilter(WsFilter.java:53) [tomcat-embed-websocket-9.0.19.jar!/:9.0.19]\r\n\tat org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:193) [tomcat-embed-core-9.0.19.jar!/:9.0.19]\r\n\tat org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:166) [tomcat-embed-core-9.0.19.jar!/:9.0.19]\r\n\tat no.kommune.bergen.portal.filter.SiteFilter.doFilter(SiteFilter.java:47) [portal-domene-12.0-SNAPSHOT.jar!/:na]\r\n\tat org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:193) [tomcat-embed-core-9.0.19.jar!/:9.0.19]\r\n\tat org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:166) [tomcat-embed-core-9.0.19.jar!/:9.0.19]\r\n\tat no.kommune.bergen.portal.filter.TransactionFilter.doFilter(TransactionFilter.java:33) [portal-domene-12.0-SNAPSHOT.jar!/:na]\r\n\tat org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:193) [tomcat-embed-core-9.0.19.jar!/:9.0.19]\r\n\tat org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:166) [tomcat-embed-core-9.0.19.jar!/:9.0.19]\r\n\tat org.springframework.web.filter.RequestContextFilter.doFilterInternal(RequestContextFilter.java:99) [spring-web-5.1.7.RELEASE.jar!/:5.1.7.RELEASE]\r\n\tat org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107) [spring-web-5.1.7.RELEASE.jar!/:5.1.7.RELEASE]\r\n\tat org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:193) [tomcat-embed-core-9.0.19.jar!/:9.0.19]\r\n\tat org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:166) [tomcat-embed-core-9.0.19.jar!/:9.0.19]\r\n\tat org.springframework.web.filter.FormContentFilter.doFilterInternal(FormContentFilter.java:92) [spring-web-5.1.7.RELEASE.jar!/:5.1.7.RELEASE]\r\n\tat org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107) [spring-web-5.1.7.RELEASE.jar!/:5.1.7.RELEASE]\r\n\tat org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:193) [tomcat-embed-core-9.0.19.jar!/:9.0.19]\r\n\tat org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:166) [tomcat-embed-core-9.0.19.jar!/:9.0.19]\r\n\tat org.springframework.web.filter.HiddenHttpMethodFilter.doFilterInternal(HiddenHttpMethodFilter.java:93) [spring-web-5.1.7.RELEASE.jar!/:5.1.7.RELEASE]\r\n\tat org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107) [spring-web-5.1.7.RELEASE.jar!/:5.1.7.RELEASE]\r\n\tat org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:193) [tomcat-embed-core-9.0.19.jar!/:9.0.19]\r\n\tat org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:166) [tomcat-embed-core-9.0.19.jar!/:9.0.19]\r\n\tat org.springframework.web.filter.CharacterEncodingFilter.doFilterInternal(CharacterEncodingFilter.java:200) [spring-web-5.1.7.RELEASE.jar!/:5.1.7.RELEASE]\r\n\tat org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107) [spring-web-5.1.7.RELEASE.jar!/:5.1.7.RELEASE]\r\n\tat org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:193) [tomcat-embed-core-9.0.19.jar!/:9.0.19]\r\n\tat org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:166) [tomcat-embed-core-9.0.19.jar!/:9.0.19]\r\n\tat org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:200) [tomcat-embed-core-9.0.19.jar!/:9.0.19]\r\n\tat org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:96) [tomcat-embed-core-9.0.19.jar!/:9.0.19]\r\n\tat org.apache.catalina.authenticator.AuthenticatorBase.invoke(AuthenticatorBase.java:490) [tomcat-embed-core-9.0.19.jar!/:9.0.19]\r\n\tat org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:139) [tomcat-embed-core-9.0.19.jar!/:9.0.19]\r\n\tat org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:92) [tomcat-embed-core-9.0.19.jar!/:9.0.19]\r\n\tat org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:74) [tomcat-embed-core-9.0.19.jar!/:9.0.19]\r\n\tat org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:343) [tomcat-embed-core-9.0.19.jar!/:9.0.19]\r\n\tat org.apache.coyote.http11.Http11Processor.service(Http11Processor.java:408) [tomcat-embed-core-9.0.19.jar!/:9.0.19]\r\n\tat org.apache.coyote.AbstractProcessorLight.process(AbstractProcessorLight.java:66) [tomcat-embed-core-9.0.19.jar!/:9.0.19]\r\n\tat org.apache.coyote.AbstractProtocol$ConnectionHandler.process(AbstractProtocol.java:836) [tomcat-embed-core-9.0.19.jar!/:9.0.19]\r\n\tat org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.doRun(NioEndpoint.java:1747) [tomcat-embed-core-9.0.19.jar!/:9.0.19]\r\n\tat org.apache.tomcat.util.net.SocketProcessorBase.run(SocketProcessorBase.java:49) [tomcat-embed-core-9.0.19.jar!/:9.0.19]\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [na:1.8.0_212]\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [na:1.8.0_212]\r\n\tat org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run(TaskThread.java:61) [tomcat-embed-core-9.0.19.jar!/:9.0.19]\r\n\tat java.lang.Thread.run(Thread.java:748) [na:1.8.0_212]\r\nCaused by: java.io.EOFException: null\r\n\tat java.io.DataInputStream.readByte(DataInputStream.java:267) ~[na:1.8.0_212]\r\n\tat com.orientechnologies.orient.enterprise.channel.binary.OChannelBinary.readByte(OChannelBinary.java:78) ~[orientdb-core-3.1.2.jar!/:3.1.2]\r\n\tat com.orientechnologies.orient.client.binary.OChannelBinaryAsynchClient.beginResponse(OChannelBinaryAsynchClient.java:185) ~[orientdb-client-3.1.2.jar!/:3.1.2]\r\n\tat com.orientechnologies.orient.client.binary.OChannelBinaryAsynchClient.beginResponse(OChannelBinaryAsynchClient.java:169) ~[orientdb-client-3.1.2.jar!/:3.1.2]\r\n\tat com.orientechnologies.orient.client.remote.OStorageRemote.beginResponse(OStorageRemote.java:2221) ~[orientdb-client-3.1.2.jar!/:3.1.2]\r\n\tat com.orientechnologies.orient.client.remote.OStorageRemote.lambda$networkOperationRetryTimeout$2(OStorageRemote.java:385) ~[orientdb-client-3.1.2.jar!/:3.1.2]\r\n\tat com.orientechnologies.orient.client.remote.OStorageRemote.baseNetworkOperation(OStorageRemote.java:447) ~[orientdb-client-3.1.2.jar!/:3.1.2]\r\n\t... 73 common frames omitted`\r\n\r\nAnd the following error in the server log:\r\n`Error on unmarshalling content. Class: q\r\ncom.orientechnologies.orient.core.exception.ODatabaseException: The database instance is not set in the current thread. Be sure to set it with: ODatabaseRecordThreadLocal.instance().set(db);\r\n\tat com.orientechnologies.orient.core.db.ODatabaseRecordThreadLocal.get(ODatabaseRecordThreadLocal.java:61)\r\n\tat com.orientechnologies.orient.core.sql.query.OSQLQuery.deserializeQueryParameters(OSQLQuery.java:147)\r\n\tat com.orientechnologies.orient.core.sql.query.OSQLQuery.queryFromStream(OSQLQuery.java:137)\r\n\tat com.orientechnologies.orient.core.sql.query.OSQLSynchQuery.queryFromStream(OSQLSynchQuery.java:147)\r\n\tat com.orientechnologies.orient.core.sql.query.OSQLQuery.fromStream(OSQLQuery.java:109)\r\n\tat com.orientechnologies.orient.core.serialization.serializer.stream.OStreamSerializerAnyStreamable.fromStream(OStreamSerializerAnyStreamable.java:86)\r\n\tat com.orientechnologies.orient.client.remote.message.OCommandRequest.read(OCommandRequest.java:70)\r\n\tat com.orientechnologies.orient.server.network.protocol.binary.ONetworkProtocolBinary.sessionRequest(ONetworkProtocolBinary.java:319)\r\n\tat com.orientechnologies.orient.server.network.protocol.binary.ONetworkProtocolBinary.execute(ONetworkProtocolBinary.java:239)\r\n\tat com.orientechnologies.common.thread.OSoftThread.run(OSoftThread.java:67)\r\nError reading request\r\ncom.orientechnologies.orient.core.exception.OSerializationException: Error on unmarshalling content. Class: q\r\n\tat com.orientechnologies.orient.core.serialization.serializer.stream.OStreamSerializerAnyStreamable.fromStream(OStreamSerializerAnyStreamable.java:92)\r\n\tat com.orientechnologies.orient.client.remote.message.OCommandRequest.read(OCommandRequest.java:70)\r\n\tat com.orientechnologies.orient.server.network.protocol.binary.ONetworkProtocolBinary.sessionRequest(ONetworkProtocolBinary.java:319)\r\n\tat com.orientechnologies.orient.server.network.protocol.binary.ONetworkProtocolBinary.execute(ONetworkProtocolBinary.java:239)\r\n\tat com.orientechnologies.common.thread.OSoftThread.run(OSoftThread.java:67)\r\nCaused by: com.orientechnologies.orient.core.exception.ODatabaseException: The database instance is not set in the current thread. Be sure to set it with: ODatabaseRecordThreadLocal.instance().set(db);\r\n\tat com.orientechnologies.orient.core.db.ODatabaseRecordThreadLocal.get(ODatabaseRecordThreadLocal.java:61)\r\n\tat com.orientechnologies.orient.core.sql.query.OSQLQuery.deserializeQueryParameters(OSQLQuery.java:147)\r\n\tat com.orientechnologies.orient.core.sql.query.OSQLQuery.queryFromStream(OSQLQuery.java:137)\r\n\tat com.orientechnologies.orient.core.sql.query.OSQLSynchQuery.queryFromStream(OSQLSynchQuery.java:147)\r\n\tat com.orientechnologies.orient.core.sql.query.OSQLQuery.fromStream(OSQLQuery.java:109)\r\n\tat com.orientechnologies.orient.core.serialization.serializer.stream.OStreamSerializerAnyStreamable.fromStream(OStreamSerializerAnyStreamable.java:86)\r\n\t... 4 more\r\n`\r\n\r\n## Steps to reproduce  \r\n\r\n1. Set up a distributed database with three nodes.\r\n2. Connect using all the hostsnames in the connectionstring.\r\n3. Wait for the connection to time out.\r\n4. Attempt reconnection.\r\n\r\n \r\n\r\n",
          "issue_comments": []
        },
        {
          "issue_url": "https://github.com/orientechnologies/orientdb/issues/9393",
          "issue_title": "I need help, how to decode orientBinaryObject??",
          "issue_number": 9393,
          "issue_text": "\r\nWhen I use pyorientdb.commend() to query the edge of the class, the returned result encapsulates the edge into orientBinaryObject. How can I get the edge rid?",
          "issue_comments": []
        },
        {
          "issue_url": "https://github.com/orientechnologies/orientdb/issues/9392",
          "issue_title": "Couldn't insert N number of edges in orientDB",
          "issue_number": 9392,
          "issue_text": "### OrientDB Version: 3.1.2  \r\n### Java Version: 8\r\n### OS: windows  \r\n\r\n## Expected behavior  \r\nadd N of edges\r\n\r\n## Actual behavior  \r\n````\r\nINFO: Windows OS is detected, 262144 limit of open files will be set for the disk cache.\r\nSep 14, 2020 10:06:56 AM com.orientechnologies.common.log.OLogManager log\r\nSEVERE: Error on transaction commit `2E3CD732`\r\ncom.orientechnologies.orient.core.exception.OStorageException: Exception during execution of component operation inside of storage healthEmptyDB\r\n    DB name=\"healthEmptyDB\"\r\n    DB name=\"healthEmptyDB\"\r\n    at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\r\n    at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\r\n    at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\r\n    at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)\r\n    at com.orientechnologies.orient.client.binary.OChannelBinaryAsynchClient.handleException(OChannelBinaryAsynchClient.java:357)\r\n    at com.orientechnologies.orient.client.binary.OChannelBinaryAsynchClient.handleStatus(OChannelBinaryAsynchClient.java:305)\r\n    at com.orientechnologies.orient.client.binary.OChannelBinaryAsynchClient.handleStatus(OChannelBinaryAsynchClient.java:327)\r\n    at com.orientechnologies.orient.client.binary.OChannelBinaryAsynchClient.beginResponse(OChannelBinaryAsynchClient.java:211)\r\n    at com.orientechnologies.orient.client.binary.OChannelBinaryAsynchClient.beginResponse(OChannelBinaryAsynchClient.java:169)\r\n    at com.orientechnologies.orient.client.remote.OStorageRemote.beginResponse(OStorageRemote.java:2221)\r\n    at com.orientechnologies.orient.client.remote.OStorageRemote.lambda$networkOperationRetryTimeout$2(OStorageRemote.java:385)\r\n    at com.orientechnologies.orient.client.remote.OStorageRemote.baseNetworkOperation(OStorageRemote.java:447)\r\n    at com.orientechnologies.orient.client.remote.OStorageRemote.networkOperationRetryTimeout(OStorageRemote.java:365)\r\n    at com.orientechnologies.orient.client.remote.OStorageRemote.networkOperationNoRetry(OStorageRemote.java:400)\r\n    at com.orientechnologies.orient.client.remote.OStorageRemote.commit(OStorageRemote.java:1304)\r\n    at com.orientechnologies.orient.core.db.document.ODatabaseDocumentAbstract.internalCommit(ODatabaseDocumentAbstract.java:2466)\r\n    at com.orientechnologies.orient.core.tx.OTransactionOptimistic.doCommit(OTransactionOptimistic.java:602)\r\n    at com.orientechnologies.orient.core.tx.OTransactionOptimistic.commit(OTransactionOptimistic.java:106)\r\n    at com.orientechnologies.orient.core.db.document.ODatabaseDocumentAbstract.commit(ODatabaseDocumentAbstract.java:1984)\r\n    at com.orientechnologies.orient.core.db.document.ODatabaseDocumentAbstract.commit(ODatabaseDocumentAbstract.java:1954)\r\n    at com.orientechnologies.orient.core.db.document.ODatabaseDocumentTx.commit(ODatabaseDocumentTx.java:741)\r\n    at com.tinkerpop.blueprints.impls.orient.OrientTransactionalGraph.commit(OrientTransactionalGraph.java:183)\r\n    at graph.orient.utils.InsertNEdges.commitGraph(InsertNEdges.java:73)\r\n    at graph.orient.utils.InsertNEdges.main(InsertNEdges.java:66)\r\n    Suppressed: com.orientechnologies.orient.core.exception.OStorageException: Exception during execution of component operation inside of storage healthEmptyDB\r\n    DB name=\"healthEmptyDB\"\r\n        at com.orientechnologies.orient.core.storage.impl.local.paginated.atomicoperations.OAtomicOperationsManager.executeInsideComponentOperation(OAtomicOperationsManager.java:218)\r\n        at com.orientechnologies.orient.core.storage.impl.local.paginated.atomicoperations.OAtomicOperationsManager.executeInsideComponentOperation(OAtomicOperationsManager.java:205)\r\n        at com.orientechnologies.orient.core.storage.impl.local.paginated.base.ODurableComponent.executeInsideComponentOperation(ODurableComponent.java:102)\r\n        at com.orientechnologies.orient.core.storage.index.sbtreebonsai.local.OSBTreeBonsaiLocal.create(OSBTreeBonsaiLocal.java:109)\r\n        at com.orientechnologies.orient.core.storage.ridbag.sbtree.OSBTreeCollectionManagerShared.createEdgeTree(OSBTreeCollectionManagerShared.java:139)\r\n        at com.orientechnologies.orient.core.storage.ridbag.sbtree.OSBTreeCollectionManagerShared.createEdgeTree(OSBTreeCollectionManagerShared.java:47)\r\n        at com.orientechnologies.orient.core.storage.ridbag.sbtree.OSBTreeCollectionManagerAbstract.createSBTree(OSBTreeCollectionManagerAbstract.java:175)\r\n        at com.orientechnologies.orient.core.storage.ridbag.sbtree.OSBTreeCollectionManagerShared.createSBTree(OSBTreeCollectionManagerShared.java:197)\r\n        at com.orientechnologies.orient.core.serialization.serializer.record.binary.HelperClasses.writeSBTreeRidbag(HelperClasses.java:411)\r\n        at com.orientechnologies.orient.core.serialization.serializer.record.binary.HelperClasses.writeRidBag(HelperClasses.java:356)\r\n        at com.orientechnologies.orient.core.serialization.serializer.record.binary.ORecordSerializerBinaryV1.writeRidBag(ORecordSerializerBinaryV1.java:731)\r\n        at com.orientechnologies.orient.core.serialization.serializer.record.binary.ORecordSerializerBinaryV1.serializeValue(ORecordSerializerBinaryV1.java:1020)\r\n        at com.orientechnologies.orient.core.serialization.serializer.record.binary.ORecordSerializerBinaryV1.serializeValues(ORecordSerializerBinaryV1.java:404)\r\n        at com.orientechnologies.orient.core.serialization.serializer.record.binary.ORecordSerializerBinaryV1.serializeDocument(ORecordSerializerBinaryV1.java:462)\r\n        at com.orientechnologies.orient.core.serialization.serializer.record.binary.ORecordSerializerBinaryV1.serialize(ORecordSerializerBinaryV1.java:483)\r\n        at com.orientechnologies.orient.core.serialization.serializer.record.binary.ORecordSerializerBinary.toStream(ORecordSerializerBinary.java:132)\r\n        at com.orientechnologies.orient.core.storage.impl.local.OAbstractPaginatedStorage.commitEntry(OAbstractPaginatedStorage.java:6012)\r\n        at com.orientechnologies.orient.core.storage.impl.local.OAbstractPaginatedStorage.commit(OAbstractPaginatedStorage.java:2427)\r\n        at com.orientechnologies.orient.core.storage.impl.local.OAbstractPaginatedStorage.commit(OAbstractPaginatedStorage.java:2241)\r\n        at com.orientechnologies.orient.core.db.document.ODatabaseDocumentAbstract.internalCommit(ODatabaseDocumentAbstract.java:2466)\r\n        at com.orientechnologies.orient.core.tx.OTransactionOptimistic.doCommit(OTransactionOptimistic.java:602)\r\n        at com.orientechnologies.orient.core.tx.OTransactionOptimistic.commit(OTransactionOptimistic.java:106)\r\n        at com.orientechnologies.orient.core.db.document.ODatabaseDocumentAbstract.commit(ODatabaseDocumentAbstract.java:1984)\r\n        at com.orientechnologies.orient.core.db.document.ODatabaseDocumentAbstract.commit(ODatabaseDocumentAbstract.java:1954)\r\n        at com.orientechnologies.orient.server.OConnectionBinaryExecutor.executeCommit38(OConnectionBinaryExecutor.java:1698)\r\n        at com.orientechnologies.orient.client.remote.message.OCommit38Request.execute(OCommit38Request.java:139)\r\n        at com.orientechnologies.orient.server.network.protocol.binary.ONetworkProtocolBinary.sessionRequest(ONetworkProtocolBinary.java:354)\r\n        at com.orientechnologies.orient.server.network.protocol.binary.ONetworkProtocolBinary.execute(ONetworkProtocolBinary.java:238)\r\n        at com.orientechnologies.common.thread.OSoftThread.run(OSoftThread.java:67)\r\n    Caused by: com.orientechnologies.orient.core.exception.OStorageException: Ridbag component with name collections_22.sbc does not exist\r\n    DB name=\"healthEmptyDB\"\r\n        at com.orientechnologies.orient.core.storage.index.sbtreebonsai.local.OSBTreeBonsaiLocal.lambda$create$1(OSBTreeBonsaiLocal.java:121)\r\n        at com.orientechnologies.orient.core.storage.impl.local.paginated.atomicoperations.OAtomicOperationsManager.executeInsideComponentOperation(OAtomicOperationsManager.java:213)\r\n        ... 28 more\r\n    [CIRCULAR REFERENCE:com.orientechnologies.orient.core.exception.OStorageException: Ridbag component with name collections_22.sbc does not exist\r\n    DB name=\"healthEmptyDB\"]\r\n\r\ncom.orientechnologies.orient.core.exception.OStorageException: Exception during execution of component operation inside of storage healthEmptyDB\r\n````\r\n\r\n## Steps to reproduce  \r\n<add here>  \r\n<An SQL script to reproduce the problem or a JUnit test case will increase **A LOT** the chance to have a quick fix>\r\n\r\n",
          "issue_comments": [
            {
              "comment_username": "laa",
              "comment_create_time": "2020-09-15T10:24:19Z",
              "comment_edit_time": "2020-09-15T10:24:19Z",
              "comment_text": "Hi @naseem91 . I will provide a fix in a couple of weeks."
            },
            {
              "comment_username": "naseem91",
              "comment_create_time": "2020-09-15T10:40:28Z",
              "comment_edit_time": "2020-09-15T10:40:28Z",
              "comment_text": "thanks "
            }
          ]
        },
        {
          "issue_url": "https://github.com/orientechnologies/orientdb/issues/9391",
          "issue_title": "File with name config.cd does not exist in <dbname>",
          "issue_number": 9391,
          "issue_text": "### OrientDB Version: 3.0.27\r\n### Java Version: 11 \r\n### OS: Ubuntu \r\n\r\n## Expected behavior  \r\nDatabase should be accessible after orientdb starts without exceptions or at least should be exported in case of disaster recovery  \r\n\r\n## Actual behavior  \r\nOrientDB is starting correctly, but database is not opening because of which all of our data is lost (in corrupted db files) and not able to run exports and I'm not able to connect to db.\r\n\r\n### Stacktrace\r\n```\r\n2020-09-15 00:11:46:972 INFO  OrientDB Studio available at http://192.168.1.50:2480/studio/index.html [OServer]\r\n2020-09-15 00:11:46:972 INFO  OrientDB Server is active v3.0.27 - Veloce (build bc8f77600494cab71fb347d4c29f71c793442464, branch 3.0.x). [OServer]\r\n2020-09-15 00:12:00:487 INFO  Page size for WAL located in /path/to/orientdb-3.0.27/databases/mydb is set to 4096 bytes. [OCASDiskWriteAheadLog]Exception `32A10CAB` in storage `plocal:/path/to/orientdb-3.0.27/databases/mydb`: 3.0.27 - Veloce (build bc8f77600494cab71fb347d4c29f71c793442464, branch 3.0.x)\r\ncom.orientechnologies.orient.core.exception.OStorageException: File with name config.cd does not exist in storage mydb\r\n\tat com.orientechnologies.orient.core.storage.cache.local.OWOWCache.loadFile(OWOWCache.java:732)\r\n\tat com.orientechnologies.orient.core.storage.impl.local.paginated.base.ODurableComponent.openFile(ODurableComponent.java:192)\r\n\tat com.orientechnologies.orient.core.storage.cluster.v1.OPaginatedClusterV1.open(OPaginatedClusterV1.java:204)\r\n\tat com.orientechnologies.orient.core.storage.config.OClusterBasedStorageConfiguration.load(OClusterBasedStorageConfiguration.java:176)\r\n\tat com.orientechnologies.orient.core.storage.disk.OLocalPaginatedStorage.initConfiguration(OLocalPaginatedStorage.java:455)\r\n\tat com.orientechnologies.orient.core.storage.impl.local.OAbstractPaginatedStorage.open(OAbstractPaginatedStorage.java:306)\r\n\tat com.orientechnologies.orient.core.db.OrientDBEmbedded.openNoAuthenticate(OrientDBEmbedded.java:238)\r\n\tat com.orientechnologies.orient.core.db.OrientDBEmbedded.openNoAuthenticate(OrientDBEmbedded.java:56)\r\n\tat com.orientechnologies.orient.server.OServer.openDatabase(OServer.java:936)\r\n\tat com.orientechnologies.orient.server.OServer.openDatabase(OServer.java:913)\r\n\tat com.orientechnologies.orient.server.OConnectionBinaryExecutor.executeDatabaseOpen37(OConnectionBinaryExecutor.java:1187)\r\n\tat com.orientechnologies.orient.client.remote.message.OOpen37Request.execute(OOpen37Request.java:77)\r\n\tat com.orientechnologies.orient.server.network.protocol.binary.ONetworkProtocolBinary.sessionRequest(ONetworkProtocolBinary.java:310)\r\n\tat com.orientechnologies.orient.server.network.protocol.binary.ONetworkProtocolBinary.execute(ONetworkProtocolBinary.java:212)\r\n\tat com.orientechnologies.common.thread.OSoftThread.run(OSoftThread.java:69)\r\n\r\n```\r\n\r\nConsole output while trying to connect to db\r\n```\r\nConnecting to database [remote:localhost/mydb] with user 'root'...\r\nError: com.orientechnologies.orient.core.exception.ODatabaseException: Cannot open database 'mydb'\r\n\r\nError: com.orientechnologies.orient.core.exception.ODatabaseException: Cannot open database 'mydb'\r\n\r\nError: com.orientechnologies.orient.core.exception.OStorageException: File with name config.cd does not exist in storage mydb\r\n\r\norientdb {server=remote:localhost/mydb}> \r\n```",
          "issue_comments": []
        },
        {
          "issue_url": "https://github.com/orientechnologies/orientdb/issues/9390",
          "issue_title": "Create much more compact version of presentation of ridbags.",
          "issue_number": 9390,
          "issue_text": "The current version of the presentation of rid bags consumes a lot of unused space which causes problems with space consumption. It is proposed to introduce a tree with more compact structure to avoid those problems. \r\n\r\n",
          "issue_comments": []
        },
        {
          "issue_url": "https://github.com/orientechnologies/orientdb/issues/9387",
          "issue_title": "Stop accepting new requests if limit of memory consumption is reached.",
          "issue_number": 9387,
          "issue_text": "Allow user requests to be processed only if consumption of all memory pools on a server node is less then given threshold. That will minimize cases when a server is not responsive because of the massive amount of work which it has complete and partially solves the issue with quorum not reached on a distributed cluster. To control the threshold value new parameter - \"server.heapUsageLimit\" will be introduced. This parameter will control the maximum level of heap consumption.\r\n\r\n",
          "issue_comments": [
            {
              "comment_username": "laa",
              "comment_create_time": "2020-09-08T09:25:17Z",
              "comment_edit_time": "2020-09-08T09:25:17Z",
              "comment_text": "Fixed."
            }
          ]
        },
        {
          "issue_url": "https://github.com/orientechnologies/orientdb/issues/9385",
          "issue_title": "NullPointerException on expand",
          "issue_number": 9385,
          "issue_text": "### OrientDB Version: 3.0.29\\r\\n### Java Version: Java 8\\r\\n### OS: Windows 10 pro (also reproduced on linux)\\r\\n\\r\\n## Expected behavior  \\r\\nWhen doing an expand(out()) on a vertex, known vertices should be retrieved, unknown ones (where i don't have Read permission) should be ignored.\\r\\n\\r\\n\\r\\n## Actual behavior  \\r\\nWhen doing an expand(out()) on a vertex, if there is vertices i don't know, OrientDb throws a NullPointerException\\r\\n\\r\\n\\r\\n## Steps to reproduce  \\r\\n\\r\\nOn an empty database:\\r\\n\\r\\n-I create 3 vertices: \"A\",\"B\" and \"C\"\\r\\n\\r\\n-I create a new record A named a\\r\\n-I create a new record B named b\\r\\n-I create a new record C named c\\r\\n\\r\\n\\r\\n-I put the vertices ORestricted (ALTER CLASS V SUPERCLASS ORestricted)\\r\\n\\r\\n-I create an Edge \"link\"\\r\\n\\r\\n-I create a relation \"link\" between a and b\\r\\n-I create a relation \"link\" between a and c\\r\\n\\r\\n-I create a use \"testUser\" with an Orole \"Default\" inherited from writer.\\r\\nThis Default role has the following permissions:\\r\\n\\r\\ndatabase.class.classname: CRUD\\r\\ndatabase.cluster.classname: CRUD\\r\\ndatabase.systemclusters: CRUD\\r\\n\\r\\n![image](https://user-images.githubusercontent.com/70526441/91731610-536dec00-eba7-11ea-89d1-0fe1e5fef939.png)\\r\\n\\r\\n\\r\\n-I put allowRead \"Default\" role on \"a\" and \"b\".\\r\\n\\r\\nHere while using \"select from V\" with testUser: only a and b are shown -> OK\\r\\n\\r\\n![image](https://user-images.githubusercontent.com/70526441/91731694-6ed8f700-eba7-11ea-989e-e27a7f870151.png)\\r\\n\\r\\n\\r\\nBut when i do a \"select expand(out()) from A where name='a'\" with testUser, i expect this to return b but instead, a NPE is thrown (due to the fact that i don't have read permission on c) . -> KO\\r\\n\\r\\n![image](https://user-images.githubusercontent.com/70526441/91729646-2ae4f280-eba5-11ea-8553-1a53d0c9aabd.png)\\r\\n\\r\\n\\r\\nI tried something similar (with Predicate Security) in OrientDb 3.1.2 but the result is the same (NPE).\\r\\n\\r\\nDatabase export in 3.0.29 (user login: testUser, user password: testUser):\\r\\n[testExpand.gz](https://github.com/orientechnologies/orientdb/files/5150948/testExpand.gz)\\r\\n\\r\\nDatabase export in 3.1.2 (user login: testUser, user password: testUser):\\r\\n[test.gz](https://github.com/orientechnologies/orientdb/files/5150954/test.gz)\\r\\n\\r\\n\\r\\n\\r\\nLooking forward for your help,\\r\\nRegards,\\r\\nBoris.\\r\\n\\r\\n",
          "issue_comments": []
        },
        {
          "issue_url": "https://github.com/orientechnologies/orientdb/issues/9384",
          "issue_title": "Config settings to maintain only weekly logs?",
          "issue_number": 9384,
          "issue_text": "### OrientDB Version: <2.1.5>  \r\n### Java Version: <1.8.0_172>  \r\n### OS: <Ubuntu 16.04>  \r\n\r\n## Expected behavior  \r\nI have the log files and the config files setup for orientdb. I want to maintain only weekly logs to keep the size of the log files low. However, i have really old logs showing up as well. These are my log files :-  \r\n\r\n```\r\n2020-08-04 14:53:44:449 INFO  {db=twitter_tweets_job_graph_990223} Created database 'twitter_tweets_job_graph_990223' of type 'plocal' [ONetworkProtocolBinary]\r\n2020-08-04 14:55:25:106 INFO  {db=twitter_tweets_job_graph_990224} Created database 'twitter_tweets_job_graph_990224' of type 'plocal' [ONetworkProtocolBinary]\r\n2020-08-05 23:59:29:937 INFO  {db=twitter_tweets_job_graph_990236} Created database 'twitter_tweets_job_graph_990236' of type 'plocal' [ONetworkProtocolBinary]\r\n2020-08-06 16:06:19:968 INFO  {db=twitter_tweets_job_graph_990238} Created database 'twitter_tweets_job_graph_990238' of type 'plocal' [ONetworkProtocolBinary]\r\n2020-08-06 16:08:03:792 INFO  {db=twitter_tweets_job_graph_990242} Created database 'twitter_tweets_job_graph_990242' of type 'plocal' [ONetworkProtocolBinary]\r\n2020-08-06 19:55:34:791 INFO  {db=twitter_tweets_job_graph_990249} Created database 'twitter_tweets_job_graph_990249' of type 'plocal' [ONetworkProtocolBinary]\r\n2020-08-07 06:06:20:622 INFO  {db=twitter_tweets_job_graph_990256} Created database 'twitter_tweets_job_graph_990256' of type 'plocal' [ONetworkProtocolBinary]\r\n2020-08-07 06:22:53:106 INFO  {db=twitter_tweets_job_graph_990257} Created database 'twitter_tweets_job_graph_990257' of type 'plocal' [ONetworkProtocolBinary]\r\n2020-08-07 06:23:48:128 INFO  {db=twitter_tweets_job_graph_990258} Created database 'twitter_tweets_job_graph_990258' of type 'plocal' [ONetworkProtocolBinary]\r\n2020-08-07 14:51:50:029 INFO  {db=twitter_tweets_job_graph_990260} Created database 'twitter_tweets_job_graph_990260' of type 'plocal' [ONetworkProtocolBinary]\r\n2020-08-24 19:15:12:980 INFO  {db=twitter_tweets_job_graph_990358} Created database 'twitter_tweets_job_graph_990358' of type 'plocal' [ONetworkProtocolBinary]\r\n2020-08-25 19:16:25:157 INFO  {db=friendship_graph_990238} Created database 'friendship_graph_990238' of type 'plocal' [ONetworkProtocolBinary]\r\n2020-08-25 20:31:58:228 INFO  {db=friendship_graph_990238} Dropped database 'friendship_graph_990238' [ONetworkProtocolBinary]\r\n```\r\n And this is my orientdb-server-log.properties config file\r\n```\r\n \r\nhandlers = java.util.logging.ConsoleHandler, java.util.logging.FileHandler\r\n\r\n# Set the default logging level for the root logger\r\n.level = INFO\r\ncom.orientechnologies.level = INFO\r\ncom.orientechnologies.orient.server.distributed.level = INFO\r\n\r\n# Set the default logging level for new ConsoleHandler instances\r\njava.util.logging.ConsoleHandler.level = FINE\r\n# Set the default formatter for new ConsoleHandler instances\r\njava.util.logging.ConsoleHandler.formatter = com.orientechnologies.common.log.OLogFormatter\r\n\r\n# Set the default logging level for new FileHandler instances\r\njava.util.logging.FileHandler.level = FINE\r\n# Naming style for the output file\r\njava.util.logging.FileHandler.pattern=../log/orient-server.log\r\n# Set the default formatter for new FileHandler instances\r\njava.util.logging.FileHandler.formatter = com.orientechnologies.common.log.OLogFormatter\r\n# Limiting size of output file in bytes:\r\njava.util.logging.FileHandler.limit=1000\r\n# Number of output files to cycle through, by appending an\r\n# integer to the base file name:\r\njava.util.logging.FileHandler.count=10\r\n```\r\n\r\n## Actual behavior  \r\nI want to maintain only weekly logs. What would be the right config settings? \r\n\r\n",
          "issue_comments": []
        },
        {
          "issue_url": "https://github.com/orientechnologies/orientdb/issues/9382",
          "issue_title": "Query returns duplicate results when ordered",
          "issue_number": 9382,
          "issue_text": "### OrientDB Version: 2.2.37\r\n### Java Version: 1.8 (server), 11 (client)\r\n### OS: Amazon Linux (server), macOS (client)\r\n\r\n## Expected behavior  \r\n\r\nOrdering should not affect the number of results returned, only the order.\r\n\r\n## Actual behavior  \r\n\r\nSpecifying an order by clause inexplicably causes duplicate results to be returned.\r\n\r\n## Steps to reproduce  \r\n\r\n```\r\norientdb {db=wdb2}> select @rid from w13_1205 where @rid = #1312:13518\r\n\r\n+----+-----------+\r\n|#   |rid        |\r\n+----+-----------+\r\n|0   |#1312:13518|\r\n+----+-----------+\r\n\r\n1 item(s) found. Query executed in 0.028 sec(s).\r\norientdb {db=wdb2}> select @rid from w13_1205 where @rid = #1312:13518 order by _entity\r\n\r\n+----+-----------+\r\n|#   |rid        |\r\n+----+-----------+\r\n|0   |#1312:13518|\r\n|1   |#1312:13518|\r\n|2   |#1312:13518|\r\n|3   |#1312:13518|\r\n|4   |#1312:13518|\r\n|5   |#1312:13518|\r\n|6   |#1312:13518|\r\n|7   |#1312:13518|\r\n+----+-----------+\r\n\r\n8 item(s) found. Query executed in 1.339 sec(s).\r\n```\r\n\r\nI know I'm on an old version but I tried upgrading and that failed for numerous reasons. I've spent the budget on trying to upgrade for the time being but I hope to get time to try again in future. Before I do that, I need to know that issues like this aren't still going to bite me in later versions.\r\n\r\nAs a side note, requiring a full export/import to upgrade just one major version is just crazy. I could understand requiring people to go up one version at a time so that you only had to maintain the current and previous code. I really wish OrientDB had a realistic upgrade path.",
          "issue_comments": [
            {
              "comment_username": "luigidellaquila",
              "comment_create_time": "2020-08-25T07:10:04Z",
              "comment_edit_time": "2020-08-25T07:10:04Z",
              "comment_text": "Hi @steinybot \r\n\r\nAs you know, v 2.2 did not have any hotfix updates for more than one year, the SQL executor is a legacy component (completely rewritten from scratch in v 3.0)  and is pretty fragile, so this kind of behaviours unfortunately is someway expected.\r\n\r\nThe new SQL is much more stable and does not suffer from these problems. \r\n\r\nThe migration needs some attention, but we did it with many customers and community users, so it's definitely possible and not too complicated. The export/import is the standard way to proceed because the binary format in the storage has some changes, you can still try to copy/paste the databases folder, but you'll lose some new functionalities for sure. BTW, the export/import can take a few hours for big databases, but it is not so complex in the end...\r\n\r\nProbably it's easier for us to give you some advice on the migration, you'll have a lot of advantages in the long term\r\n\r\nThanks\r\n\r\nLuigi\r\n\r\n\r\n"
            },
            {
              "comment_username": "steinybot",
              "comment_create_time": "2020-08-25T07:24:48Z",
              "comment_edit_time": "2020-08-25T07:24:48Z",
              "comment_text": "Hi @luigidellaquila \r\n\r\nI appreciate the response. Sorry if I was on a bit of a rant in my original post. It's OSS software after all, and I am grateful for that even if I do come across as being frustrated.\r\n\r\nOne of the issues I raised about importing has been fixed in the most recent 3.0.x which is awesome.\r\n\r\nPart of the problem I have is self inflicted, well not by me personally but from the person who I inherited this project from. There are some RID's which are serialised in a separate database and potentially in other unknown places. When I did the import I didn't use the `-preserveClusterIDs` option and found records which seemed to be linked to the wrong things although I can't say for certain. Being schema-less is causing so many problems. I have to reverse engineer some sort of schema over hundreds of different entities which have been all jammed into a single class. Is it possible to preserve the full RID and not just the cluster ID? I know that there is also the `-deleteRIDMapping` option but finding where all the RIDs are in my data is the hard part.\r\n\r\nCheers."
            },
            {
              "comment_username": "luigidellaquila",
              "comment_create_time": "2020-08-25T09:12:10Z",
              "comment_edit_time": "2020-08-25T09:12:10Z",
              "comment_text": "Hi @steinybot \r\n\r\nUnfortunately, preserving RIDs is not an option for now.\r\nIf that's a constraint for you, I'd suggest to try copy/pasting the databases and see if it works in your case (there is a significant chance it does)\r\n\r\nThanks\r\n\r\nLuigi\r\n"
            }
          ]
        },
        {
          "issue_url": "https://github.com/orientechnologies/orientdb/issues/9380",
          "issue_title": "Use ETL import CSV file ,but don't print information",
          "issue_number": 9380,
          "issue_text": "### OrientDB Version: <V3.0.10>  \r\n### Java Version: <1.8>  \r\n### OS: <windows10>  \r\n\r\n## Expected behavior  \r\n<$ ./oetl.sh /temp/openbeer/beers.json\r\n\r\nOrientDB etl v.2.0.9 (build @BUILD@) www.orientechnologies.com\r\nBEGIN ETL PROCESSOR\r\n...\r\n+ extracted 5.862 rows (1.041 rows/sec) - 5.862 rows -> loaded 4.332 vertices (929 vertices/sec) Total time: 10801ms [0 warnings, 27 errors]\r\nEND ETL PROCESSOR>  \r\n\r\n## Actual behavior  \r\n<C:\\openbeer>oetl.bat bre.json\r\nOrientDB etl v.3.0.10 - Veloce (build eac0654847df662ca03b45a6a5efa5eadd229ca5, branch 3.0.x) https://www.orientdb.com\r\ndbName = openbeerdb\r\nC:\\openbeer>>  \r\n\r\n\r\n\r\n",
          "issue_comments": []
        },
        {
          "issue_url": "https://github.com/orientechnologies/orientdb/issues/9377",
          "issue_title": "Import database comsumes all RAM",
          "issue_number": 9377,
          "issue_text": "### OrientDB Version: 3.0.33\r\n### Java Version: openjdk version \"11.0.8\" 2020-07-14\r\n### OS: Ubuntu 20.04\r\n\r\n## Expected behavior  \r\nTo successfully import a database from a json backup.\r\n\r\n## Actual behavior  \r\nI have a big backup, exported from a database with 36+ millions of nodes (it is not gziped). When I try to import it to another base, the process consumes all of my 32 GB of RAM progressively before it finishes, hanging my PC. In one of my attempts, with all my memory exclusively destined to the import process, it finishes importing and migrating nodes, but then the rebuild index process ended up consuming the few GB left. I am still unable to import it.\r\n",
          "issue_comments": [
            {
              "comment_username": "laa",
              "comment_create_time": "2020-08-18T12:36:33Z",
              "comment_edit_time": "2020-08-18T12:36:33Z",
              "comment_text": "Hi, @johny65 . To understand the reason of this issue we need to have a heap dump. Could you generate it for us? "
            },
            {
              "comment_username": "johny65",
              "comment_create_time": "2020-08-18T13:04:15Z",
              "comment_edit_time": "2020-08-18T13:04:15Z",
              "comment_text": "@laa The heap is always inside of its limits during the process, 1 GB in my case. It even never reaches 750 MB. Does it help a heap dump?"
            },
            {
              "comment_username": "misu200",
              "comment_create_time": "2020-08-21T13:04:39Z",
              "comment_edit_time": "2020-08-22T09:58:54Z",
              "comment_text": "I have the same problem on a massive insert ETL process  in the database in  OrientDB 3.0.33 . The memory used keeps growing slowly...and eventually the import process gets killed by OS. The same import worked fine on my OrientDB 3.0.18 database.\r\n\r\nFixed the issue by changing OpenJDK to Oracle JDK"
            },
            {
              "comment_username": "laa",
              "comment_create_time": "2020-08-25T13:07:49Z",
              "comment_edit_time": "2020-08-25T13:07:49Z",
              "comment_text": "@johny65 could you send me your log files and heap dump I will check how much \"direct memory\" RAM do you use."
            },
            {
              "comment_username": "laa",
              "comment_create_time": "2020-08-25T14:24:52Z",
              "comment_edit_time": "2020-08-25T14:24:52Z",
              "comment_text": "@misu200 sorry I did not notice your comment. Let me do some changes in code and @johny65 will check that."
            },
            {
              "comment_username": "laa",
              "comment_create_time": "2020-08-28T08:51:03Z",
              "comment_edit_time": "2020-08-28T08:51:03Z",
              "comment_text": "Hi, @johny65 could you try this build?"
            },
            {
              "comment_username": "johny65",
              "comment_create_time": "2020-08-31T11:31:09Z",
              "comment_edit_time": "2020-08-31T11:31:09Z",
              "comment_text": "Excuse me @laa, which build do you refer yo?"
            },
            {
              "comment_username": "laa",
              "comment_create_time": "2020-08-31T12:13:35Z",
              "comment_edit_time": "2020-08-31T12:13:35Z",
              "comment_text": "@johny65 Oh sorry :-) you can use 3.0.34 release. We just released it."
            },
            {
              "comment_username": "johny65",
              "comment_create_time": "2020-09-02T11:58:03Z",
              "comment_edit_time": "2020-09-02T11:58:03Z",
              "comment_text": "@laa I tried the import with the 3.0.34 release and I have the same problem... I'll send you a head dump."
            },
            {
              "comment_username": "laa",
              "comment_create_time": "2020-09-02T12:00:08Z",
              "comment_edit_time": "2020-09-02T12:00:08Z",
              "comment_text": "HI @johny65 , thank you I will check it."
            }
          ]
        },
        {
          "issue_url": "https://github.com/orientechnologies/orientdb/issues/9376",
          "issue_title": "Load a dataset with more than 32767 labels",
          "issue_number": 9376,
          "issue_text": "### OrientDB Version: 3.0.x  \r\n### Java Version: any  \r\n### OS: linux \r\n\r\n```\r\n    <dependencies>\r\n        <!-- https://mvnrepository.com/artifact/com.orientechnologies/orientdb-gremlin -->\r\n        <dependency>\r\n            <groupId>com.orientechnologies</groupId>\r\n            <artifactId>orientdb-gremlin</artifactId>\r\n            <version>3.0.30</version>\r\n        </dependency>\r\n        <dependency>\r\n            <groupId>com.orientechnologies</groupId>\r\n            <artifactId>orientdb-client</artifactId>\r\n            <version>3.0.30</version>\r\n        </dependency>\r\n    </dependencies>\r\n``` \r\n\r\n## Task \r\nLoad a dataset with _more than_ 32767 different labels (_e.g._ [DBpedia](https://wiki.dbpedia.org/))\r\n\r\n## Issue\r\nHave a way to load it.  \r\n\r\n## Possibly related\r\n- From issue https://github.com/orientechnologies/orientdb/issues/6577  , set `MINIMUMCLUSTERS`:\r\n```ALTER DATABASE MINIMUMCLUSTERS 1```\r\n- In `OrientGraphFactory.java` there is a method with a promising name `setLabelAsClassName(boolean is)`, but then documentation and implementation talks about prefixes and not actual implementation differences (from 3.0.30):\r\n```\r\n /**\r\n   * Enable or disable the prefixing of class names with V_&lt;label&gt; for vertices or E_&lt;label&gt; for edges.\r\n   *\r\n   * @param is if true classname equals label, if false classname is prefixed with V_ or E_ (default)\r\n   */\r\n  public OrientGraphBaseFactory setLabelAsClassName(boolean is) {\r\n    this.labelAsClassName = is;\r\n    return this;\r\n  }\r\n```\r\n\r\n<br><br><br>\r\n\r\nCould someone tell me if it is at all possible? and if so pointing me in the right direction?\r\nThank you in advance,\r\nMartin",
          "issue_comments": [
            {
              "comment_username": "MartinBrugnara",
              "comment_create_time": "2020-08-19T09:15:58Z",
              "comment_edit_time": "2020-08-19T09:15:58Z",
              "comment_text": "Update:\r\nUsing `ALTER DATABASE MINIMUMCLUSTERS 1` does indeed allow to load datasets with more than 32767 labels.\r\n\r\nThis conflicts with the statement from @smolinari in #6577  on Aug 19, 2016,\r\n> You can change this behavior by running ALTER DATABASE MINIMUMCLUSTERS 1, before you start loading data, which will then give you the full 32676 available classes. This could be at the cost of future performance, however.\r\n\r\nThough, there we were discussing about v2.x .\r\n\r\nThe only references to `MINIMUMCLUSTERS` in the v3.x docs says nothing about scalability in terms of labels/classes, they only talk about speeding up insert in a multi threaded environment.\r\n\r\nSo, you may mark now this issue as **documentation request**."
            },
            {
              "comment_username": "smolinari",
              "comment_create_time": "2020-08-19T17:10:32Z",
              "comment_edit_time": "2020-08-19T17:10:32Z",
              "comment_text": "I believe, if I recall correctly, that they wanted to increase that limit in 3.0. Not sure though. It's been a while since I've had interest in ODB. 😄 \r\n\r\nScott"
            }
          ]
        },
        {
          "issue_url": "https://github.com/orientechnologies/orientdb/issues/9375",
          "issue_title": "[WIP] unique index for distributed API part 3",
          "issue_number": 9375,
          "issue_text": "Initial testing finished, reviews and tidy in progress.\r\n\r\nMake unique key index version logic persistent.\r\n",
          "issue_comments": [
            {
              "comment_username": "codecov-commenter",
              "comment_create_time": "2020-08-17T08:09:27Z",
              "comment_edit_time": "2020-09-29T12:02:41Z",
              "comment_text": "# [Codecov](https://codecov.io/gh/orientechnologies/orientdb/pull/9375?src=pr&el=h1) Report\n> Merging [#9375](https://codecov.io/gh/orientechnologies/orientdb/pull/9375?src=pr&el=desc) into [develop](https://codecov.io/gh/orientechnologies/orientdb/commit/c294e0d1cc9719c8d823c1ede44c1c37527d9d84?el=desc) will **increase** coverage by `0.09%`.\n> The diff coverage is `64.23%`.\n\n[![Impacted file tree graph](https://codecov.io/gh/orientechnologies/orientdb/pull/9375/graphs/tree.svg?width=650&height=150&src=pr&token=JVej89Tdc3)](https://codecov.io/gh/orientechnologies/orientdb/pull/9375?src=pr&el=tree)\n\n```diff\n@@              Coverage Diff              @@\n##             develop    #9375      +/-   ##\n=============================================\n+ Coverage      54.11%   54.21%   +0.09%     \n- Complexity     35549    35685     +136     \n=============================================\n  Files           2429     2446      +17     \n  Lines         192393   192920     +527     \n  Branches       35100    35116      +16     \n=============================================\n+ Hits          104115   104584     +469     \n- Misses         73329    73361      +32     \n- Partials       14949    14975      +26     \n```\n\n\n| [Impacted Files](https://codecov.io/gh/orientechnologies/orientdb/pull/9375?src=pr&el=tree) | Coverage Δ | Complexity Δ | |\n|---|---|---|---|\n| [...ies/orient/core/index/engine/OBaseIndexEngine.java](https://codecov.io/gh/orientechnologies/orientdb/pull/9375/diff?src=pr&el=tree#diff-Y29yZS9zcmMvbWFpbi9qYXZhL2NvbS9vcmllbnRlY2hub2xvZ2llcy9vcmllbnQvY29yZS9pbmRleC9lbmdpbmUvT0Jhc2VJbmRleEVuZ2luZS5qYXZh) | `100.00% <ø> (ø)` | `0.00 <0.00> (ø)` | |\n| [...dex/engine/v1/OCellBTreeMultiValueIndexEngine.java](https://codecov.io/gh/orientechnologies/orientdb/pull/9375/diff?src=pr&el=tree#diff-Y29yZS9zcmMvbWFpbi9qYXZhL2NvbS9vcmllbnRlY2hub2xvZ2llcy9vcmllbnQvY29yZS9pbmRleC9lbmdpbmUvdjEvT0NlbGxCVHJlZU11bHRpVmFsdWVJbmRleEVuZ2luZS5qYXZh) | `38.34% <ø> (-0.38%)` | `42.00 <0.00> (-1.00)` | |\n| [...t/core/sharding/auto/OAutoShardingIndexEngine.java](https://codecov.io/gh/orientechnologies/orientdb/pull/9375/diff?src=pr&el=tree#diff-Y29yZS9zcmMvbWFpbi9qYXZhL2NvbS9vcmllbnRlY2hub2xvZ2llcy9vcmllbnQvY29yZS9zaGFyZGluZy9hdXRvL09BdXRvU2hhcmRpbmdJbmRleEVuZ2luZS5qYXZh) | `67.00% <0.00%> (-0.50%)` | `36.00 <0.00> (-1.00)` | |\n| [.../orient/core/storage/cache/chm/AsyncReadCache.java](https://codecov.io/gh/orientechnologies/orientdb/pull/9375/diff?src=pr&el=tree#diff-Y29yZS9zcmMvbWFpbi9qYXZhL2NvbS9vcmllbnRlY2hub2xvZ2llcy9vcmllbnQvY29yZS9zdG9yYWdlL2NhY2hlL2NobS9Bc3luY1JlYWRDYWNoZS5qYXZh) | `59.35% <ø> (+0.35%)` | `44.00 <0.00> (+1.00)` | |\n| [...ies/orient/core/storage/cache/local/OWOWCache.java](https://codecov.io/gh/orientechnologies/orientdb/pull/9375/diff?src=pr&el=tree#diff-Y29yZS9zcmMvbWFpbi9qYXZhL2NvbS9vcmllbnRlY2hub2xvZ2llcy9vcmllbnQvY29yZS9zdG9yYWdlL2NhY2hlL2xvY2FsL09XT1dDYWNoZS5qYXZh) | `45.88% <ø> (+0.06%)` | `112.00 <0.00> (ø)` | |\n| [...core/storage/cluster/v1/OClusterPositionMapV1.java](https://codecov.io/gh/orientechnologies/orientdb/pull/9375/diff?src=pr&el=tree#diff-Y29yZS9zcmMvbWFpbi9qYXZhL2NvbS9vcmllbnRlY2hub2xvZ2llcy9vcmllbnQvY29yZS9zdG9yYWdlL2NsdXN0ZXIvdjEvT0NsdXN0ZXJQb3NpdGlvbk1hcFYxLmphdmE=) | `0.00% <ø> (ø)` | `0.00 <0.00> (ø)` | |\n| [...core/storage/cluster/v2/OClusterPositionMapV2.java](https://codecov.io/gh/orientechnologies/orientdb/pull/9375/diff?src=pr&el=tree#diff-Y29yZS9zcmMvbWFpbi9qYXZhL2NvbS9vcmllbnRlY2hub2xvZ2llcy9vcmllbnQvY29yZS9zdG9yYWdlL2NsdXN0ZXIvdjIvT0NsdXN0ZXJQb3NpdGlvbk1hcFYyLmphdmE=) | `76.25% <ø> (ø)` | `52.00 <0.00> (ø)` | |\n| [...t/core/storage/cluster/v2/OPaginatedClusterV2.java](https://codecov.io/gh/orientechnologies/orientdb/pull/9375/diff?src=pr&el=tree#diff-Y29yZS9zcmMvbWFpbi9qYXZhL2NvbS9vcmllbnRlY2hub2xvZ2llcy9vcmllbnQvY29yZS9zdG9yYWdlL2NsdXN0ZXIvdjIvT1BhZ2luYXRlZENsdXN0ZXJWMi5qYXZh) | `79.90% <ø> (ø)` | `120.00 <0.00> (ø)` | |\n| [.../storage/impl/local/OAbstractPaginatedStorage.java](https://codecov.io/gh/orientechnologies/orientdb/pull/9375/diff?src=pr&el=tree#diff-Y29yZS9zcmMvbWFpbi9qYXZhL2NvbS9vcmllbnRlY2hub2xvZ2llcy9vcmllbnQvY29yZS9zdG9yYWdlL2ltcGwvbG9jYWwvT0Fic3RyYWN0UGFnaW5hdGVkU3RvcmFnZS5qYXZh) | `47.61% <ø> (-0.14%)` | `395.00 <0.00> (-6.00)` | |\n| [...omicoperations/OAtomicOperationBinaryTracking.java](https://codecov.io/gh/orientechnologies/orientdb/pull/9375/diff?src=pr&el=tree#diff-Y29yZS9zcmMvbWFpbi9qYXZhL2NvbS9vcmllbnRlY2hub2xvZ2llcy9vcmllbnQvY29yZS9zdG9yYWdlL2ltcGwvbG9jYWwvcGFnaW5hdGVkL2F0b21pY29wZXJhdGlvbnMvT0F0b21pY09wZXJhdGlvbkJpbmFyeVRyYWNraW5nLmphdmE=) | `70.39% <ø> (ø)` | `72.00 <0.00> (ø)` | |\n| ... and [209 more](https://codecov.io/gh/orientechnologies/orientdb/pull/9375/diff?src=pr&el=tree-more) | |\n\n------\n\n[Continue to review full report at Codecov](https://codecov.io/gh/orientechnologies/orientdb/pull/9375?src=pr&el=continue).\n> **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta)\n> `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`\n> Powered by [Codecov](https://codecov.io/gh/orientechnologies/orientdb/pull/9375?src=pr&el=footer). Last update [c294e0d...e6abf4a](https://codecov.io/gh/orientechnologies/orientdb/pull/9375?src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).\n"
            }
          ]
        },
        {
          "issue_url": "https://github.com/orientechnologies/orientdb/issues/9374",
          "issue_title": "Create FDW integration for PostgreSQL",
          "issue_number": 9374,
          "issue_text": "### OrientDB Version: 3.0 \r\n### Java Version: Java 11\r\n### OS: Debian\r\n\r\n## Expected behavior  \r\nFDW Integration for PostgreSQL 11.\r\nhttps://wiki.postgresql.org/wiki/Foreign_data_wrappers\r\n\r\nWe are interested in buying the interprise, but without FDW the team will have to switch to Neo4J",
          "issue_comments": []
        },
        {
          "issue_url": "https://github.com/orientechnologies/orientdb/issues/9373",
          "issue_title": "Cannot Test if Linkedlist Column Contains Null Date Value",
          "issue_number": 9373,
          "issue_text": "### OrientDB Version: 3.0.31 \\r\\n### Java Version: 11\\r\\n### OS: MacOS Catalina\\r\\n\\r\\n## Expected behavior  \\r\\ndateCustomerCompliantClosed | containNullElement\\r\\n-- | --\\r\\n[\"2020-06-10 00:00:00\"] | no\\r\\n[\"2020-06-18 00:00:00\"] | no\\r\\n[\"2020-06-10 00:00:00\"] | no\\r\\n[\"2020-06-08 00:00:00\"] | no\\r\\n[\"2020-08-12 00:00:00\",\"2020-08-05 00:00:00\",null] | yes\\r\\n[null] | yes\\r\\n\\r\\n\\r\\n## Actual behavior  \\r\\n\\r\\ndateCustomerCompliantClosed | containNullElement\\r\\n-- | --\\r\\n[\"2020-06-10 00:00:00\"] | no\\r\\n[\"2020-06-18 00:00:00\"] | no\\r\\n[\"2020-06-10 00:00:00\"] | no\\r\\n[\"2020-06-08 00:00:00\"] | no\\r\\n[\"2020-08-12 00:00:00\",\"2020-08-05 00:00:00\",null] | no\\r\\n[null] | no\\r\\n\\r\\n![image](https://user-images.githubusercontent.com/61701811/90302316-8f444a00-de62-11ea-82fa-556801966cf0.png)\\r\\n\\r\\n\\r\\n \\r\\n\\r\\n## Steps to reproduce  \\r\\nselect compliantLog.dateCustomerCompliantClosed as dateCustomerCompliantClosed, \\r\\nif(eval(\"compliantLog.dateCustomerCompliantClosed contains null\"), 'yes', 'no') as containNullElement\\r\\nfrom Rpt_Three where compliantLog.dateCustomerCompliantClosed is not null\\r\\n\\r\\nI want to test for null element in the linkedlist column but the contains function doesn't work. \\r\\n",
          "issue_comments": [
            {
              "comment_username": "luigidellaquila",
              "comment_create_time": "2020-09-03T07:07:41Z",
              "comment_edit_time": "2020-09-03T07:07:41Z",
              "comment_text": "Hi @pa-emmanuel \r\n\r\nI managed to reproduce the problem.\r\n\r\nUnfortunately, the `eval()` function uses some legacy components that I cannot break in a hotfix release, so the only thing I can suggest is to define a custom javascript function (see https://orientdb.com/docs/3.0.x/admin/Functions-Creation.html ) and use it in your queries.\r\n\r\nThanks\r\n\r\nLuigi"
            }
          ]
        },
        {
          "issue_url": "https://github.com/orientechnologies/orientdb/issues/9371",
          "issue_title": "deadlock when writing in parallel from several java clients",
          "issue_number": 9371,
          "issue_text": "### OrientDB Version: 3.0.33\\r\\n### Java Version: 1.8\\r\\n### OS: linux\\r\\n\\r\\n## Expected behavior  \\r\\nOrientDB is more resilient to errors.\\r\\nAdvice on a simple java client to write to OrientDB\\r\\n\\r\\n## Actual behavior  \\r\\nOrientDB master deadlocks\\r\\n\\r\\n## Steps to reproduce  \\r\\nI have a deployment with 1 master and 2 replicas\\r\\nThe main thing that I think that affects is that I'm writing in parallel from several java clients\\r\\n\\r\\nI've attached a thread dump with 36 blocked threads\\r\\n[thread_dump.txt](https://github.com/orientechnologies/orientdb/files/5067898/thread_dump.txt)\\r\\n\\r\\nAfter analyzing it in https://spotify.github.io/threaddump-analyzer/ I see that one thread has a lock and all the rest are waiting for it. The thread is blocked in a socket write operation and another thing that I have seen is that the TCP connection 172.26.75.92:41320 no longer exists.\\r\\n\\r\\nPlease add some mechanism to at least mitigate this by adding a timeout or configuration. And also please point me to an example of a properly implemented java client writer.\\r\\n\\r\\nThanks a lot\\r\\n\\r\\n`\\r\\n\"OrientDB (/172.26.76.23:2424) <- BinaryClient (/172.26.75.92:41320)\": running, holding [0x00000005c307c030, 0x00000005c307c228, 0x00000005c309a4a0]\\r\\n\\tat java.net.SocketOutputStream.socketWrite0(Native Method)\\r\\n\\tat java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:111)\\r\\n\\tat java.net.SocketOutputStream.write(SocketOutputStream.java:155)\\r\\n\\tat java.io.BufferedOutputStream.write(BufferedOutputStream.java:122)\\r\\n\\tat java.io.DataOutputStream.write(DataOutputStream.java:107)\\r\\n\\tat java.io.FilterOutputStream.write(FilterOutputStream.java:97)\\r\\n\\tat com.orientechnologies.orient.client.remote.message.OMessageHelper.writeTransactionEntry(OMessageHelper.java:192)\\r\\n\\tat com.orientechnologies.orient.server.distributed.impl.task.OTransactionPhase1Task.toStream(OTransactionPhase1Task.java:218)\\r\\n\\tat com.orientechnologies.orient.server.distributed.ODistributedRequest.toStream(ODistributedRequest.java:100)\\r\\n\\tat com.orientechnologies.orient.server.distributed.ORemoteServerChannel.lambda$sendRequest$0(ORemoteServerChannel.java:103)\\r\\n\\tat com.orientechnologies.orient.server.distributed.ORemoteServerChannel$$Lambda$201/461793454.execute(Unknown Source)\\r\\n\\tat com.orientechnologies.orient.server.distributed.ORemoteServerChannel.networkOperation(ORemoteServerChannel.java:159)\\r\\n\\tat com.orientechnologies.orient.server.distributed.ORemoteServerChannel.sendRequest(ORemoteServerChannel.java:102)\\r\\n\\tat com.orientechnologies.orient.server.distributed.ORemoteServerController.sendRequest(ORemoteServerController.java:66)\\r\\n\\tat com.orientechnologies.orient.server.distributed.impl.ODistributedDatabaseImpl.send2Nodes(ODistributedDatabaseImpl.java:498)\\r\\n\\tat com.orientechnologies.orient.server.distributed.impl.ODistributedAbstractPlugin.sendRequest(ODistributedAbstractPlugin.java:625)\\r\\n\\tat com.orientechnologies.orient.server.distributed.impl.ONewDistributedTransactionManager.retriedCommit(ONewDistributedTransactionManager.java:196)\\r\\n\\tat com.orientechnologies.orient.server.distributed.impl.ONewDistributedTransactionManager.commit(ONewDistributedTransactionManager.java:84)\\r\\n\\tat com.orientechnologies.orient.server.distributed.impl.ODatabaseDocumentDistributed.internalCommit(ODatabaseDocumentDistributed.java:372)\\r\\n\\tat com.orientechnologies.orient.core.tx.OTransactionOptimistic.doCommit(OTransactionOptimistic.java:533)\\r\\n\\tat com.orientechnologies.orient.core.tx.OTransactionOptimistic.commit(OTransactionOptimistic.java:103)\\r\\n\\tat com.orientechnologies.orient.core.db.document.ODatabaseDocumentAbstract.commit(ODatabaseDocumentAbstract.java:2238)\\r\\n\\tat com.orientechnologies.orient.core.db.document.ODatabaseDocumentAbstract.commit(ODatabaseDocumentAbstract.java:2208)\\r\\n\\tat com.orientechnologies.orient.core.sql.parser.OCommitStatement.executeSimple(OCommitStatement.java:30)\\r\\n\\tat com.orientechnologies.orient.core.sql.executor.OSingleOpExecutionPlan.executeInternal(OSingleOpExecutionPlan.java:99)\\r\\n\\tat com.orientechnologies.orient.core.sql.executor.ScriptLineStep.syncPull(ScriptLineStep.java:37)\\r\\n\\tat com.orientechnologies.orient.core.sql.executor.OScriptExecutionPlan.doExecute(OScriptExecutionPlan.java:89)\\r\\n\\tat com.orientechnologies.orient.core.sql.executor.OScriptExecutionPlan.fetchNext(OScriptExecutionPlan.java:50)\\r\\n\\tat com.orientechnologies.orient.core.sql.parser.OLocalResultSet.fetchNext(OLocalResultSet.java:39)\\r\\n\\tat com.orientechnologies.orient.core.sql.parser.OLocalResultSet.<init>(OLocalResultSet.java:30)\\r\\n\\tat com.orientechnologies.orient.core.command.OSqlScriptExecutor.executeInternal(OSqlScriptExecutor.java:114)\\r\\n\\tat com.orientechnologies.orient.core.command.OSqlScriptExecutor.execute(OSqlScriptExecutor.java:60)\\r\\n\\tat com.orientechnologies.orient.core.db.document.ODatabaseDocumentEmbedded.execute(ODatabaseDocumentEmbedded.java:638)\\r\\n\\tat com.orientechnologies.orient.server.OConnectionBinaryExecutor.executeQuery(OConnectionBinaryExecutor.java:1192)\\r\\n\\tat com.orientechnologies.orient.client.remote.message.OQueryRequest.execute(OQueryRequest.java:136)\\r\\n\\tat com.orientechnologies.orient.server.network.protocol.binary.ONetworkProtocolBinary.sessionRequest(ONetworkProtocolBinary.java:319)\\r\\n\\tat com.orientechnologies.orient.server.network.protocol.binary.ONetworkProtocolBinary.execute(ONetworkProtocolBinary.java:214)\\r\\n\\tat com.orientechnologies.common.thread.OSoftThread.run(OSoftThread.java:69)`\\r\\n\\r\\n",
          "issue_comments": [
            {
              "comment_username": "tglman",
              "comment_create_time": "2020-08-14T17:37:31Z",
              "comment_edit_time": "2020-08-14T17:37:31Z",
              "comment_text": "Hi,\r\n\r\nAs you point also on the stack trace there is a thread writing on the socket that, block the others, the lock that we use has the granularity of record, so if there are multiple transactions trying to do write on a specific record they will go ahead 1 by 1.\r\n\r\nRegards"
            },
            {
              "comment_username": "madmac2501",
              "comment_create_time": "2020-08-14T18:41:43Z",
              "comment_edit_time": "2020-08-14T18:51:54Z",
              "comment_text": "Hi,\r\n\r\nin this case this thread have been locked for hours in that point without progressing. Even the TCP connection referenced in the thread name was gone.\r\n\r\nThanks for taking a look at it!"
            },
            {
              "comment_username": "madmac2501",
              "comment_create_time": "2020-08-16T08:41:55Z",
              "comment_edit_time": "2020-08-16T08:41:55Z",
              "comment_text": "Hi,\r\n\r\nit just happened to me again. I was deleting from OrientDB using this query with the Orientdb web ui. No insertion was being done at the same time.\r\n\r\nBEGIN;\r\nDELETE VERTEX myVertex where age < (SYSDATE().asLong() - 86400000) LIMIT 5000;\r\nCOMMIT;\r\n\r\n[thread_dump_delete.txt](https://github.com/orientechnologies/orientdb/files/5080289/thread_dump_delete.txt)\r\n\r\n\r\nThe deadlocked thread was \"OrientDB HTTP Connection /172.26.76.23:2480<-/172.26.16.95:34428\" and I found that the TCP connection was in CLOSE_WAIT state\r\n\r\ntcp6       1      0 172.26.76.23:2480       172.26.16.95:34428      CLOSE_WAIT  31423/java  \r\n\r\nBest regards"
            }
          ]
        },
        {
          "issue_url": "https://github.com/orientechnologies/orientdb/issues/9370",
          "issue_title": "Running in azure",
          "issue_number": 9370,
          "issue_text": "### OrientDB Version: 3.x.x\r\n\r\nI am trying to run version 3 in Azure as a container instance (only because in Azure marketplace, only 2.2 is available). I am completely stuck with a problem where I have absolutely no idea how to specify persistent volumes. In docker run \"-v <databases_path>:/orientdb/databases....\" is used but in Azure there is nowhere to enter this parameters. I searched through web for days without any solution. Can anyone help me how should I properly create a container instance?\r\n\r\nThanks,\r\nTomaz",
          "issue_comments": []
        },
        {
          "issue_url": "https://github.com/orientechnologies/orientdb/issues/9369",
          "issue_title": "hazelcast connections are not stable",
          "issue_number": 9369,
          "issue_text": "### OrientDB Version: 3.0.32\r\n### Java Version: 1.8\r\n### OS: Linux\r\n\r\n## Expected behavior  \r\nhazelcast connections are not stable\r\n\r\n## Actual behavior  \r\nafter some time the hazelcast connections begin to degrade and are closed and/or doesn't reply\r\n\r\n## Steps to reproduce  \r\nI have 1 master and 2 replica nodes.\r\nhazelcast is configured with default hazelcast.xml but with TCP\r\n\r\nMASTER CONNECTIONS\r\ntcp6       0      0 :::2434                 :::*                    LISTEN      21872/java      \r\ntcp6       0      0 172.25.1.18:47653       172.25.1.198:2434       ESTABLISHED 21872/java      \r\ntcp6       0      0 172.25.1.18:57277       172.25.1.74:2434        ESTABLISHED 21872/java  \r\n\r\nREPLICA 1 CONNECTIONS\r\ntcp6       0      0 :::2434                 :::*                    LISTEN      3825/java       \r\ntcp6   12913      0 172.25.1.74:49780       172.25.1.198:2434       ESTABLISHED 3825/java       \r\ntcp6       0   6168 172.25.1.74:52427       172.25.1.18:2434        CLOSE_WAIT  3825/java       \r\ntcp6      73      0 172.25.1.74:2434        172.25.1.18:57277       ESTABLISHED 3825/java       \r\ntcp6      74      0 172.25.1.74:2434        172.25.1.198:49836      ESTABLISHED 3825/java     \r\n\r\nREPLICA 2 CONNECTIONS\r\ntcp6       0      0 :::2434                 :::*                    LISTEN      10595/java      \r\ntcp6       0      0 172.25.1.198:49836      172.25.1.74:2434        ESTABLISHED 10595/java      \r\ntcp6       0      0 172.25.1.198:2434       172.25.1.18:47653       ESTABLISHED 10595/java      \r\ntcp6       0      0 172.25.1.198:2434       172.25.1.74:49780       ESTABLISHED 10595/java\r\n\r\n\r\nLogs from replica node number 2\r\n2020-08-11 16:05:14:701 INFO  [172.25.1.198]:2434 [orientdb] [3.10.6] Invocations:9 timeouts:1 backup-timeouts:0 [InvocationMonitor]\r\n2020-08-11 16:05:29:701 INFO  [172.25.1.198]:2434 [orientdb] [3.10.6] Invocations:9 timeouts:1 backup-timeouts:0 [InvocationMonitor]\r\n2020-08-11 16:05:44:701 INFO  [172.25.1.198]:2434 [orientdb] [3.10.6] Invocations:9 timeouts:1 backup-timeouts:0 [InvocationMonitor]\r\n\r\n\r\nPlease let me know if there is a recommended configuration for hazelcast different than the default provided in the tar.gz. Or a recommended configuration for the operating system.\r\nIs UDP hazelcast recommended over TCP?\r\n\r\nThanks!\r\n",
          "issue_comments": [
            {
              "comment_username": "madmac2501",
              "comment_create_time": "2020-08-11T16:35:06Z",
              "comment_edit_time": "2020-08-11T16:35:06Z",
              "comment_text": "[replica1.log](https://github.com/orientechnologies/orientdb/files/5058176/replica1.log)\r\n\r\nHere is a thread dump from replica 1. It's also showing errors about hazelcast."
            }
          ]
        },
        {
          "issue_url": "https://github.com/orientechnologies/orientdb/issues/9368",
          "issue_title": "OrientDB Java client only configured with master node IP trying to write to replicas",
          "issue_number": 9368,
          "issue_text": "### OrientDB Version: 3.0.32\r\n### Java Version: 1.8\r\n### OS: <os here>  \r\n\r\n## Expected behavior  \r\nOrientDB Java client only connects to nodes that it has configured\r\n\r\n## Actual behavior  \r\nOrientDB Java client stablishes connections to all nodes of a cluster, even if they are a replica and the query cannot write.\r\n\r\n## Steps to reproduce  \r\nConfigure a cluster with 1 master and 2 replicas\r\nIn the Java client connector only specify the ip of the master node, for example remote:192.168.1.1:2424/my_database\r\nStop the master node\r\n\r\nPlease let me know if there is a configuration option so the java client only connects to specified IP's provided\r\n",
          "issue_comments": []
        },
        {
          "issue_url": "https://github.com/orientechnologies/orientdb/issues/9367",
          "issue_title": "Getting NullPointerException on database startup (at com.orientechnologies.orient.core.storage.cache.chm.AsyncReadCache.releaseFromRead)",
          "issue_number": 9367,
          "issue_text": "### OrientDB Version: 3.0.31\\r\\n### Java Version:\\r\\n\\r\\n```\\r\\nopenjdk version \"1.8.0_252\"\\r\\nOpenJDK Runtime Environment (build 1.8.0_252-b09)\\r\\nOpenJDK 64-Bit Server VM (build 25.252-b09, mixed mode)\\r\\n```\\r\\n\\r\\n### OS:\\r\\n\\r\\n```\\r\\nPRETTY_NAME=\"Debian GNU/Linux 10 (buster)\"\\r\\nNAME=\"Debian GNU/Linux\"\\r\\nVERSION_ID=\"10\"\\r\\nVERSION=\"10 (buster)\"\\r\\nVERSION_CODENAME=buster\\r\\nID=debian\\r\\n```\\r\\n\\r\\n## Expected behavior  \\r\\n\\r\\nNo exceptions on startup.\\r\\n\\r\\n## Actual behavior  \\r\\n\\r\\n```\\r\\n         ,,.     `,                                              VELOCE  \\r\\n       ``        `.                                                          \\r\\n                 ``                                       www.orientdb.com\\r\\n                 `                                    \\r\\n2020-08-07 14:59:26:321 INFO  Detected limit of amount of simultaneously open files is 1048576,  limit of open files for disk cache will be set to 523776 [ONative]\\r\\n2020-08-07 14:59:26:332 INFO  Loading configuration from: /orientdb/config/orientdb-server-config.xml... [OServerConfigurationLoaderXml]\\r\\n2020-08-07 14:59:26:486 INFO  OrientDB Server v3.0.31 - Veloce (build 2a8412458048d53cd1e1a3544d19b87c190c2d73, branch UNKNOWN) is starting up... [OServer]\\r\\n2020-08-07 14:59:26:531 INFO  67557953536 B/64428 MB/62 GB of physical memory were detected on machine [ONative]\\r\\n2020-08-07 14:59:26:532 INFO  Soft memory limit for this process is set to -1 B/-1 MB/-1 GB [ONative]\\r\\n2020-08-07 14:59:26:532 INFO  Hard memory limit for this process is set to -1 B/-1 MB/-1 GB [ONative]\\r\\n2020-08-07 14:59:26:533 INFO  Path to 'memory' cgroup is '/kubepods/burstable/pod381302ef-d709-11ea-9320-525400a3535a/d8d499430f21742087c284f2ee53229e346b4baac671b4b44099b5ad521cb32d' [ONative]\\r\\n2020-08-07 14:59:26:535 INFO  Mounting path for memory cgroup controller is '/sys/fs/cgroup/memory' [ONative]\\r\\n2020-08-07 14:59:26:535 INFO  Can not find '/sys/fs/cgroup/memory/kubepods/burstable/pod381302ef-d709-11ea-9320-525400a3535a/d8d499430f21742087c284f2ee53229e346b4baac671b4b44099b5ad521cb32d' path for memory cgroup, it is supposed that process is running in container, will try to read root '/sys/fs/cgroup/memory' memory cgroup data [ONative]\\r\\n2020-08-07 14:59:26:535 INFO  cgroup soft memory limit is 9223372036854771712 B/8796093022207 MB/8589934591 GB [ONative]\\r\\n2020-08-07 14:59:26:536 INFO  cgroup hard memory limit is 524288000 B/500 MB/0 GB [ONative]\\r\\n2020-08-07 14:59:26:536 INFO  Detected memory limit for current process is 524288000 B/500 MB/0 GB [ONative]\\r\\n2020-08-07 14:59:26:537 INFO  JVM can use maximum 371MB of heap memory [OMemoryAndLocalPaginatedEnginesInitializer]\\r\\n2020-08-07 14:59:26:537 INFO  Because OrientDB is running inside a container 12% of memory will be left unallocated according to the setting 'memory.leftToContainer' not taking into account heap memory [OMemoryAndLocalPaginatedEnginesInitializer]\\r\\n2020-08-07 14:59:26:539 INFO  OrientDB auto-config DISKCACHE=68MB (heap=371MB os=500MB) [orientechnologies]\\r\\n2020-08-07 14:59:26:543 INFO  System is started under an effective user : `root` [OEngineLocalPaginated]\\r\\n2020-08-07 14:59:26:544 INFO  Allocation of 1033 pages. [OEngineLocalPaginated]\\r\\n2020-08-07 14:59:26:643 INFO  WAL maximum segment size is set to 190,617 MB [OrientDBDistributed]\\r\\n2020-08-07 14:59:26:644 INFO  Databases directory: /orientdb/databases [OServer]\\r\\n2020-08-07 14:59:26:710 INFO  Page size for WAL located in /orientdb/databases/OSystem is set to 4096 bytes. [OCASDiskWriteAheadLog]\\r\\n2020-08-07 14:59:27:020 INFO  Storage 'plocal:/orientdb/databases/OSystem' is opened under OrientDB distribution : 3.0.31 - Veloce (build 2a8412458048d53cd1e1a3544d19b87c190c2d73, branch UNKNOWN) [OLocalPaginatedStorage]Exception `558633DB` in storage `plocal:/orientdb/databases/OSystem`: 3.0.31 - Veloce (build 2a8412458048d53cd1e1a3544d19b87c190c2d73, branch UNKNOWN)\\r\\njava.lang.NullPointerException\\r\\n\\tat com.orientechnologies.orient.core.storage.cache.chm.AsyncReadCache.releaseFromRead(AsyncReadCache.java:226)\\r\\n\\tat com.orientechnologies.orient.core.storage.impl.local.paginated.base.ODurableComponent.releasePageFromRead(ODurableComponent.java:167)\\r\\n\\tat com.orientechnologies.orient.core.storage.cluster.v1.OClusterPositionMapV1.getFirstPosition(OClusterPositionMapV1.java:511)\\r\\n\\tat com.orientechnologies.orient.core.storage.cluster.v1.OPaginatedClusterV1.getFirstPosition(OPaginatedClusterV1.java:1231)\\r\\n\\tat com.orientechnologies.orient.core.storage.impl.local.OAbstractPaginatedStorage.getClusterDataRange(OAbstractPaginatedStorage.java:1349)\\r\\n\\tat com.orientechnologies.orient.core.iterator.ORecordIteratorCluster.<init>(ORecordIteratorCluster.java:62)\\r\\n\\tat com.orientechnologies.orient.core.iterator.ORecordIteratorCluster.<init>(ORecordIteratorCluster.java:44)\\r\\n\\tat com.orientechnologies.orient.core.sql.executor.FetchFromClusterExecutionStep.syncPull(FetchFromClusterExecutionStep.java:53)\\r\\n\\tat com.orientechnologies.orient.core.sql.executor.FetchFromClassExecutionStep$1.hasNext(FetchFromClassExecutionStep.java:128)\\r\\n\\tat com.orientechnologies.orient.core.sql.executor.OrderByStep.init(OrderByStep.java:101)\\r\\n\\tat com.orientechnologies.orient.core.sql.executor.OrderByStep.lambda$syncPull$0(OrderByStep.java:40)\\r\\n\\tat java.util.Optional.ifPresent(Optional.java:159)\\r\\n\\tat com.orientechnologies.orient.core.sql.executor.OrderByStep.syncPull(OrderByStep.java:40)\\r\\n\\tat com.orientechnologies.orient.core.sql.executor.OSelectExecutionPlan.fetchNext(OSelectExecutionPlan.java:37)\\r\\n\\tat com.orientechnologies.orient.core.sql.parser.OLocalResultSet.fetchNext(OLocalResultSet.java:39)\\r\\n\\tat com.orientechnologies.orient.core.sql.parser.OLocalResultSet.<init>(OLocalResultSet.java:30)\\r\\n\\tat com.orientechnologies.orient.core.sql.parser.OSelectStatement.execute(OSelectStatement.java:276)\\r\\n\\tat com.orientechnologies.orient.core.sql.parser.OStatement.execute(OStatement.java:79)\\r\\n\\tat com.orientechnologies.orient.core.db.document.ODatabaseDocumentEmbedded.query(ODatabaseDocumentEmbedded.java:538)\\r\\n\\tat com.orientechnologies.orient.core.metadata.function.OFunctionLibraryImpl.load(OFunctionLibraryImpl.java:83)\\r\\n\\tat com.orientechnologies.orient.core.db.OSharedContextEmbedded.load(OSharedContextEmbedded.java:69)\\r\\n\\tat com.orientechnologies.orient.core.db.document.ODatabaseDocumentEmbedded.loadMetadata(ODatabaseDocumentEmbedded.java:291)\\r\\n\\tat com.orientechnologies.orient.core.db.document.ODatabaseDocumentEmbedded.init(ODatabaseDocumentEmbedded.java:161)\\r\\n\\tat com.orientechnologies.orient.core.db.OrientDBEmbedded.openNoAuthorization(OrientDBEmbedded.java:262)\\r\\n\\tat com.orientechnologies.orient.core.db.OrientDBEmbedded.openNoAuthorization(OrientDBEmbedded.java:56)\\r\\n\\tat com.orientechnologies.orient.server.OSystemDatabase.openSystemDatabase(OSystemDatabase.java:97)\\r\\n\\tat com.orientechnologies.orient.server.OServer.initSystemDatabase(OServer.java:1212)\\r\\n\\tat com.orientechnologies.orient.server.OServer.activate(OServer.java:421)\\r\\n\\tat com.orientechnologies.orient.server.OServerMain$1.run(OServerMain.java:48)\\r\\n2020-08-07 14:59:27:222 INFO  - shutdown storage: OSystem... [OrientDBDistributed]Error during server execution\\r\\ncom.orientechnologies.orient.core.exception.ODatabaseException: Cannot open database 'OSystem'\\r\\n\\tat com.orientechnologies.orient.core.db.OrientDBEmbedded.openNoAuthorization(OrientDBEmbedded.java:268)\\r\\n\\tat com.orientechnologies.orient.core.db.OrientDBEmbedded.openNoAuthorization(OrientDBEmbedded.java:56)\\r\\n\\tat com.orientechnologies.orient.server.OSystemDatabase.openSystemDatabase(OSystemDatabase.java:97)\\r\\n\\tat com.orientechnologies.orient.server.OSystemDatabase.checkServerId(OSystemDatabase.java:174)\\r\\n\\tat com.orientechnologies.orient.server.OSystemDatabase.init(OSystemDatabase.java:162)\\r\\n\\tat com.orientechnologies.orient.server.OSystemDatabase.<init>(OSystemDatabase.java:49)\\r\\n\\tat com.orientechnologies.orient.server.OServer.initSystemDatabase(OServer.java:1212)\\r\\n\\tat com.orientechnologies.orient.server.OServer.activate(OServer.java:421)\\r\\n\\tat com.orientechnologies.orient.server.OServerMain$1.run(OServerMain.java:48)\\r\\nCaused by: com.orientechnologies.orient.core.exception.ODatabaseException: Cannot open database url=plocal:/orientdb/databases/OSystem\\r\\n\\tat com.orientechnologies.orient.core.db.document.ODatabaseDocumentEmbedded.init(ODatabaseDocumentEmbedded.java:175)\\r\\n\\tat com.orientechnologies.orient.core.db.OrientDBEmbedded.openNoAuthorization(OrientDBEmbedded.java:262)\\r\\n\\t... 8 more\\r\\nCaused by: java.lang.NullPointerException\\r\\n\\tat com.orientechnologies.orient.core.storage.cache.chm.AsyncReadCache.releaseFromRead(AsyncReadCache.java:226)\\r\\n\\tat com.orientechnologies.orient.core.storage.impl.local.paginated.base.ODurableComponent.releasePageFromRead(ODurableComponent.java:167)\\r\\n\\tat com.orientechnologies.orient.core.storage.cluster.v1.OClusterPositionMapV1.getFirstPosition(OClusterPositionMapV1.java:511)\\r\\n\\tat com.orientechnologies.orient.core.storage.cluster.v1.OPaginatedClusterV1.getFirstPosition(OPaginatedClusterV1.java:1231)\\r\\n\\tat com.orientechnologies.orient.core.storage.impl.local.OAbstractPaginatedStorage.getClusterDataRange(OAbstractPaginatedStorage.java:1349)\\r\\n\\tat com.orientechnologies.orient.core.iterator.ORecordIteratorCluster.<init>(ORecordIteratorCluster.java:62)\\r\\n\\tat com.orientechnologies.orient.core.iterator.ORecordIteratorCluster.<init>(ORecordIteratorCluster.java:44)\\r\\n\\tat com.orientechnologies.orient.core.sql.executor.FetchFromClusterExecutionStep.syncPull(FetchFromClusterExecutionStep.java:53)\\r\\n\\tat com.orientechnologies.orient.core.sql.executor.FetchFromClassExecutionStep$1.hasNext(FetchFromClassExecutionStep.java:128)\\r\\n\\tat com.orientechnologies.orient.core.sql.executor.OrderByStep.init(OrderByStep.java:101)\\r\\n\\tat com.orientechnologies.orient.core.sql.executor.OrderByStep.lambda$syncPull$0(OrderByStep.java:40)\\r\\n\\tat java.util.Optional.ifPresent(Optional.java:159)\\r\\n\\tat com.orientechnologies.orient.core.sql.executor.OrderByStep.syncPull(OrderByStep.java:40)\\r\\n\\tat com.orientechnologies.orient.core.sql.executor.OSelectExecutionPlan.fetchNext(OSelectExecutionPlan.java:37)\\r\\n\\tat com.orientechnologies.orient.core.sql.parser.OLocalResultSet.fetchNext(OLocalResultSet.java:39)\\r\\n\\tat com.orientechnologies.orient.core.sql.parser.OLocalResultSet.<init>(OLocalResultSet.java:30)\\r\\n\\tat com.orientechnologies.orient.core.sql.parser.OSelectStatement.execute(OSelectStatement.java:276)\\r\\n\\tat com.orientechnologies.orient.core.sql.parser.OStatement.execute(OStatement.java:79)\\r\\n\\tat com.orientechnologies.orient.core.db.document.ODatabaseDocumentEmbedded.query(ODatabaseDocumentEmbedded.java:538)\\r\\n\\tat com.orientechnologies.orient.core.metadata.function.OFunctionLibraryImpl.load(OFunctionLibraryImpl.java:83)\\r\\n\\tat com.orientechnologies.orient.core.db.OSharedContextEmbedded.load(OSharedContextEmbedded.java:69)\\r\\n\\tat com.orientechnologies.orient.core.db.document.ODatabaseDocumentEmbedded.loadMetadata(ODatabaseDocumentEmbedded.java:291)\\r\\n\\tat com.orientechnologies.orient.core.db.document.ODatabaseDocumentEmbedded.init(ODatabaseDocumentEmbedded.java:161)\\r\\n\\t... 9 more\\r\\n```\\r\\n\\r\\n## Steps to reproduce  \\r\\n\\r\\nNone, but maybe you have come clues?",
          "issue_comments": [
            {
              "comment_username": "earshinov",
              "comment_create_time": "2020-08-07T15:35:49Z",
              "comment_edit_time": "2020-08-07T15:35:49Z",
              "comment_text": "Is it something that could possibly be fixed by upgrading to 3.0.32?"
            },
            {
              "comment_username": "laa",
              "comment_create_time": "2020-08-13T11:29:13Z",
              "comment_edit_time": "2020-08-13T11:29:13Z",
              "comment_text": "Hi @earshinov . OSystem database is not critical, you can delete it. But in general, the problem is caused by the fact that you use V1 version of cluster. You should use V2 version of cluster, which is durable. If your database is not big, you can export to JSNO and then import it and you will perform the migration. In future, we are planning to implement automatic migration between versions."
            },
            {
              "comment_username": "laa",
              "comment_create_time": "2020-08-26T10:10:35Z",
              "comment_edit_time": "2020-08-26T10:10:35Z",
              "comment_text": "Hi @earshinov , did removal of system database resolve your issue?"
            },
            {
              "comment_username": "earshinov",
              "comment_create_time": "2020-08-26T15:31:53Z",
              "comment_edit_time": "2020-08-26T15:32:06Z",
              "comment_text": "@laa , Yes, `rm -rf databases/OSystem/` helped, thanks! I will try migrating to cluster V2 a bit later."
            },
            {
              "comment_username": "earshinov",
              "comment_create_time": "2020-08-27T07:22:41Z",
              "comment_edit_time": "2020-08-27T07:22:41Z",
              "comment_text": "@laa , Can you please point me to the documentation on cluster versions (v1 vs v2)? I must be missing something: as far as I understand, in the test setup I am tinkering with at the moment I shouldn't be using OrientDB in distributed mode at all because I only have a single server."
            },
            {
              "comment_username": "laa",
              "comment_create_time": "2020-08-27T10:05:09Z",
              "comment_edit_time": "2020-08-27T10:05:09Z",
              "comment_text": "@earshinov by cluster I mean storage cluster, disk-based cluster, like a table, not distributed cluster."
            },
            {
              "comment_username": "earshinov",
              "comment_create_time": "2020-08-28T00:20:39Z",
              "comment_edit_time": "2020-08-28T00:20:39Z",
              "comment_text": "@laa , Could you give me a hint about how to switch between cluster v1 and v2?"
            },
            {
              "comment_username": "earshinov",
              "comment_create_time": "2020-09-01T20:01:07Z",
              "comment_edit_time": "2020-09-01T20:04:19Z",
              "comment_text": "One of the reasons why I'm asking is that I recreated the database recently. Given that OrientDB used cluster v1 when recreating the database (apparently), why would it switch to cluster v2 when I export and import the database?\r\n\r\nI'm still trying to glean some information on cluster v1 and v2. Is cluster v2 something that appeared between 3.0.30 and 3.0.33? I didn't use older versions of OrientDB. I can't find any information about cluster v2 on the [OrientDB 3.0 Release Notes](https://github.com/orientechnologies/orientdb/wiki/OrientDB-3.0-Release-Notes) page."
            }
          ]
        },
        {
          "issue_url": "https://github.com/orientechnologies/orientdb/issues/9364",
          "issue_title": "OrientDB in master/replica using 100% cpu of all cores",
          "issue_number": 9364,
          "issue_text": "### OrientDB Version: orientdb-community-3.0.33-20200729.120423-18.tar.gz\r\n### Java Version: 1.8\r\n### OS: Linux\r\n\r\n## Expected behavior  \r\nOrientDB stays responsive\r\n\r\n## Actual behavior  \r\nOrientDB master  consuming using 100% cpu of all cores and replicas using 0% cpu.\r\nOrientDB master not responsive\r\n\r\n## Steps to reproduce  \r\nI configured 1 master with 2 replica nodes\r\nAfter inserting upserts for a few hours I found that the master node was unresponsive and it was using 100% cpu of all cores and replicas using 0% cpu.\r\n\r\nI attached logs from all nodes. At the end of the logs are several thread dumps for all nodes.\r\n\r\n[orient-server-boot-master1.log.gz](https://github.com/orientechnologies/orientdb/files/5028590/orient-server-boot-master1.log.gz)\r\n[orient-server-boot-replica1.log.gz](https://github.com/orientechnologies/orientdb/files/5028591/orient-server-boot-replica1.log.gz)\r\n[orient-server-boot-replica2.log.gz](https://github.com/orientechnologies/orientdb/files/5028592/orient-server-boot-replica2.log.gz)\r\n\r\n",
          "issue_comments": []
        },
        {
          "issue_url": "https://github.com/orientechnologies/orientdb/issues/9362",
          "issue_title": "com.orientechnologies.orient.server.distributed.task.ODistributedRecordLockedException: Timeout (100ms) on acquiring lock",
          "issue_number": 9362,
          "issue_text": "### OrientDB Version: 3.1.1\\r\\n### Java Version: java version \"1.8.0_251\"  \\r\\n### OS: Red Hat Linux 7.8  \\r\\n\\r\\n## Expected behavior  \\r\\nData get inserted  \\r\\n\\r\\n## Actual behavior  \\r\\nGetting below exception\\r\\ncom.orientechnologies.orient.server.distributed.task.ODistributedRecordLockedException: Timeout (100ms) on acquiring lock on record #202:-1 on server 'vishal01'. ^M\\r\\n        DB name=\"vishald\"^M\\r\\n        DB name=\"vishald\"\\r\\n        at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) ~[?:1.8.0_251]\\r\\n        at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62) ~[?:1.8.0_251]\\r\\n        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) ~[?:1.8.0_251]\\r\\n        at java.lang.reflect.Constructor.newInstance(Constructor.java:423) ~[?:1.8.0_251]\\r\\n        at com.orientechnologies.orient.client.binary.OChannelBinaryAsynchClient.handleException(OChannelBinaryAsynchClient.java:357) ~[orientdb-client-3.1.1.jar:3.1.1]\\r\\n        at com.orientechnologies.orient.client.binary.OChannelBinaryAsynchClient.handleStatus(OChannelBinaryAsynchClient.java:305) ~[orientdb-client-3.1.1.jar:3.1.1]\\r\\n        at com.orientechnologies.orient.client.binary.OChannelBinaryAsynchClient.handleStatus(OChannelBinaryAsynchClient.java:327) ~[orientdb-client-3.1.1.jar:3.1.1]\\r\\n        at com.orientechnologies.orient.client.binary.OChannelBinaryAsynchClient.beginResponse(OChannelBinaryAsynchClient.java:211) ~[orientdb-client-3.1.1.jar:3.1.1]\\r\\n        at com.orientechnologies.orient.client.binary.OChannelBinaryAsynchClient.beginResponse(OChannelBinaryAsynchClient.java:169) ~[orientdb-client-3.1.1.jar:3.1.1]\\r\\n        at com.orientechnologies.orient.client.remote.OStorageRemote.beginResponse(OStorageRemote.java:2221) ~[orientdb-client-3.1.1.jar:3.1.1]\\r\\n        at com.orientechnologies.orient.client.remote.OStorageRemote.lambda$networkOperationRetryTimeout$2(OStorageRemote.java:385) ~[orientdb-client-3.1.1.jar:3.1.1]\\r\\n        at com.orientechnologies.orient.client.remote.OStorageRemote.baseNetworkOperation(OStorageRemote.java:447) ~[orientdb-client-3.1.1.jar:3.1.1]\\r\\n        at com.orientechnologies.orient.client.remote.OStorageRemote.networkOperationRetryTimeout(OStorageRemote.java:365) ~[orientdb-client-3.1.1.jar:3.1.1]\\r\\n        at com.orientechnologies.orient.client.remote.OStorageRemote.networkOperationNoRetry(OStorageRemote.java:400) ~[orientdb-client-3.1.1.jar:3.1.1]\\r\\n        at com.orientechnologies.orient.client.remote.OStorageRemote.commit(OStorageRemote.java:1304) ~[orientdb-client-3.1.1.jar:3.1.1]\\r\\n        at com.orientechnologies.orient.core.db.document.ODatabaseDocumentAbstract.internalCommit(ODatabaseDocumentAbstract.java:2466) ~[orientdb-core-3.1.1.jar:3.1.1]\\r\\n        at com.orientechnologies.orient.core.tx.OTransactionOptimistic.doCommit(OTransactionOptimistic.java:602) ~[orientdb-core-3.1.1.jar:3.1.1]\\r\\n        at com.orientechnologies.orient.core.tx.OTransactionOptimistic.commit(OTransactionOptimistic.java:106) ~[orientdb-core-3.1.1.jar:3.1.1]\\r\\n        at com.orientechnologies.orient.core.db.document.ODatabaseDocumentAbstract.commit(ODatabaseDocumentAbstract.java:1984) ~[orientdb-core-3.1.1.jar:3.1.1]\\r\\n        at com.orientechnologies.orient.core.db.document.ODatabaseDocumentAbstract.commit(ODatabaseDocumentAbstract.java:1954) ~[orientdb-core-3.1.1.jar:3.1.1]\\r\\n\\r\\n## Using below code to acquire acquire connection  \\r\\n\\r\\nprivate OrientDBUtil () {\\r\\n\\t\\torientDB = new OrientDB(\"remote:vishal01, vishal02\",\"root\",\"xxx\", OrientDBConfig.defaultConfig());\\r\\n\\r\\n\\t\\tOrientDBConfigBuilder poolCfg = OrientDBConfig.builder();\\r\\n\\t\\tpoolCfg.addConfig(OGlobalConfiguration.DB_POOL_ACQUIRE_TIMEOUT, 1000);\\r\\n\\t\\tpoolCfg.addConfig(OGlobalConfiguration.CLIENT_CHANNEL_MAX_POOL, 5000);\\r\\n\\t\\tpoolCfg.addConfig(OGlobalConfiguration.DB_POOL_MIN, 50);\\r\\n\\t\\tpoolCfg.addConfig(OGlobalConfiguration.DB_POOL_MAX, 100);\\r\\n\\t\\tpool = new ODatabasePool(orientDB,\"vishald\",\"root\",\"xxx\", poolCfg.build()); \\r\\n\\t} \\r\\n\\t\\r\\n\\tpublic ODatabasePool connection() {\\r\\n\\t\\treturn this.pool;\\r\\n\\t}\\r\\n\\r\\n\\r\\nMultiple threads are inserting the different data in distributed server vishal01 & vishal02",
          "issue_comments": []
        },
        {
          "issue_url": "https://github.com/orientechnologies/orientdb/issues/9361",
          "issue_title": "Cannot route TX operation against distributed node",
          "issue_number": 9361,
          "issue_text": "### OrientDB Version: 3.1.2-SNAPSHOT from https://oss.sonatype.org/content/repositories/snapshots/com/orientechnologies/orientdb-community/3.1.2-SNAPSHOT/orientdb-community-3.1.2-20200729.194604-8.zip\r\n\r\n### Java Version: 8\r\n### OS: Linux\r\n## Expected behavior  \r\nDistributed mode is stable.\r\n\r\nAside from this please let me know when is ok to test the distributed mode in 3.1.2-SNAPSHOT version. Thanks!\r\n\r\n\r\n## Actual behavior  \r\nI get this exception when just writing anything into the database\r\n\r\njava.lang.NullPointerException\r\nError on transaction commit `618061B9`\r\ncom.orientechnologies.orient.core.exception.OStorageException: Cannot route TX operation against distributed node\r\n\tDB name=\"xxx\"\r\n\tat com.orientechnologies.orient.server.distributed.impl.ODistributedStorage.handleDistributedException(ODistributedStorage.java:773)\r\n\tat com.orientechnologies.orient.server.distributed.impl.ODatabaseDocumentDistributed.distributedCommitV1(ODatabaseDocumentDistributed.java:460)\r\n\tat com.orientechnologies.orient.server.distributed.impl.ODatabaseDocumentDistributed.internalCommit(ODatabaseDocumentDistributed.java:400)\r\n\tat com.orientechnologies.orient.core.tx.OTransactionOptimistic.doCommit(OTransactionOptimistic.java:602)\r\n\tat com.orientechnologies.orient.core.tx.OTransactionOptimistic.commit(OTransactionOptimistic.java:106)\r\n\tat com.orientechnologies.orient.core.db.document.ODatabaseDocumentAbstract.commit(ODatabaseDocumentAbstract.java:1984)\r\n\tat com.orientechnologies.orient.core.db.document.ODatabaseDocumentAbstract.commit(ODatabaseDocumentAbstract.java:1954)\r\n\tat com.orientechnologies.orient.core.sql.parser.OCommitStatement.executeSimple(OCommitStatement.java:30)\r\n\tat com.orientechnologies.orient.core.sql.executor.OSingleOpExecutionPlan.executeInternal(OSingleOpExecutionPlan.java:95)\r\n\tat com.orientechnologies.orient.core.sql.executor.ScriptLineStep.syncPull(ScriptLineStep.java:37)\r\n\tat com.orientechnologies.orient.core.sql.executor.OScriptExecutionPlan.doExecute(OScriptExecutionPlan.java:78)\r\n\tat com.orientechnologies.orient.core.sql.executor.OScriptExecutionPlan.fetchNext(OScriptExecutionPlan.java:39)\r\n\tat com.orientechnologies.orient.core.sql.parser.OLocalResultSet.fetchNext(OLocalResultSet.java:36)\r\n\tat com.orientechnologies.orient.core.sql.parser.OLocalResultSet.<init>(OLocalResultSet.java:27)\r\n\tat com.orientechnologies.orient.core.command.OSqlScriptExecutor.executeInternal(OSqlScriptExecutor.java:126)\r\n\tat com.orientechnologies.orient.core.command.OSqlScriptExecutor.execute(OSqlScriptExecutor.java:65)\r\n\tat com.orientechnologies.orient.core.db.document.ODatabaseDocumentEmbedded.execute(ODatabaseDocumentEmbedded.java:675)\r\n\tat com.orientechnologies.orient.server.OConnectionBinaryExecutor.executeQuery(OConnectionBinaryExecutor.java:1494)\r\n\tat com.orientechnologies.orient.client.remote.message.OQueryRequest.execute(OQueryRequest.java:143)\r\n\tat com.orientechnologies.orient.server.network.protocol.binary.ONetworkProtocolBinary.sessionRequest(ONetworkProtocolBinary.java:354)\r\n\tat com.orientechnologies.orient.server.network.protocol.binary.ONetworkProtocolBinary.execute(ONetworkProtocolBinary.java:238)\r\n\tat com.orientechnologies.common.thread.OSoftThread.run(OSoftThread.java:67)\r\nCaused by: java.lang.NullPointerException\r\n\r\n\r\n## Steps to reproduce  \r\nSetup a 3 node distributed database with 3.1.2-snapshot from 20200729\r\nWrite to a node\r\n",
          "issue_comments": [
            {
              "comment_username": "leoChaoGlut",
              "comment_create_time": "2020-09-08T03:15:12Z",
              "comment_edit_time": "2020-09-08T03:15:12Z",
              "comment_text": "+1"
            },
            {
              "comment_username": "leoChaoGlut",
              "comment_create_time": "2020-09-08T06:24:42Z",
              "comment_edit_time": "2020-09-08T06:24:42Z",
              "comment_text": "3.0.34 without this problem"
            }
          ]
        },
        {
          "issue_url": "https://github.com/orientechnologies/orientdb/issues/9356",
          "issue_title": "Hang connecting to database via plocal in console",
          "issue_number": 9356,
          "issue_text": "### OrientDB Version: 3.1.1\r\n### Java Version: 1.8\r\n### OS: Amazon Linux  \r\n\r\n## Expected behavior  \r\n\r\nI should be able to connect the console to a local database.\r\n\r\n## Actual behavior  \r\n\r\n```\r\norientdb> connect plocal:/var/erpdb/wdb2 admin\r\nEnter password:\r\n\r\nConnecting to database [plocal:/var/erpdb/wdb2] with user 'admin'...\r\n```\r\n\r\n(Hangs here forever)\r\n\r\nSomehow this still works when running in embedded mode but when I do that then I hit this bug #9353. I need the console so that I can rebuild my indices which are causing yet more issues. Really stuck.\r\n\r\n## Steps to reproduce  \r\n\r\n```\r\norientdb> connect plocal:/var/erpdb/wdb2 admin\r\n```",
          "issue_comments": [
            {
              "comment_username": "steinybot",
              "comment_create_time": "2020-07-28T02:13:12Z",
              "comment_edit_time": "2020-07-28T02:13:12Z",
              "comment_text": "Is there a way that I can get debug information out of the console?"
            },
            {
              "comment_username": "steinybot",
              "comment_create_time": "2020-07-28T02:22:05Z",
              "comment_edit_time": "2020-07-28T02:22:05Z",
              "comment_text": "It seems to create the `dirty.fl` file lock so it is doing something..."
            },
            {
              "comment_username": "laa",
              "comment_create_time": "2020-07-29T13:20:00Z",
              "comment_edit_time": "2020-07-29T13:20:00Z",
              "comment_text": "Hi @steinybot if you still have an issue could you send me thread dump?"
            }
          ]
        },
        {
          "issue_url": "https://github.com/orientechnologies/orientdb/issues/9355",
          "issue_title": "ClassCastException during execution of component operation inside of storage",
          "issue_number": 9355,
          "issue_text": "### OrientDB Version: 3.1.1\r\n### Java Version: 1.8\r\n### OS: Amazon Linux 2\r\n\r\n## Expected behavior  \r\n\r\nTransaction can be commited without errors.\r\n\r\n## Actual behavior  \r\n\r\n```\r\ncom.orientechnologies.orient.core.exception.OStorageException: Exception during execution of component operation inside of storage wdb2\r\n\tDB name=\"wdb2\"\r\n\tat com.orientechnologies.orient.core.storage.impl.local.paginated.atomicoperations.OAtomicOperationsManager.calculateInsideComponentOperation(OAtomicOperationsManager.java:272)\r\n\tat com.orientechnologies.orient.core.storage.impl.local.paginated.atomicoperations.OAtomicOperationsManager.calculateInsideComponentOperation(OAtomicOperationsManager.java:259)\r\n\tat com.orientechnologies.orient.core.storage.impl.local.paginated.base.ODurableComponent.calculateInsideComponentOperation(ODurableComponent.java:96)\r\n\tat com.orientechnologies.orient.core.storage.index.sbtree.local.v1.OSBTreeV1.update(OSBTreeV1.java:282)\r\n\tat com.orientechnologies.orient.core.storage.index.engine.OSBTreeIndexEngine.update(OSBTreeIndexEngine.java:255)\r\n\tat com.orientechnologies.orient.core.storage.impl.local.OAbstractPaginatedStorage.doUpdateIndexEntry(OAbstractPaginatedStorage.java:3368)\r\n\tat com.orientechnologies.orient.core.storage.impl.local.OAbstractPaginatedStorage.updateIndexEntry(OAbstractPaginatedStorage.java:3288)\r\n\tat com.orientechnologies.orient.core.index.OIndexMultiValues.doPutV0(OIndexMultiValues.java:217)\r\n\tat com.orientechnologies.orient.core.index.OIndexMultiValues.doPut(OIndexMultiValues.java:149)\r\n\tat com.orientechnologies.orient.core.index.OIndexAbstractDelegate.doPut(OIndexAbstractDelegate.java:436)\r\n\tat com.orientechnologies.orient.core.storage.impl.local.OAbstractPaginatedStorage.applyTxChanges(OAbstractPaginatedStorage.java:2542)\r\n\tat com.orientechnologies.orient.core.storage.impl.local.OAbstractPaginatedStorage.commitIndexes(OAbstractPaginatedStorage.java:2525)\r\n\tat com.orientechnologies.orient.core.storage.impl.local.OAbstractPaginatedStorage.commit(OAbstractPaginatedStorage.java:2439)\r\n\tat com.orientechnologies.orient.core.storage.impl.local.OAbstractPaginatedStorage.commit(OAbstractPaginatedStorage.java:2241)\r\n\tat com.orientechnologies.orient.core.db.document.ODatabaseDocumentAbstract.internalCommit(ODatabaseDocumentAbstract.java:2466)\r\n\tat com.orientechnologies.orient.core.tx.OTransactionOptimistic.doCommit(OTransactionOptimistic.java:602)\r\n\tat com.orientechnologies.orient.core.tx.OTransactionOptimistic.commit(OTransactionOptimistic.java:106)\r\n\tat com.orientechnologies.orient.core.db.document.ODatabaseDocumentAbstract.commit(ODatabaseDocumentAbstract.java:1984)\r\n\tat com.orientechnologies.orient.core.db.document.ODatabaseDocumentAbstract.commit(ODatabaseDocumentAbstract.java:135)\r\n\tat com.orientechnologies.orient.core.db.document.ODatabaseDocumentTx.commit(ODatabaseDocumentTx.java:748)\r\n...\r\nCaused by: java.lang.ClassCastException: com.orientechnologies.orient.core.storage.ridbag.sbtree.OMixedIndexRIDContainer cannot be cast to com.orientechnologies.orient.core.storage.ridbag.sbtree.OIndexRIDContainer\r\n\tat com.orientechnologies.orient.core.serialization.serializer.stream.OStreamSerializerSBTreeIndexRIDContainer.getObjectSize(OStreamSerializerSBTreeIndexRIDContainer.java:41)\r\n\tat com.orientechnologies.orient.core.storage.index.sbtree.local.v1.OSBTreeV1.lambda$update$3(OSBTreeV1.java:354)\r\n\tat com.orientechnologies.orient.core.storage.impl.local.paginated.atomicoperations.OAtomicOperationsManager.calculateInsideComponentOperation(OAtomicOperationsManager.java:267)\r\n\t... 31 common frames omitted\r\n```\r\n\r\nLooks like it is the same as #8983 which is \"fixed\" but this is not fixed.\r\n\r\n## Steps to reproduce  \r\n\r\nUnknown.",
          "issue_comments": []
        }
      ]
    }
  },
  {
    "github_url": "https://github.com/picketlink/picketlink-bindings",
    "github_info": {
      "name": "picketlink/picketlink-bindings",
      "language": "Java"
    },
    "github_branches": {
      "branch_datas": [
        {
          "branch_version": "2.6.x",
          "branch_url": "https://github.com/picketlink/picketlink-bindings/tree/2.6.x",
          "branch_download_url": "https://github.com/picketlink/picketlink-bindings/archive/2.6.x.zip"
        },
        {
          "branch_version": "PLINK-629",
          "branch_url": "https://github.com/picketlink/picketlink-bindings/tree/PLINK-629",
          "branch_download_url": "https://github.com/picketlink/picketlink-bindings/archive/PLINK-629.zip"
        },
        {
          "branch_version": "master",
          "branch_url": "https://github.com/picketlink/picketlink-bindings/tree/master",
          "branch_download_url": "https://github.com/picketlink/picketlink-bindings/archive/master.zip"
        }
      ]
    },
    "github_pull_requests": { "pull_datas": [] },
    "github_issues": { "issue_datas": [] }
  },
  {
    "github_url": "https://github.com/apache/activemq-artemis",
    "github_info": {
      "name": "apache/activemq-artemis",
      "language": "Java"
    },
    "github_branches": {
      "branch_datas": [
        {
          "branch_version": "1.x",
          "branch_url": "https://github.com/apache/activemq-artemis/tree/1.x",
          "branch_download_url": "https://github.com/apache/activemq-artemis/archive/1.x.zip"
        },
        {
          "branch_version": "2.6.x",
          "branch_url": "https://github.com/apache/activemq-artemis/tree/2.6.x",
          "branch_download_url": "https://github.com/apache/activemq-artemis/archive/2.6.x.zip"
        },
        {
          "branch_version": "master",
          "branch_url": "https://github.com/apache/activemq-artemis/tree/master",
          "branch_download_url": "https://github.com/apache/activemq-artemis/archive/master.zip"
        }
      ]
    },
    "github_pull_requests": {
      "pull_datas": [
        {
          "pull_number": 3280,
          "pull_title": "ARTEMIS-2920 Fix ActiveMQ Artemis Features build on IBM JDK 1.8",
          "pull_version": "apache:master",
          "pull_version_url": "https://github.com/apache/activemq-artemis/tree/master"
        },
        {
          "pull_number": 3279,
          "pull_title": "ARTEMIS-2919 support timestamping incoming messages",
          "pull_version": "apache:master",
          "pull_version_url": "https://github.com/apache/activemq-artemis/tree/master"
        },
        {
          "pull_number": 3257,
          "pull_title": "ARTEMIS-2838 - migrate to HawtIO 2",
          "pull_version": "apache:master",
          "pull_version_url": "https://github.com/apache/activemq-artemis/tree/master"
        },
        {
          "pull_number": 3240,
          "pull_title": "ARTEMIS-2921:Upgrade to Netty 4.1.51.Final and netty-tcnative 2.0.33.Final",
          "pull_version": "apache:master",
          "pull_version_url": "https://github.com/apache/activemq-artemis/tree/master"
        },
        {
          "pull_number": 3204,
          "pull_title": "ARTEMIS-2823 Use datasource with JDBC store db connections",
          "pull_version": "apache:master",
          "pull_version_url": "https://github.com/apache/activemq-artemis/tree/master"
        },
        {
          "pull_number": 3198,
          "pull_title": "Closing the directbytebuffer locally to prevent leaks",
          "pull_version": "apache:master",
          "pull_version_url": "https://github.com/apache/activemq-artemis/tree/master"
        },
        {
          "pull_number": 3124,
          "pull_title": "ARTEMIS-2614 Create queues based on FQQN for AMQP protocol",
          "pull_version": "apache:master",
          "pull_version_url": "https://github.com/apache/activemq-artemis/tree/master"
        },
        {
          "pull_number": 3053,
          "pull_title": "ARTEMIS-2697 Avoid using raw types for Persister<T>",
          "pull_version": "apache:master",
          "pull_version_url": "https://github.com/apache/activemq-artemis/tree/master"
        },
        {
          "pull_number": 2982,
          "pull_title": "ARTEMIS-2620 Avoid Exception on client when large message file corrupted",
          "pull_version": "apache:master",
          "pull_version_url": "https://github.com/apache/activemq-artemis/tree/master"
        },
        {
          "pull_number": 2978,
          "pull_title": "ARTEMIS-2563 Added option to create queues on all clustered nodes",
          "pull_version": "apache:master",
          "pull_version_url": "https://github.com/apache/activemq-artemis/tree/master"
        },
        {
          "pull_number": 2972,
          "pull_title": "Restart Sequence",
          "pull_version": "apache:master",
          "pull_version_url": "https://github.com/apache/activemq-artemis/tree/master"
        },
        {
          "pull_number": 2893,
          "pull_title": "ARTEMIS-1925 combine STRICT and OFF with redistribution",
          "pull_version": "apache:master",
          "pull_version_url": "https://github.com/apache/activemq-artemis/tree/master"
        },
        {
          "pull_number": 2845,
          "pull_title": "ARTEMIS-2336 Use zero copy to replicate journal/page/large message file (AGAIN)",
          "pull_version": "apache:master",
          "pull_version_url": "https://github.com/apache/activemq-artemis/tree/master"
        },
        {
          "pull_number": 2839,
          "pull_title": "[ARTEMIS-2490] Prevent NumberFormatExc when reading large message",
          "pull_version": "apache:master",
          "pull_version_url": "https://github.com/apache/activemq-artemis/tree/master"
        },
        {
          "pull_number": 2793,
          "pull_title": "ARTEMIS-2452 group-name ignored in shared store colocated setup",
          "pull_version": "apache:master",
          "pull_version_url": "https://github.com/apache/activemq-artemis/tree/master"
        },
        {
          "pull_number": 2792,
          "pull_title": "NO-JIRA: Javadoc for ActiveMQServerMessagePlugin interface",
          "pull_version": "apache:master",
          "pull_version_url": "https://github.com/apache/activemq-artemis/tree/master"
        },
        {
          "pull_number": 2643,
          "pull_title": "ARTEMIS-2320 Update javac-errorprone to 2.8.5 and error_prone_core to 2.3.3",
          "pull_version": "apache:master",
          "pull_version_url": "https://github.com/apache/activemq-artemis/tree/master"
        },
        {
          "pull_number": 2220,
          "pull_title": "[ARTEMIS-1946] Cluster with allow-direct-connections-only=\"true\" and …",
          "pull_version": "apache:master",
          "pull_version_url": "https://github.com/apache/activemq-artemis/tree/master"
        }
      ]
    },
    "github_issues": {
      "issue_datas": [
        {
          "issue_url": "https://github.com/apache/activemq-artemis/issues/3280",
          "issue_title": "ARTEMIS-2920 Fix ActiveMQ Artemis Features build on IBM JDK 1.8",
          "issue_number": 3280,
          "issue_text": "",
          "issue_comments": []
        },
        {
          "issue_url": "https://github.com/apache/activemq-artemis/issues/3279",
          "issue_title": "ARTEMIS-2919 support timestamping incoming messages",
          "issue_number": 3279,
          "issue_text": "",
          "issue_comments": [
            {
              "comment_username": "jbertram",
              "comment_create_time": "2020-09-28T16:39:45Z",
              "comment_edit_time": "2020-09-28T16:39:45Z",
              "comment_text": "@gemmellr, I think I've resolved all the issues you enumerated. Please review and let me know if there's anything left. Thanks for the feedback. It's always appreciated."
            }
          ]
        },
        {
          "issue_url": "https://github.com/apache/activemq-artemis/issues/3257",
          "issue_title": "ARTEMIS-2838 - migrate to HawtIO 2",
          "issue_number": 3257,
          "issue_text": "https://issues.apache.org/jira/browse/ARTEMIS-2838\r\n\r\nThe main reason of upgrading to HawtIO 2 is that HawtIO 1 is no longer supported for CVE's etc.\r\n\r\nAlthough this is a rewrite all the screens perform the same functionally, although there are improvements and additions.\r\n\r\nRefer to the Jira for more info.\r\n\r\nSince this is a big change Id be grateful if anyone has the time to review and kick the tyres.",
          "issue_comments": [
            {
              "comment_username": "michaelandrepearce",
              "comment_create_time": "2020-09-12T07:36:09Z",
              "comment_edit_time": "2020-09-12T07:36:09Z",
              "comment_text": "Have been able to build and run at first look. Nice. I havent gone through all screens yet to check for issues ill try do that during the week. "
            },
            {
              "comment_username": "jbertram",
              "comment_create_time": "2020-09-14T19:18:39Z",
              "comment_edit_time": "2020-09-14T19:18:39Z",
              "comment_text": "I just built the branch and tested it out. When I created my broker is used `--allow-anonymous` and when I \"logged in\" to the console it just put in a dummy username and password. The console loaded fine, but whenever I clicked on anything in the tree (e.g. addresses, acceptors, etc.) the panel on the right would just flash something quickly and return to the normal status screen with the address size, etc. I figured this was related to security so I stopped the broker and changed my `etc/management.xml` to be:\r\n```xml\r\n<management-context xmlns=\"http://activemq.org/schema\">\r\n</management-context>\r\n```\r\nWhen I restarted the broker and tried to log back in I got this NPE in the log:\r\n```\r\n2020-09-14 14:11:59,952 ERROR [io.hawt.system.RBACMBeanInvoker] Error while invoking JMXSecurity MBean: javax.management.RuntimeMBeanException: java.lang.NullPointerException: com.google.common.util.concurrent.UncheckedExecutionException: javax.management.RuntimeMBeanException: java.lang.NullPointerException\r\n\tat com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2052) [guava-24.1.1-jre.jar:]\r\n\tat com.google.common.cache.LocalCache.get(LocalCache.java:3943) [guava-24.1.1-jre.jar:]\r\n\tat com.google.common.cache.LocalCache.getOrLoad(LocalCache.java:3967) [guava-24.1.1-jre.jar:]\r\n\tat com.google.common.cache.LocalCache$LocalLoadingCache.get(LocalCache.java:4952) [guava-24.1.1-jre.jar:]\r\n\tat io.hawt.system.RBACMBeanInvoker.canInvoke(RBACMBeanInvoker.java:215) [hawtio-system-2.10.2.jar:2.10.2]\r\n\tat io.hawt.system.RBACRestrictor.isOperationAllowed(RBACRestrictor.java:70) [hawtio-system-2.10.2.jar:2.10.2]\r\n\tat org.jolokia.handler.ExecHandler.checkForRestriction(ExecHandler.java:63) [jolokia-core-1.6.2.jar:]\r\n\tat org.jolokia.handler.ExecHandler.checkForRestriction(ExecHandler.java:40) [jolokia-core-1.6.2.jar:]\r\n\tat org.jolokia.handler.JsonRequestHandler.handleRequest(JsonRequestHandler.java:87) [jolokia-core-1.6.2.jar:]\r\n\tat org.jolokia.backend.MBeanServerExecutorLocal.handleRequest(MBeanServerExecutorLocal.java:109) [jolokia-core-1.6.2.jar:]\r\n\tat org.jolokia.backend.MBeanServerHandler.dispatchRequest(MBeanServerHandler.java:161) [jolokia-core-1.6.2.jar:]\r\n\tat org.jolokia.backend.LocalRequestDispatcher.dispatchRequest(LocalRequestDispatcher.java:99) [jolokia-core-1.6.2.jar:]\r\n\tat org.jolokia.backend.BackendManager.callRequestDispatcher(BackendManager.java:429) [jolokia-core-1.6.2.jar:]\r\n\tat org.jolokia.backend.BackendManager.handleRequest(BackendManager.java:158) [jolokia-core-1.6.2.jar:]\r\n\tat org.jolokia.http.HttpRequestHandler.executeRequest(HttpRequestHandler.java:197) [jolokia-core-1.6.2.jar:]\r\n\tat org.jolokia.http.HttpRequestHandler.handlePostRequest(HttpRequestHandler.java:137) [jolokia-core-1.6.2.jar:]\r\n\tat org.jolokia.http.AgentServlet$3.handleRequest(AgentServlet.java:460) [jolokia-core-1.6.2.jar:]\r\n\tat org.jolokia.http.AgentServlet.handleSecurely(AgentServlet.java:350) [jolokia-core-1.6.2.jar:]\r\n\tat org.jolokia.http.AgentServlet.handle(AgentServlet.java:321) [jolokia-core-1.6.2.jar:]\r\n\tat org.jolokia.http.AgentServlet.doPost(AgentServlet.java:284) [jolokia-core-1.6.2.jar:]\r\n\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:707) [jetty-all-9.4.27.v20200227-uber.jar:9.4.27.v20200227]\r\n\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:790) [jetty-all-9.4.27.v20200227-uber.jar:9.4.27.v20200227]\r\n\tat org.eclipse.jetty.servlet.ServletHolder$NotAsyncServlet.service(ServletHolder.java:1395)\r\n\tat org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:755)\r\n\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1617)\r\n\tat io.hawt.web.auth.LoginRedirectFilter.doFilter(LoginRedirectFilter.java:57) [hawtio-system-2.10.2.jar:2.10.2]\r\n\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1604)\r\n\tat io.hawt.web.auth.AuthenticationFilter.lambda$executeAs$1(AuthenticationFilter.java:106) [hawtio-system-2.10.2.jar:2.10.2]\r\n\tat java.security.AccessController.doPrivileged(Native Method) [rt.jar:1.8.0_151]\r\n\tat javax.security.auth.Subject.doAs(Subject.java:422) [rt.jar:1.8.0_151]\r\n\tat io.hawt.web.auth.AuthenticationFilter.executeAs(AuthenticationFilter.java:105) [hawtio-system-2.10.2.jar:2.10.2]\r\n\tat io.hawt.web.auth.AuthenticationFilter.doFilter(AuthenticationFilter.java:72) [hawtio-system-2.10.2.jar:2.10.2]\r\n\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1604)\r\n\tat io.hawt.web.filters.HttpHeaderFilter.doFilter(HttpHeaderFilter.java:43) [hawtio-system-2.10.2.jar:2.10.2]\r\n\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1604)\r\n\tat io.hawt.web.filters.HttpHeaderFilter.doFilter(HttpHeaderFilter.java:43) [hawtio-system-2.10.2.jar:2.10.2]\r\n\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1604)\r\n\tat io.hawt.web.filters.HttpHeaderFilter.doFilter(HttpHeaderFilter.java:43) [hawtio-system-2.10.2.jar:2.10.2]\r\n\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1604)\r\n\tat io.hawt.web.filters.HttpHeaderFilter.doFilter(HttpHeaderFilter.java:43) [hawtio-system-2.10.2.jar:2.10.2]\r\n\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1604)\r\n\tat io.hawt.web.filters.HttpHeaderFilter.doFilter(HttpHeaderFilter.java:43) [hawtio-system-2.10.2.jar:2.10.2]\r\n\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1604)\r\n\tat io.hawt.web.filters.HttpHeaderFilter.doFilter(HttpHeaderFilter.java:43) [hawtio-system-2.10.2.jar:2.10.2]\r\n\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1604)\r\n\tat io.hawt.web.filters.HttpHeaderFilter.doFilter(HttpHeaderFilter.java:43) [hawtio-system-2.10.2.jar:2.10.2]\r\n\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1604)\r\n\tat io.hawt.web.filters.HttpHeaderFilter.doFilter(HttpHeaderFilter.java:43) [hawtio-system-2.10.2.jar:2.10.2]\r\n\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1604)\r\n\tat io.hawt.web.auth.SessionExpiryFilter.process(SessionExpiryFilter.java:166) [hawtio-system-2.10.2.jar:2.10.2]\r\n\tat io.hawt.web.auth.SessionExpiryFilter.doFilter(SessionExpiryFilter.java:60) [hawtio-system-2.10.2.jar:2.10.2]\r\n\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1596)\r\n\tat org.apache.activemq.artemis.component.JolokiaFilter.doFilter(JolokiaFilter.java:50) [artemis-web-2.16.0-SNAPSHOT.jar:2.16.0-SNAPSHOT]\r\n\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1604)\r\n\tat org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:545)\r\n\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:143)\r\n\tat org.eclipse.jetty.security.SecurityHandler.handle(SecurityHandler.java:590)\r\n\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)\r\n\tat org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:235)\r\n\tat org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:1610)\r\n\tat org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:233)\r\n\tat org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1300)\r\n\tat org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:188)\r\n\tat org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:485)\r\n\tat org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:1580)\r\n\tat org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:186)\r\n\tat org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1215)\r\n\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)\r\n\tat org.eclipse.jetty.server.handler.HandlerList.handle(HandlerList.java:59)\r\n\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)\r\n\tat org.eclipse.jetty.server.Server.handle(Server.java:500)\r\n\tat org.eclipse.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:383)\r\n\tat org.eclipse.jetty.server.HttpChannel.dispatch(HttpChannel.java:547)\r\n\tat org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:375)\r\n\tat org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:273)\r\n\tat org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)\r\n\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:103)\r\n\tat org.eclipse.jetty.io.ChannelEndPoint$2.run(ChannelEndPoint.java:117)\r\n\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:336)\r\n\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:313)\r\n\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:171)\r\n\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:129)\r\n\tat org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:375)\r\n\tat org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:806)\r\n\tat org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:938)\r\n\tat java.lang.Thread.run(Thread.java:748) [rt.jar:1.8.0_151]\r\nCaused by: javax.management.RuntimeMBeanException: java.lang.NullPointerException\r\n\tat com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.rethrow(DefaultMBeanServerInterceptor.java:839) [rt.jar:1.8.0_151]\r\n\tat com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.rethrowMaybeMBeanException(DefaultMBeanServerInterceptor.java:852) [rt.jar:1.8.0_151]\r\n\tat com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.invoke(DefaultMBeanServerInterceptor.java:821) [rt.jar:1.8.0_151]\r\n\tat com.sun.jmx.mbeanserver.JmxMBeanServer.invoke(JmxMBeanServer.java:801) [rt.jar:1.8.0_151]\r\n\tat io.hawt.system.RBACMBeanInvoker.doCanInvoke(RBACMBeanInvoker.java:165) [hawtio-system-2.10.2.jar:2.10.2]\r\n\tat io.hawt.system.RBACMBeanInvoker$1.load(RBACMBeanInvoker.java:143) [hawtio-system-2.10.2.jar:2.10.2]\r\n\tat io.hawt.system.RBACMBeanInvoker$1.load(RBACMBeanInvoker.java:139) [hawtio-system-2.10.2.jar:2.10.2]\r\n\tat com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3524) [guava-24.1.1-jre.jar:]\r\n\tat com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2273) [guava-24.1.1-jre.jar:]\r\n\tat com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2156) [guava-24.1.1-jre.jar:]\r\n\tat com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2046) [guava-24.1.1-jre.jar:]\r\n\t... 85 more\r\nCaused by: java.lang.NullPointerException\r\n\tat org.apache.activemq.artemis.core.server.management.impl.HawtioSecurityControlImpl.canInvoke(HawtioSecurityControlImpl.java:93) [artemis-server-2.16.0-SNAPSHOT.jar:2.16.0-SNAPSHOT]\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) [rt.jar:1.8.0_151]\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) [rt.jar:1.8.0_151]\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) [rt.jar:1.8.0_151]\r\n\tat java.lang.reflect.Method.invoke(Method.java:498) [rt.jar:1.8.0_151]\r\n\tat sun.reflect.misc.Trampoline.invoke(MethodUtil.java:71) [rt.jar:1.8.0_151]\r\n\tat sun.reflect.GeneratedMethodAccessor6.invoke(Unknown Source) [:1.8.0_151]\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) [rt.jar:1.8.0_151]\r\n\tat java.lang.reflect.Method.invoke(Method.java:498) [rt.jar:1.8.0_151]\r\n\tat sun.reflect.misc.MethodUtil.invoke(MethodUtil.java:275) [rt.jar:1.8.0_151]\r\n\tat com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:112) [rt.jar:1.8.0_151]\r\n\tat com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:46) [rt.jar:1.8.0_151]\r\n\tat com.sun.jmx.mbeanserver.MBeanIntrospector.invokeM(MBeanIntrospector.java:237) [rt.jar:1.8.0_151]\r\n\tat com.sun.jmx.mbeanserver.PerInterface.invoke(PerInterface.java:138) [rt.jar:1.8.0_151]\r\n\tat com.sun.jmx.mbeanserver.MBeanSupport.invoke(MBeanSupport.java:252) [rt.jar:1.8.0_151]\r\n\tat javax.management.StandardMBean.invoke(StandardMBean.java:405) [rt.jar:1.8.0_151]\r\n\tat com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.invoke(DefaultMBeanServerInterceptor.java:819) [rt.jar:1.8.0_151]\r\n\t... 93 more\r\n```"
            },
            {
              "comment_username": "jbertram",
              "comment_create_time": "2020-09-14T19:27:32Z",
              "comment_edit_time": "2020-09-14T19:27:32Z",
              "comment_text": "I think I see what the \"issue\" was when clicking on anything...I was expecting to see the attributes of the object when I clicked on it. However, that information is on the \"Attributes\" tab. The default tab (i.e. the \"Status\" tab) continues to display when clicking on anything in the tree. Fair enough."
            },
            {
              "comment_username": "andytaylor",
              "comment_create_time": "2020-09-15T11:20:28Z",
              "comment_edit_time": "2020-09-15T11:20:28Z",
              "comment_text": "@jbertram The npe is caused because we dont create a guard if no access list is configured. I'll add a check to only add  the security mbean if this is not null"
            },
            {
              "comment_username": "andytaylor",
              "comment_create_time": "2020-09-15T11:52:19Z",
              "comment_edit_time": "2020-09-15T11:52:19Z",
              "comment_text": "> @jbertram The npe is caused because we dont create a guard if no access list is configured. I'll add a check to only add the security mbean if this is not null\r\n\r\nThis is now fixed"
            },
            {
              "comment_username": "michaelpearce-gain",
              "comment_create_time": "2020-09-17T11:00:23Z",
              "comment_edit_time": "2020-09-17T11:01:23Z",
              "comment_text": "A few look and feel bits, on login the brand seems a bit off having some kind of greeny yellow glow, maybe should be the magenta, like wise button be magenta and not hard white background but more transparent\r\n\r\n![image](https://user-images.githubusercontent.com/50627341/93461774-29be0000-f8dd-11ea-8ba4-c9bb360828f2.png)\r\n\r\ncompared with\r\n\r\n![image](https://user-images.githubusercontent.com/50627341/93461840-41958400-f8dd-11ea-9bde-72a6c7aa951c.png)\r\n\r\n\r\n\r\nalso there seems to be no remember me option."
            },
            {
              "comment_username": "michaelpearce-gain",
              "comment_create_time": "2020-09-17T11:05:23Z",
              "comment_edit_time": "2020-09-17T11:13:19Z",
              "comment_text": "Next again on brand, something with the fonts seem off, see here it seems to have gone for some times new roman kind of font, and hover is blue not magenta etc.\r\n\r\n![image](https://user-images.githubusercontent.com/50627341/93462379-24ad8080-f8de-11ea-9c72-46f89018bd29.png)\r\n\r\nwhere as previous we had a smoother inline with logo - using opensans and hover with magenta lines, highlight and text on hover\r\n\r\n![image](https://user-images.githubusercontent.com/50627341/93462033-8de0c400-f8dd-11ea-8fa4-57c6167d05a3.png)\r\n\r\nalot of this was in css here:\r\n\r\nhttps://github.com/apache/activemq-artemis/blob/master/artemis-hawtio/activemq-branding/src/main/webapp/plugin/css/activemq.css\r\n\r\n\r\n"
            },
            {
              "comment_username": "michaelpearce-gain",
              "comment_create_time": "2020-09-17T11:07:54Z",
              "comment_edit_time": "2020-09-17T11:08:56Z",
              "comment_text": "functionality wise, everything seems to be there, so its just activemq branding that needs some attention here, which was very important when we first introduced the console, as some in the PMC we very heavy they wanted it to look like an activemq thing, not a hawtio default. thus why we spent that time originally with all the custom brand/theme/css"
            },
            {
              "comment_username": "michaelpearce-gain",
              "comment_create_time": "2020-09-17T11:15:59Z",
              "comment_edit_time": "2020-09-17T11:16:06Z",
              "comment_text": "Lastly, once theming is fixed, i would expect we probably want new screenshots for the docs, and docs updated with new screenshots.,"
            },
            {
              "comment_username": "andytaylor",
              "comment_create_time": "2020-09-17T12:24:12Z",
              "comment_edit_time": "2020-09-17T12:24:12Z",
              "comment_text": "I'll take a look at the themes and see if I can cop over and get working all of the old activemq.css. I'll get on this next week."
            },
            {
              "comment_username": "andytaylor",
              "comment_create_time": "2020-09-24T08:17:51Z",
              "comment_edit_time": "2020-09-24T08:17:51Z",
              "comment_text": "Ive gone thru and fixed up the css so magenta is used for hovers and tables etc. Also fixed up the login page and quite a few other areas.\r\n\r\n@michaelandrepearce could you give it another look and I can make another pass with any other issues on branding you find.\r\n\r\nre the remember me, this isnt possible at the moment but the HawtIO team will fix it and we can update HawtIO once available, its tracked at https://github.com/hawtio/hawtio/issues/2652"
            },
            {
              "comment_username": "andytaylor",
              "comment_create_time": "2020-09-24T12:24:05Z",
              "comment_edit_time": "2020-09-24T12:24:05Z",
              "comment_text": "actually re the rememberme, do we really need or want this. HawtIO team think its a bad idea?"
            },
            {
              "comment_username": "michaelpearce-gain",
              "comment_create_time": "2020-09-24T12:33:51Z",
              "comment_edit_time": "2020-09-24T12:33:51Z",
              "comment_text": "i cannot speak for all users, but its used where i am working."
            }
          ]
        },
        {
          "issue_url": "https://github.com/apache/activemq-artemis/issues/3240",
          "issue_title": "ARTEMIS-2921:Upgrade to Netty 4.1.51.Final and netty-tcnative 2.0.33.Final",
          "issue_number": 3240,
          "issue_text": "Upgrade netty and netty-tcnative to its latest version which includes for security fixes and AArch64 performance improvements.\r\nRefer release notes for detail: https://netty.io/news/2020/05/13/4-1-50-Final.html\r\n\r\nSigned-off-by: odidev <odidev@puresoftware.com>",
          "issue_comments": [
            {
              "comment_username": "odidev",
              "comment_create_time": "2020-09-03T07:23:38Z",
              "comment_edit_time": "2020-09-03T07:23:38Z",
              "comment_text": "Hi Team,\r\nPlease review this PR. "
            },
            {
              "comment_username": "odidev",
              "comment_create_time": "2020-09-28T06:43:02Z",
              "comment_edit_time": "2020-09-28T06:43:02Z",
              "comment_text": "Hi Team,\r\nPlease review this PR."
            },
            {
              "comment_username": "brusdev",
              "comment_create_time": "2020-09-28T13:36:08Z",
              "comment_edit_time": "2020-09-28T13:36:08Z",
              "comment_text": "You need a JIRA like this https://issues.apache.org/jira/browse/ARTEMIS-2727"
            },
            {
              "comment_username": "brusdev",
              "comment_create_time": "2020-09-29T13:49:42Z",
              "comment_edit_time": "2020-09-29T13:49:42Z",
              "comment_text": "@odidev can you squash and amend your commits according to https://github.com/apache/activemq-artemis/blob/master/docs/hacking-guide/en/code.md#typical-development-cycle ?\r\nEverything else looks good to me."
            }
          ]
        },
        {
          "issue_url": "https://github.com/apache/activemq-artemis/issues/3204",
          "issue_title": "ARTEMIS-2823 Use datasource with JDBC store db connections",
          "issue_number": 3204,
          "issue_text": "Replaces direct jdbc connections with dbcp2 datasource. Adda\r\nconfiguration options to use alternative datasources and to alter the\r\nparameters. While adding slight overhead, this vastly improves the\r\nmanagement and pooling capabilities with db connections.",
          "issue_comments": [
            {
              "comment_username": "sebthom",
              "comment_create_time": "2020-06-29T07:21:00Z",
              "comment_edit_time": "2020-06-29T07:21:00Z",
              "comment_text": "Before on settling down on dbcp. Did you evaluate other connection pool implementations. I remember that dbcp had performance and memory issues in the past (maybe nowadays too?). HikariCP might be a better alternative.\r\n\r\nhttps://github.com/brettwooldridge/HikariCP\r\nhttps://programminghints.com/2017/07/hikaricp/\r\n"
            },
            {
              "comment_username": "uomik",
              "comment_create_time": "2020-06-29T07:50:29Z",
              "comment_edit_time": "2020-06-29T07:50:29Z",
              "comment_text": "> Before on settling down on dbcp. Did you evaluate other connection pool implementations. I remember that dbcp had performance and memory issues in the past (maybe nowadays too?). HikariCP might be a better alternative.\r\n\r\nYes I did evaluate HikariCP and c3p0 (https://www.mchange.com/projects/c3p0/) along with dbcp2. Each of them worked just fine and I saw no noticeable performance difference beween them. Yes there used to be some major performance issues with dbcp2, but to my knowledge the latest version is much better. I did not run any real performance tests with each of the connection pools, but have done it with other applications. Dbcp2 has been slightly slower, but proven to be more reliable in an environment where database drops connections very aggressively, when compared to HikariCP. c3p0 was simply dropped because I'm not that familiar with it.\r\n\r\nAt the end, I made the choice based on the fact that with default settings dbcp2 was the only one that I got to work flawlessly in our test cluster. Both HikariCP and c3p0 required some extra configuration."
            },
            {
              "comment_username": "franz1981",
              "comment_create_time": "2020-09-10T20:03:20Z",
              "comment_edit_time": "2020-09-10T20:03:20Z",
              "comment_text": "@uomik \r\nI'm getting some test failures eg `JDBCJournalTest::testCleanupTxRecords`:\r\n```\r\n[main] 22:01:31,134 INFO  [org.apache.activemq.artemis.tests.util.ActiveMQTestBase] **** start #test testCleanupTxRecords[authentication = false]() ***\r\n[main] 22:01:31,136 INFO  [org.apache.activemq.artemis.jdbc.store.drivers.JDBCDataSourceUtils] Initialising JDBC data source: org.apache.commons.dbcp2.BasicDataSource {driverClassName=org.apache.derby.jdbc.EmbeddedDriver, url=jdbc:derby:/home/forked_franz/IdeaProjects/activemq-artemis-upstream/tests/integration-tests/./target/tmp/JDBCJournalTest/junit12974489528425881245/derby;create=true}\r\n[main] 22:01:31,878 INFO  [org.apache.activemq.artemis.tests.util.ActiveMQTestBase] **** end #test testCleanupTxRecords[authentication = false]() ***\r\n\r\njava.lang.AssertionError: \r\nExpected :0\r\nActual   :1\r\n<Click to see difference>\r\n\r\n\r\n\tat org.junit.Assert.fail(Assert.java:88)\r\n\tat org.junit.Assert.failNotEquals(Assert.java:743)\r\n\tat org.junit.Assert.assertEquals(Assert.java:118)\r\n\tat org.junit.Assert.assertEquals(Assert.java:555)\r\n\tat org.junit.Assert.assertEquals(Assert.java:542)\r\n\tat org.apache.activemq.artemis.tests.integration.jdbc.store.journal.JDBCJournalTest.testCleanupTxRecords(JDBCJournalTest.java:175)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n\tat org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)\r\n\tat org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)\r\n\tat org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)\r\n\tat org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)\r\n\tat org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)\r\n\tat org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)\r\n\tat org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55)\r\n\tat org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55)\r\n\tat org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)\r\n\tat org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55)\r\n\tat org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)\r\n\tat org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)\r\n\tat org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55)\r\n\tat org.junit.rules.RunRules.evaluate(RunRules.java:20)\r\n\tat org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)\r\n\tat org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)\r\n\tat org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)\r\n\tat org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)\r\n\tat org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)\r\n\tat org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)\r\n\tat org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)\r\n\tat org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)\r\n\tat org.junit.runners.ParentRunner.run(ParentRunner.java:309)\r\n\tat org.junit.runners.Suite.runChild(Suite.java:127)\r\n\tat org.junit.runners.Suite.runChild(Suite.java:26)\r\n\tat org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)\r\n\tat org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)\r\n\tat org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)\r\n\tat org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)\r\n\tat org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)\r\n\tat org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55)\r\n\tat org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55)\r\n\tat org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55)\r\n\tat org.junit.rules.RunRules.evaluate(RunRules.java:20)\r\n\tat org.junit.runners.ParentRunner.run(ParentRunner.java:309)\r\n\tat org.junit.runner.JUnitCore.run(JUnitCore.java:160)\r\n\tat com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:69)\r\n\tat com.intellij.rt.junit.IdeaTestRunner$Repeater.startRunnerWithArgs(IdeaTestRunner.java:33)\r\n\tat com.intellij.rt.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:220)\r\n\tat com.intellij.rt.junit.JUnitStarter.main(JUnitStarter.java:53)\r\n```"
            },
            {
              "comment_username": "franz1981",
              "comment_create_time": "2020-09-10T21:31:11Z",
              "comment_edit_time": "2020-09-10T21:31:11Z",
              "comment_text": "@uomik I'm sending a PR to your branch to fix the error"
            },
            {
              "comment_username": "franz1981",
              "comment_create_time": "2020-09-10T21:38:45Z",
              "comment_edit_time": "2020-09-10T21:45:01Z",
              "comment_text": "Same for the tests using JDBC authentication, but probably is just a derby issue \r\neg\r\n`JDBJouurnalTest::testInsertRecords` with `authentication = true` \r\n```\r\n[main] 23:43:39,138 INFO  [org.apache.activemq.artemis.tests.util.ActiveMQTestBase] **** start #test testInsertRecords[authentication = true]() ***\r\n[main] 23:43:39,288 INFO  [org.apache.activemq.artemis.jdbc.store.drivers.JDBCDataSourceUtils] Initialising JDBC data source: org.apache.commons.dbcp2.BasicDataSource {driverClassName=org.apache.derby.jdbc.EmbeddedDriver, url=jdbc:derby:/home/forked_franz/IdeaProjects/activemq-artemis-upstream/tests/integration-tests/./target/tmp/JDBCJournalTest/junit7593526893071523014/derby;create=true}\r\n[main] 23:43:39,582 ERROR [org.apache.activemq.artemis.jdbc.store.drivers.JDBCConnectionProvider] \r\nSQL EXCEPTIONS: \r\nSQLState: null ErrorCode: 0 Message: Cannot create PoolableConnectionFactory (Connection authentication failure occurred.  Reason: Invalid authentication..)\r\n[main] 23:43:39,583 ERROR [org.apache.activemq.artemis.jdbc.store.drivers.JDBCConnectionProvider] \r\n```"
            },
            {
              "comment_username": "franz1981",
              "comment_create_time": "2020-09-10T22:30:42Z",
              "comment_edit_time": "2020-09-10T22:30:42Z",
              "comment_text": "Please rebase over master and try the latest commits on https://github.com/franz1981/activemq-artemis/tree/2.13.0-jdbc-ds\r\nSeems that I cannot send a PR to your branch"
            },
            {
              "comment_username": "franz1981",
              "comment_create_time": "2020-09-14T14:36:15Z",
              "comment_edit_time": "2020-09-14T14:36:15Z",
              "comment_text": "@uomik any news on the changes I've proposed on https://github.com/apache/activemq-artemis/pull/3204#issuecomment-690766208?"
            },
            {
              "comment_username": "uomik",
              "comment_create_time": "2020-09-14T14:56:56Z",
              "comment_edit_time": "2020-09-14T15:01:41Z",
              "comment_text": "> @uomik any news on the changes I've proposed on [#3204 (comment)](https://github.com/apache/activemq-artemis/pull/3204#issuecomment-690766208)?\r\n\r\nSorry. I'm burden under work currently. Had a quick look on changes (that seem to be ok and on point) and did the rebase, but was little confused about the outcome (I'm no means a git expert), and have had absolutely no time to dig into it. The unit tests did go through though."
            },
            {
              "comment_username": "franz1981",
              "comment_create_time": "2020-09-14T15:05:50Z",
              "comment_edit_time": "2020-09-14T15:06:11Z",
              "comment_text": "@uomik I will try to send a PR to your repository so you can accept/merge it and push force the new version, do you need me to help more?"
            },
            {
              "comment_username": "franz1981",
              "comment_create_time": "2020-09-14T15:11:34Z",
              "comment_edit_time": "2020-09-14T15:11:34Z",
              "comment_text": "@uomik \r\nThat's the PR: https://github.com/uomik/activemq-artemis/pull/1\r\n\r\n"
            },
            {
              "comment_username": "franz1981",
              "comment_create_time": "2020-09-15T11:06:38Z",
              "comment_edit_time": "2020-09-15T11:24:18Z",
              "comment_text": "@uomik Just a note: I see that you have not modified both deps and features hence the commons-dbcp2 artifact is not deployed with the broker libs: this is done on purpose? How you do in production, if you're using this?"
            },
            {
              "comment_username": "uomik",
              "comment_create_time": "2020-09-15T11:22:47Z",
              "comment_edit_time": "2020-09-15T11:22:47Z",
              "comment_text": "> @uomik Just a note: I see that you have modified both deps and features hence the commons-dbcp2 artifact is not deployed with the broker libs: this is done on purpose? How you do in production, if you're using this?\r\n\r\nThis was not intended, if this is the case it's definitely a mistake, or something I have overlooked.\r\n\r\nWe are not yet on production with this feature. In test environment we patch the official Artemis release with modified jars and add commons-dbcp2, commons-logging and commons-pool2 jars to lib (along with jdbc driver). This is done in CI/CD pipeline. We chose this approach simply because it was easier in our environment when developing this datasource feature and is in no means the way we would like to do things in production."
            },
            {
              "comment_username": "franz1981",
              "comment_create_time": "2020-09-15T11:43:40Z",
              "comment_edit_time": "2020-09-15T11:43:40Z",
              "comment_text": "thanks @uomik \\r\\n\\r\\n> In test environment we patch the official Artemis release with modified jars and add commons-dbcp2, commons-logging and commons-pool2 jars to lib (along with jdbc driver)\\r\\n\\r\\nProbably is a better way to handle this for users too: forcing a specific data source isn't good although we assume dbcp2 to be used (looking at the unwrapping logic for PosgresSQL) or whatever pooled datasource. \\r\\nUsers with existing JDBC connection configuration need to change their config (and libs) if they want \"decent\" performance"
            },
            {
              "comment_username": "franz1981",
              "comment_create_time": "2020-09-15T18:03:24Z",
              "comment_edit_time": "2020-09-15T18:03:24Z",
              "comment_text": "I've managed to run some tests with MySql and found something odd: if you add this configuration to `broker.xml`:\r\n```xml\r\n\t<ha-policy>\r\n\t    <shared-store>\r\n\t        <master/>\r\n\t    </shared-store>\r\n\t</ha-policy>\r\n```\r\nAnd stop the database, the broker will stop after:\r\n```\r\n2020-09-15 19:59:05,080 WARN  [org.apache.activemq.artemis.core.server] AMQ222010: Critical IO Error, shutting down the server. file=NULL, message=Critical error while on live renew: java.lang.IllegalStateException: com.mysql.cj.jdbc.exceptions.CommunicationsException: Communications link failure\r\n\r\nThe last packet successfully received from the server was 2 milliseconds ago. The last packet sent successfully to the server was 2 milliseconds ago.\r\n        at org.apache.activemq.artemis.core.server.impl.jdbc.JdbcLeaseLock.renew(JdbcLeaseLock.java:169) [artemis-server-2.14.0-SNAPSHOT.jar:2.14.0-SNAPSHOT]\r\n        at org.apache.activemq.artemis.core.server.impl.jdbc.ActiveMQScheduledLeaseLock.run(ActiveMQScheduledLeaseLock.java:91) [artemis-server-2.14.0-SNAPSHOT.jar:2.14.0-SNAPSHOT]\r\n        at org.apache.activemq.artemis.core.server.ActiveMQScheduledComponent$2.run(ActiveMQScheduledComponent.java:306) [artemis-commons-2.14.0-SNAPSHOT.jar:2.14.0-SNAPSHOT]\r\n        at org.apache.activemq.artemis.utils.actors.OrderedExecutor.doTask(OrderedExecutor.java:42) [artemis-commons-2.14.0-SNAPSHOT.jar:2.14.0-SNAPSHOT]\r\n        at org.apache.activemq.artemis.utils.actors.OrderedExecutor.doTask(OrderedExecutor.java:31) [artemis-commons-2.14.0-SNAPSHOT.jar:2.14.0-SNAPSHOT]\r\n        at org.apache.activemq.artemis.utils.actors.ProcessorBase.executePendingTasks(ProcessorBase.java:65) [artemis-commons-2.14.0-SNAPSHOT.jar:2.14.0-SNAPSHOT]\r\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [rt.jar:1.8.0_265]\r\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [rt.jar:1.8.0_265]\r\n        at org.apache.activemq.artemis.utils.ActiveMQThreadFactory$1.run(ActiveMQThreadFactory.java:118) [artemis-commons-2.14.0-SNAPSHOT.jar:2.14.0-SNAPSHOT]\r\nCaused by: com.mysql.cj.jdbc.exceptions.CommunicationsException: Communications link failure\r\n```\r\nI suppose that the data source pool need to be configured somehow or the lease lock renew will make it stop. \r\nThat's the expected behaviour?"
            },
            {
              "comment_username": "uomik",
              "comment_create_time": "2020-09-16T05:30:59Z",
              "comment_edit_time": "2020-09-16T05:30:59Z",
              "comment_text": "> I suppose that the data source pool need to be configured somehow or the lease lock renew will make it stop.\r\n> That's the expected behaviour?\r\n\r\nThe drawback with shared store, being database or something else, is that it essentially becomes a single point of failure for the whole cluster. If we lose the database, we lose the ability to sync data between master and backup, the ability to ensure data integrity and broker high availability. In this case, on my opinion, the only thing we can do is to shut down the broker node as precautionary measure. This is the way it was before I fiddled with anything, and I saw no reasons to change it.\r\n\r\nThe way you can mitigate this single point of failure is to have multiple master-backup pairs distributed accross multiple databases (servers). Or build some redundancy in the infrastrusture (clustering the database etc.)."
            },
            {
              "comment_username": "franz1981",
              "comment_create_time": "2020-09-16T05:49:50Z",
              "comment_edit_time": "2020-09-16T05:50:47Z",
              "comment_text": "@uomik \r\n\r\n> The drawback with shared store, being database or something else, is that it essentially becomes a single point of failure for the whole cluster. If we lose the database, we lose the ability to sync data between master and backup, the ability to ensure data integrity and broker high availability. In this case, on my opinion, the only thing we can do is to shut down the broker node as precautionary measure. This is the way it was before I fiddled with anything, and I saw no reasons to change it.\r\n\r\nAgree, but consider the simple case of a database upgrade ( ie a database restart): a shared store implies a live and backup connected to it; the expectation during a database upgrade would be that the live will go down (that's what is happening now) while the slave should keep trying to became live (ie while now it just die).\r\nProbably the backup behaviour of shared store ha should be different. It seems a separate problem to me then this PR, just thinking loud."
            },
            {
              "comment_username": "uomik",
              "comment_create_time": "2020-09-16T06:33:06Z",
              "comment_edit_time": "2020-09-16T06:33:06Z",
              "comment_text": "@franz1981 \r\n\r\n> Agree, but consider the simple case of a database upgrade ( ie a database restart): a shared store implies a live and backup connected to it; the expectation during a database upgrade would be that the live will go down (that's what is happening now) while the slave should keep trying to became live (ie while now it just die).\r\n> Probably the backup behaviour of shared store ha should be different. It seems a separate problem to me then this PR, just thinking loud.\r\n\r\nMy time to completely agree :) I thought you were just referring to the behaviour of the master. Backup should definitely behave differently on these occasions. "
            },
            {
              "comment_username": "franz1981",
              "comment_create_time": "2020-09-16T07:57:38Z",
              "comment_edit_time": "2020-09-16T07:57:38Z",
              "comment_text": "I've found another issue with shared store HA on live stop:\r\n```\r\n2020-09-16 09:51:36,116 WARN  [org.apache.activemq.artemis.jdbc.store.drivers.JDBCConnectionProvider] Unable to set a network timeout on the JDBC connection: java.util.concurrent.RejectedExecutionException: Task org.apache.activemq.artemis.utils.actors.ProcessorBase$$Lambda$61/1932332324@40491594 rejected from org.apache.activemq.artemis.utils.ActiveMQThreadPoolExecutor@6cfdff08[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 92]\r\n        at java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2063) [rt.jar:1.8.0_265]\r\n        at java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:830) [rt.jar:1.8.0_265]\r\n        at java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1379) [rt.jar:1.8.0_265]\r\n        at org.apache.activemq.artemis.utils.actors.ProcessorBase.onAddedTaskIfNotRunning(ProcessorBase.java:186) [artemis-commons-2.14.0-SNAPSHOT.jar:2.14.0-SNAPSHOT]\r\n        at org.apache.activemq.artemis.utils.actors.ProcessorBase.task(ProcessorBase.java:174) [artemis-commons-2.14.0-SNAPSHOT.jar:2.14.0-SNAPSHOT]\r\n        at org.apache.activemq.artemis.utils.actors.OrderedExecutor.execute(OrderedExecutor.java:54) [artemis-commons-2.14.0-SNAPSHOT.jar:2.14.0-SNAPSHOT]\r\n        at com.mysql.cj.jdbc.ConnectionImpl.setNetworkTimeout(ConnectionImpl.java:2488) [mysql-connector-java-8.0.21.jar:8.0.21]\r\n        at org.apache.commons.dbcp2.Jdbc41Bridge.setNetworkTimeout(Jdbc41Bridge.java:386) [commons-dbcp2-2.7.0.jar:2.7.0]\r\n        at org.apache.commons.dbcp2.DelegatingConnection.setNetworkTimeout(DelegatingConnection.java:985) [commons-dbcp2-2.7.0.jar:2.7.0]\r\n        at org.apache.commons.dbcp2.Jdbc41Bridge.setNetworkTimeout(Jdbc41Bridge.java:386) [commons-dbcp2-2.7.0.jar:2.7.0]\r\n        at org.apache.commons.dbcp2.DelegatingConnection.setNetworkTimeout(DelegatingConnection.java:985) [commons-dbcp2-2.7.0.jar:2.7.0]\r\n        at org.apache.activemq.artemis.jdbc.store.drivers.JDBCConnectionProvider.getConnection(JDBCConnectionProvider.java:62) [artemis-jdbc-store-2.14.0-SNAPSHOT.jar:2.14.0-SNAPSHOT]\r\n        at org.apache.activemq.artemis.core.server.impl.jdbc.JdbcLeaseLock.release(JdbcLeaseLock.java:267) [artemis-server-2.14.0-SNAPSHOT.jar:2.14.0-SNAPSHOT]\r\n        at org.apache.activemq.artemis.core.server.impl.jdbc.JdbcNodeManager.pauseLiveServer(JdbcNodeManager.java:413) [artemis-server-2.14.0-SNAPSHOT.jar:2.14.0-SNAPSHOT]\r\n        at org.apache.activemq.artemis.core.server.impl.SharedStoreLiveActivation.close(SharedStoreLiveActivation.java:161) [artemis-server-2.14.0-SNAPSHOT.jar:2.14.0-SNAPSHOT]\r\n        at org.apache.activemq.artemis.core.server.impl.ActiveMQServerImpl.stop(ActiveMQServerImpl.java:1247) [artemis-server-2.14.0-SNAPSHOT.jar:2.14.0-SNAPSHOT]\r\n        at org.apache.activemq.artemis.core.server.impl.ActiveMQServerImpl.stop(ActiveMQServerImpl.java:1033) [artemis-server-2.14.0-SNAPSHOT.jar:2.14.0-SNAPSHOT]\r\n        at org.apache.activemq.artemis.core.server.impl.ActiveMQServerImpl.stop(ActiveMQServerImpl.java:876) [artemis-server-2.14.0-SNAPSHOT.jar:2.14.0-SNAPSHOT]\r\n        at org.apache.activemq.artemis.integration.FileBroker.stop(FileBroker.java:98) [artemis-cli-2.14.0-SNAPSHOT.jar:2.14.0-SNAPSHOT]\r\n        at org.apache.activemq.artemis.cli.commands.Run.stop(Run.java:166) [artemis-cli-2.14.0-SNAPSHOT.jar:2.14.0-SNAPSHOT]\r\n        at org.apache.activemq.artemis.cli.commands.Run$2.run(Run.java:157) [artemis-cli-2.14.0-SNAPSHOT.jar:2.14.0-SNAPSHOT]\r\n```\r\n`activation.close(failoverOnServerShutdown, restarting);` on server stop is causing `JdbcLeaseLock.release` to set a network timeout on the JDBC connection while using an already closed executor (previously on stop). \r\nNeed to check how to fix this\r\n"
            },
            {
              "comment_username": "uomik",
              "comment_create_time": "2020-09-16T08:19:33Z",
              "comment_edit_time": "2020-09-16T08:19:33Z",
              "comment_text": "> `activation.close(failoverOnServerShutdown, restarting);` on server stop is causing `JdbcLeaseLock.release` to set a network timeout on the JDBC connection while using an already closed executor (previously on stop).\r\n> Need to check how to fix this\r\n\r\nI've come across this as well. After investigating I came to conclusion that this might be an issue with MySQL or Connector/J. But have not yet had a proper look into this, so might be something else."
            },
            {
              "comment_username": "franz1981",
              "comment_create_time": "2020-09-16T13:09:08Z",
              "comment_edit_time": "2020-09-16T13:09:08Z",
              "comment_text": "Nope it seems related to how we configure the network connection timeout (still need to understand if it makes sense...).\r\n\r\nI see a similar problem on backup side while live is restarting, see:\r\n```\r\n2020-09-16 15:07:09,990 INFO  [org.apache.activemq.artemis.core.server] AMQ221008: live server wants to restart, restarting server in backup\r\n2020-09-16 15:07:10,181 INFO  [org.apache.activemq.artemis.core.server] AMQ221002: Apache ActiveMQ Artemis Message Broker version 2.14.0-SNAPSHOT [b25dd37c-f77c-11ea-9566-1aabf22f14df] stopped, uptime 1 minute\r\n2020-09-16 15:07:10,183 WARN  [org.apache.activemq.artemis.jdbc.store.drivers.JDBCConnectionProvider] Unable to set a network timeout on the JDBC connection: java.util.concurrent.RejectedExecutionException: Task org.apache.activemq.artemis.utils.actors.ProcessorBase$$Lambda$61/1394940518@15f4294c rejected from org.apache.activemq.artemis.utils.ActiveMQThreadPoolExecutor@69d38d0a[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 265]\r\n        at java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2063) [rt.jar:1.8.0_265]\r\n        at java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:830) [rt.jar:1.8.0_265]\r\n        at java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1379) [rt.jar:1.8.0_265]\r\n        at org.apache.activemq.artemis.utils.actors.ProcessorBase.onAddedTaskIfNotRunning(ProcessorBase.java:186) [artemis-commons-2.14.0-SNAPSHOT.jar:2.14.0-SNAPSHOT]\r\n        at org.apache.activemq.artemis.utils.actors.ProcessorBase.task(ProcessorBase.java:174) [artemis-commons-2.14.0-SNAPSHOT.jar:2.14.0-SNAPSHOT]\r\n        at org.apache.activemq.artemis.utils.actors.OrderedExecutor.execute(OrderedExecutor.java:54) [artemis-commons-2.14.0-SNAPSHOT.jar:2.14.0-SNAPSHOT]\r\n        at com.mysql.cj.jdbc.ConnectionImpl.setNetworkTimeout(ConnectionImpl.java:2488) [mysql-connector-java-8.0.21.jar:8.0.21]\r\n        at org.apache.commons.dbcp2.Jdbc41Bridge.setNetworkTimeout(Jdbc41Bridge.java:386) [commons-dbcp2-2.7.0.jar:2.7.0]\r\n        at org.apache.commons.dbcp2.DelegatingConnection.setNetworkTimeout(DelegatingConnection.java:985) [commons-dbcp2-2.7.0.jar:2.7.0]\r\n        at org.apache.commons.dbcp2.Jdbc41Bridge.setNetworkTimeout(Jdbc41Bridge.java:386) [commons-dbcp2-2.7.0.jar:2.7.0]\r\n        at org.apache.commons.dbcp2.DelegatingConnection.setNetworkTimeout(DelegatingConnection.java:985) [commons-dbcp2-2.7.0.jar:2.7.0]\r\n        at org.apache.activemq.artemis.jdbc.store.drivers.JDBCConnectionProvider.getConnection(JDBCConnectionProvider.java:62) [artemis-jdbc-store-2.14.0-SNAPSHOT.jar:2.14.0-SNAPSHOT]\r\n        at org.apache.activemq.artemis.jdbc.store.drivers.AbstractJDBCDriver.createTableIfNotExists(AbstractJDBCDriver.java:90) [artemis-jdbc-store-2.14.0-SNAPSHOT.jar:2.14.0-SNAPSHOT]\r\n        at org.apache.activemq.artemis.jdbc.store.drivers.AbstractJDBCDriver.createTable(AbstractJDBCDriver.java:63) [artemis-jdbc-store-2.14.0-SNAPSHOT.jar:2.14.0-SNAPSHOT]\r\n        at org.apache.activemq.artemis.core.server.impl.jdbc.JdbcSharedStateManager.createSchema(JdbcSharedStateManager.java:68) [artemis-server-2.14.0-SNAPSHOT.jar:2.14.0-SNAPSHOT]\r\n        at org.apache.activemq.artemis.jdbc.store.drivers.AbstractJDBCDriver.start(AbstractJDBCDriver.java:50) [artemis-jdbc-store-2.14.0-SNAPSHOT.jar:2.14.0-SNAPSHOT]\r\n        at org.apache.activemq.artemis.core.server.impl.jdbc.JdbcSharedStateManager.usingConnectionProvider(JdbcSharedStateManager.java:58) [artemis-server-2.14.0-SNAPSHOT.jar:2.14.0-SNAPSHOT]\r\n        at org.apache.activemq.artemis.core.server.impl.jdbc.JdbcNodeManager.lambda$usingConnectionProvider$0(JdbcNodeManager.java:91) [artemis-server-2.14.0-SNAPSHOT.jar:2.14.0-SNAPSHOT]\r\n        at org.apache.activemq.artemis.core.server.impl.jdbc.JdbcNodeManager.start(JdbcNodeManager.java:162) [artemis-server-2.14.0-SNAPSHOT.jar:2.14.0-SNAPSHOT]\r\n        at org.apache.activemq.artemis.core.server.impl.SharedStoreBackupActivation$FailbackChecker$1.run(SharedStoreBackupActivation.java:217) [artemis-server-2.14.0-SNAPSHOT.jar:2.14.0-SNAPSHOT]\r\n        at java.lang.Thread.run(Thread.java:748) [rt.jar:1.8.0_265]\r\n\r\n2020-09-16 15:07:10,191 WARN  [org.apache.activemq.artemis.jdbc.store.drivers.JDBCConnectionProvider] Unable to set a network timeout on the JDBC connection: java.util.concurrent.RejectedExecutionException: Task org.apache.activemq.artemis.utils.actors.ProcessorBase$$Lambda$61/1394940518@15f4294c rejected from org.apache.activemq.artemis.utils.ActiveMQThreadPoolExecutor@69d38d0a[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 265]\r\n        at java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2063) [rt.jar:1.8.0_265]\r\n        at java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:830) [rt.jar:1.8.0_265]\r\n        at java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1379) [rt.jar:1.8.0_265]\r\n        at org.apache.activemq.artemis.utils.actors.ProcessorBase.onAddedTaskIfNotRunning(ProcessorBase.java:186) [artemis-commons-2.14.0-SNAPSHOT.jar:2.14.0-SNAPSHOT]\r\n        at org.apache.activemq.artemis.utils.actors.ProcessorBase.task(ProcessorBase.java:174) [artemis-commons-2.14.0-SNAPSHOT.jar:2.14.0-SNAPSHOT]\r\n        at org.apache.activemq.artemis.utils.actors.OrderedExecutor.execute(OrderedExecutor.java:54) [artemis-commons-2.14.0-SNAPSHOT.jar:2.14.0-SNAPSHOT]\r\n        at com.mysql.cj.jdbc.ConnectionImpl.setNetworkTimeout(ConnectionImpl.java:2488) [mysql-connector-java-8.0.21.jar:8.0.21]\r\n        at org.apache.commons.dbcp2.Jdbc41Bridge.setNetworkTimeout(Jdbc41Bridge.java:386) [commons-dbcp2-2.7.0.jar:2.7.0]\r\n        at org.apache.commons.dbcp2.DelegatingConnection.setNetworkTimeout(DelegatingConnection.java:985) [commons-dbcp2-2.7.0.jar:2.7.0]\r\n        at org.apache.commons.dbcp2.Jdbc41Bridge.setNetworkTimeout(Jdbc41Bridge.java:386) [commons-dbcp2-2.7.0.jar:2.7.0]\r\n        at org.apache.commons.dbcp2.DelegatingConnection.setNetworkTimeout(DelegatingConnection.java:985) [commons-dbcp2-2.7.0.jar:2.7.0]\r\n        at org.apache.activemq.artemis.jdbc.store.drivers.JDBCConnectionProvider.getConnection(JDBCConnectionProvider.java:62) [artemis-jdbc-store-2.14.0-SNAPSHOT.jar:2.14.0-SNAPSHOT]\r\n        at org.apache.activemq.artemis.core.server.impl.jdbc.JdbcSharedStateManager.setup(JdbcSharedStateManager.java:193) [artemis-server-2.14.0-SNAPSHOT.jar:2.14.0-SNAPSHOT]\r\n        at org.apache.activemq.artemis.core.server.impl.jdbc.JdbcNodeManager.start(JdbcNodeManager.java:164) [artemis-server-2.14.0-SNAPSHOT.jar:2.14.0-SNAPSHOT]\r\n        at org.apache.activemq.artemis.core.server.impl.SharedStoreBackupActivation$FailbackChecker$1.run(SharedStoreBackupActivation.java:217) [artemis-server-2.14.0-SNAPSHOT.jar:2.14.0-SNAPSHOT]\r\n        at java.lang.Thread.run(Thread.java:748) [rt.jar:1.8.0_265]\r\n\r\n2020-09-16 15:07:10,195 WARN  [org.apache.activemq.artemis.jdbc.store.drivers.JDBCConnectionProvider] Unable to set a network timeout on the JDBC connection: java.util.concurrent.RejectedExecutionException: Task org.apache.activemq.artemis.utils.actors.ProcessorBase$$Lambda$61/1394940518@15f4294c rejected from org.apache.activemq.artemis.utils.ActiveMQThreadPoolExecutor@69d38d0a[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 265]\r\n        at java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2063) [rt.jar:1.8.0_265]\r\n        at java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:830) [rt.jar:1.8.0_265]\r\n        at java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1379) [rt.jar:1.8.0_265]\r\n        at org.apache.activemq.artemis.utils.actors.ProcessorBase.onAddedTaskIfNotRunning(ProcessorBase.java:186) [artemis-commons-2.14.0-SNAPSHOT.jar:2.14.0-SNAPSHOT]\r\n        at org.apache.activemq.artemis.utils.actors.ProcessorBase.task(ProcessorBase.java:174) [artemis-commons-2.14.0-SNAPSHOT.jar:2.14.0-SNAPSHOT]\r\n        at org.apache.activemq.artemis.utils.actors.OrderedExecutor.execute(OrderedExecutor.java:54) [artemis-commons-2.14.0-SNAPSHOT.jar:2.14.0-SNAPSHOT]\r\n        at com.mysql.cj.jdbc.ConnectionImpl.setNetworkTimeout(ConnectionImpl.java:2488) [mysql-connector-java-8.0.21.jar:8.0.21]\r\n        at org.apache.commons.dbcp2.Jdbc41Bridge.setNetworkTimeout(Jdbc41Bridge.java:386) [commons-dbcp2-2.7.0.jar:2.7.0]\r\n        at org.apache.commons.dbcp2.DelegatingConnection.setNetworkTimeout(DelegatingConnection.java:985) [commons-dbcp2-2.7.0.jar:2.7.0]\r\n        at org.apache.commons.dbcp2.Jdbc41Bridge.setNetworkTimeout(Jdbc41Bridge.java:386) [commons-dbcp2-2.7.0.jar:2.7.0]\r\n        at org.apache.commons.dbcp2.DelegatingConnection.setNetworkTimeout(DelegatingConnection.java:985) [commons-dbcp2-2.7.0.jar:2.7.0]\r\n        at org.apache.activemq.artemis.jdbc.store.drivers.JDBCConnectionProvider.getConnection(JDBCConnectionProvider.java:62) [artemis-jdbc-store-2.14.0-SNAPSHOT.jar:2.14.0-SNAPSHOT]\r\n        at org.apache.activemq.artemis.core.server.impl.jdbc.JdbcSharedStateManager.readState(JdbcSharedStateManager.java:285) [artemis-server-2.14.0-SNAPSHOT.jar:2.14.0-SNAPSHOT]\r\n        at org.apache.activemq.artemis.core.server.impl.jdbc.JdbcNodeManager.readSharedState(JdbcNodeManager.java:477) [artemis-server-2.14.0-SNAPSHOT.jar:2.14.0-SNAPSHOT]\r\n        at org.apache.activemq.artemis.core.server.impl.jdbc.JdbcNodeManager.awaitLiveStatus(JdbcNodeManager.java:451) [artemis-server-2.14.0-SNAPSHOT.jar:2.14.0-SNAPSHOT]\r\n        at org.apache.activemq.artemis.core.server.impl.SharedStoreBackupActivation$FailbackChecker$1.run(SharedStoreBackupActivation.java:219) [artemis-server-2.14.0-SNAPSHOT.jar:2.14.0-SNAPSHOT]\r\n        at java.lang.Thread.run(Thread.java:748) [rt.jar:1.8.0_265]\r\n\r\n2020-09-16 15:07:12,198 WARN  [org.apache.activemq.artemis.jdbc.store.drivers.JDBCConnectionProvider] Unable to set a network timeout on the JDBC connection: java.util.concurrent.RejectedExecutionException: Task org.apache.activemq.artemis.utils.actors.ProcessorBase$$Lambda$61/1394940518@15f4294c rejected from org.apache.activemq.artemis.utils.ActiveMQThreadPoolExecutor@69d38d0a[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 265]\r\n        at java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2063) [rt.jar:1.8.0_265]\r\n        at java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:830) [rt.jar:1.8.0_265]\r\n        at java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1379) [rt.jar:1.8.0_265]\r\n        at org.apache.activemq.artemis.utils.actors.ProcessorBase.onAddedTaskIfNotRunning(ProcessorBase.java:186) [artemis-commons-2.14.0-SNAPSHOT.jar:2.14.0-SNAPSHOT]\r\n        at org.apache.activemq.artemis.utils.actors.ProcessorBase.task(ProcessorBase.java:174) [artemis-commons-2.14.0-SNAPSHOT.jar:2.14.0-SNAPSHOT]\r\n        at org.apache.activemq.artemis.utils.actors.OrderedExecutor.execute(OrderedExecutor.java:54) [artemis-commons-2.14.0-SNAPSHOT.jar:2.14.0-SNAPSHOT]\r\n        at com.mysql.cj.jdbc.ConnectionImpl.setNetworkTimeout(ConnectionImpl.java:2488) [mysql-connector-java-8.0.21.jar:8.0.21]\r\n        at org.apache.commons.dbcp2.Jdbc41Bridge.setNetworkTimeout(Jdbc41Bridge.java:386) [commons-dbcp2-2.7.0.jar:2.7.0]\r\n        at org.apache.commons.dbcp2.DelegatingConnection.setNetworkTimeout(DelegatingConnection.java:985) [commons-dbcp2-2.7.0.jar:2.7.0]\r\n        at org.apache.commons.dbcp2.Jdbc41Bridge.setNetworkTimeout(Jdbc41Bridge.java:386) [commons-dbcp2-2.7.0.jar:2.7.0]\r\n        at org.apache.commons.dbcp2.DelegatingConnection.setNetworkTimeout(DelegatingConnection.java:985) [commons-dbcp2-2.7.0.jar:2.7.0]\r\n        at org.apache.activemq.artemis.jdbc.store.drivers.JDBCConnectionProvider.getConnection(JDBCConnectionProvider.java:62) [artemis-jdbc-store-2.14.0-SNAPSHOT.jar:2.14.0-SNAPSHOT]\r\n        at org.apache.activemq.artemis.core.server.impl.jdbc.JdbcSharedStateManager.readState(JdbcSharedStateManager.java:285) [artemis-server-2.14.0-SNAPSHOT.jar:2.14.0-SNAPSHOT]\r\n        at org.apache.activemq.artemis.core.server.impl.jdbc.JdbcNodeManager.readSharedState(JdbcNodeManager.java:477) [artemis-server-2.14.0-SNAPSHOT.jar:2.14.0-SNAPSHOT]\r\n        at org.apache.activemq.artemis.core.server.impl.jdbc.JdbcNodeManager.awaitLiveStatus(JdbcNodeManager.java:451) [artemis-server-2.14.0-SNAPSHOT.jar:2.14.0-SNAPSHOT]\r\n        at org.apache.activemq.artemis.core.server.impl.SharedStoreBackupActivation$FailbackChecker$1.run(SharedStoreBackupActivation.java:219) [artemis-server-2.14.0-SNAPSHOT.jar:2.14.0-SNAPSHOT]\r\n        at java.lang.Thread.run(Thread.java:748) [rt.jar:1.8.0_265]\r\n\r\n2020-09-16 15:07:12,201 WARN  [org.apache.activemq.artemis.jdbc.store.drivers.JDBCConnectionProvider] Unable to set a network timeout on the JDBC connection: java.util.concurrent.RejectedExecutionException: Task org.apache.activemq.artemis.utils.actors.ProcessorBase$$Lambda$61/1394940518@15f4294c rejected from org.apache.activemq.artemis.utils.ActiveMQThreadPoolExecutor@69d38d0a[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 265]\r\n        at java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2063) [rt.jar:1.8.0_265]\r\n        at java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:830) [rt.jar:1.8.0_265]\r\n        at java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1379) [rt.jar:1.8.0_265]\r\n        at org.apache.activemq.artemis.utils.actors.ProcessorBase.onAddedTaskIfNotRunning(ProcessorBase.java:186) [artemis-commons-2.14.0-SNAPSHOT.jar:2.14.0-SNAPSHOT]\r\n        at org.apache.activemq.artemis.utils.actors.ProcessorBase.task(ProcessorBase.java:174) [artemis-commons-2.14.0-SNAPSHOT.jar:2.14.0-SNAPSHOT]\r\n        at org.apache.activemq.artemis.utils.actors.OrderedExecutor.execute(OrderedExecutor.java:54) [artemis-commons-2.14.0-SNAPSHOT.jar:2.14.0-SNAPSHOT]\r\n        at com.mysql.cj.jdbc.ConnectionImpl.setNetworkTimeout(ConnectionImpl.java:2488) [mysql-connector-java-8.0.21.jar:8.0.21]\r\n        at org.apache.commons.dbcp2.Jdbc41Bridge.setNetworkTimeout(Jdbc41Bridge.java:386) [commons-dbcp2-2.7.0.jar:2.7.0]\r\n        at org.apache.commons.dbcp2.DelegatingConnection.setNetworkTimeout(DelegatingConnection.java:985) [commons-dbcp2-2.7.0.jar:2.7.0]\r\n        at org.apache.commons.dbcp2.Jdbc41Bridge.setNetworkTimeout(Jdbc41Bridge.java:386) [commons-dbcp2-2.7.0.jar:2.7.0]\r\n        at org.apache.commons.dbcp2.DelegatingConnection.setNetworkTimeout(DelegatingConnection.java:985) [commons-dbcp2-2.7.0.jar:2.7.0]\r\n        at org.apache.activemq.artemis.jdbc.store.drivers.JDBCConnectionProvider.getConnection(JDBCConnectionProvider.java:62) [artemis-jdbc-store-2.14.0-SNAPSHOT.jar:2.14.0-SNAPSHOT]\r\n        at org.apache.activemq.artemis.jdbc.store.sql.PropertySQLProvider$Factory.investigateDialect(PropertySQLProvider.java:405) [artemis-jdbc-store-2.14.0-SNAPSHOT.jar:2.14.0-SNAPSHOT]\r\n        at org.apache.activemq.artemis.jdbc.store.sql.PropertySQLProvider$Factory.<init>(PropertySQLProvider.java:366) [artemis-jdbc-store-2.14.0-SNAPSHOT.jar:2.14.0-SNAPSHOT]\r\n        at org.apache.activemq.artemis.core.server.impl.jdbc.JdbcNodeManager.with(JdbcNodeManager.java:67) [artemis-server-2.14.0-SNAPSHOT.jar:2.14.0-SNAPSHOT]\r\n        at org.apache.activemq.artemis.core.server.impl.ActiveMQServerImpl.createNodeManager(ActiveMQServerImpl.java:501) [artemis-server-2.14.0-SNAPSHOT.jar:2.14.0-SNAPSHOT]\r\n        at org.apache.activemq.artemis.core.server.impl.ActiveMQServerImpl.internalStart(ActiveMQServerImpl.java:578) [artemis-server-2.14.0-SNAPSHOT.jar:2.14.0-SNAPSHOT]\r\n        at org.apache.activemq.artemis.core.server.impl.ActiveMQServerImpl.start(ActiveMQServerImpl.java:526) [artemis-server-2.14.0-SNAPSHOT.jar:2.14.0-SNAPSHOT]\r\n        at org.apache.activemq.artemis.core.server.impl.SharedStoreBackupActivation$FailbackChecker$1.run(SharedStoreBackupActivation.java:230) [artemis-server-2.14.0-SNAPSHOT.jar:2.14.0-SNAPSHOT]\r\n        at java.lang.Thread.run(Thread.java:748) [rt.jar:1.8.0_265]\r\n\r\n2020-09-16 15:07:12,202 WARN  [org.apache.activemq.artemis.jdbc.store.drivers.JDBCConnectionProvider] Unable to set a network timeout on the JDBC connection: java.util.concurrent.RejectedExecutionException: Task org.apache.activemq.artemis.utils.actors.ProcessorBase$$Lambda$61/1394940518@15f4294c rejected from org.apache.activemq.artemis.utils.ActiveMQThreadPoolExecutor@69d38d0a[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 265]\r\n        at java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2063) [rt.jar:1.8.0_265]\r\n        at java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:830) [rt.jar:1.8.0_265]\r\n        at java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1379) [rt.jar:1.8.0_265]\r\n        at org.apache.activemq.artemis.utils.actors.ProcessorBase.onAddedTaskIfNotRunning(ProcessorBase.java:186) [artemis-commons-2.14.0-SNAPSHOT.jar:2.14.0-SNAPSHOT]\r\n        at org.apache.activemq.artemis.utils.actors.ProcessorBase.task(ProcessorBase.java:174) [artemis-commons-2.14.0-SNAPSHOT.jar:2.14.0-SNAPSHOT]\r\n        at org.apache.activemq.artemis.utils.actors.OrderedExecutor.execute(OrderedExecutor.java:54) [artemis-commons-2.14.0-SNAPSHOT.jar:2.14.0-SNAPSHOT]\r\n        at com.mysql.cj.jdbc.ConnectionImpl.setNetworkTimeout(ConnectionImpl.java:2488) [mysql-connector-java-8.0.21.jar:8.0.21]\r\n        at org.apache.commons.dbcp2.Jdbc41Bridge.setNetworkTimeout(Jdbc41Bridge.java:386) [commons-dbcp2-2.7.0.jar:2.7.0]\r\n        at org.apache.commons.dbcp2.DelegatingConnection.setNetworkTimeout(DelegatingConnection.java:985) [commons-dbcp2-2.7.0.jar:2.7.0]\r\n        at org.apache.commons.dbcp2.Jdbc41Bridge.setNetworkTimeout(Jdbc41Bridge.java:386) [commons-dbcp2-2.7.0.jar:2.7.0]\r\n        at org.apache.commons.dbcp2.DelegatingConnection.setNetworkTimeout(DelegatingConnection.java:985) [commons-dbcp2-2.7.0.jar:2.7.0]\r\n        at org.apache.activemq.artemis.jdbc.store.drivers.JDBCConnectionProvider.getConnection(JDBCConnectionProvider.java:62) [artemis-jdbc-store-2.14.0-SNAPSHOT.jar:2.14.0-SNAPSHOT]\r\n        at org.apache.activemq.artemis.jdbc.store.drivers.AbstractJDBCDriver.createTableIfNotExists(AbstractJDBCDriver.java:90) [artemis-jdbc-store-2.14.0-SNAPSHOT.jar:2.14.0-SNAPSHOT]\r\n        at org.apache.activemq.artemis.jdbc.store.drivers.AbstractJDBCDriver.createTable(AbstractJDBCDriver.java:63) [artemis-jdbc-store-2.14.0-SNAPSHOT.jar:2.14.0-SNAPSHOT]\r\n        at org.apache.activemq.artemis.core.server.impl.jdbc.JdbcSharedStateManager.createSchema(JdbcSharedStateManager.java:68) [artemis-server-2.14.0-SNAPSHOT.jar:2.14.0-SNAPSHOT]\r\n        at org.apache.activemq.artemis.jdbc.store.drivers.AbstractJDBCDriver.start(AbstractJDBCDriver.java:50) [artemis-jdbc-store-2.14.0-SNAPSHOT.jar:2.14.0-SNAPSHOT]\r\n        at org.apache.activemq.artemis.core.server.impl.jdbc.JdbcSharedStateManager.usingConnectionProvider(JdbcSharedStateManager.java:58) [artemis-server-2.14.0-SNAPSHOT.jar:2.14.0-SNAPSHOT]\r\n        at org.apache.activemq.artemis.core.server.impl.jdbc.JdbcNodeManager.lambda$usingConnectionProvider$0(JdbcNodeManager.java:91) [artemis-server-2.14.0-SNAPSHOT.jar:2.14.0-SNAPSHOT]\r\n        at org.apache.activemq.artemis.core.server.impl.jdbc.JdbcNodeManager.start(JdbcNodeManager.java:162) [artemis-server-2.14.0-SNAPSHOT.jar:2.14.0-SNAPSHOT]\r\n        at org.apache.activemq.artemis.core.server.impl.ActiveMQServerImpl.internalStart(ActiveMQServerImpl.java:580) [artemis-server-2.14.0-SNAPSHOT.jar:2.14.0-SNAPSHOT]\r\n        at org.apache.activemq.artemis.core.server.impl.ActiveMQServerImpl.start(ActiveMQServerImpl.java:526) [artemis-server-2.14.0-SNAPSHOT.jar:2.14.0-SNAPSHOT]\r\n        at org.apache.activemq.artemis.core.server.impl.SharedStoreBackupActivation$FailbackChecker$1.run(SharedStoreBackupActivation.java:230) [artemis-server-2.14.0-SNAPSHOT.jar:2.14.0-SNAPSHOT]\r\n        at java.lang.Thread.run(Thread.java:748) [rt.jar:1.8.0_265]\r\n\r\n2020-09-16 15:07:12,205 WARN  [org.apache.activemq.artemis.jdbc.store.drivers.JDBCConnectionProvider] Unable to set a network timeout on the JDBC connection: java.util.concurrent.RejectedExecutionException: Task org.apache.activemq.artemis.utils.actors.ProcessorBase$$Lambda$61/1394940518@15f4294c rejected from org.apache.activemq.artemis.utils.ActiveMQThreadPoolExecutor@69d38d0a[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 265]\r\n        at java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2063) [rt.jar:1.8.0_265]\r\n        at java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:830) [rt.jar:1.8.0_265]\r\n        at java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1379) [rt.jar:1.8.0_265]\r\n        at org.apache.activemq.artemis.utils.actors.ProcessorBase.onAddedTaskIfNotRunning(ProcessorBase.java:186) [artemis-commons-2.14.0-SNAPSHOT.jar:2.14.0-SNAPSHOT]\r\n        at org.apache.activemq.artemis.utils.actors.ProcessorBase.task(ProcessorBase.java:174) [artemis-commons-2.14.0-SNAPSHOT.jar:2.14.0-SNAPSHOT]\r\n        at org.apache.activemq.artemis.utils.actors.OrderedExecutor.execute(OrderedExecutor.java:54) [artemis-commons-2.14.0-SNAPSHOT.jar:2.14.0-SNAPSHOT]\r\n        at com.mysql.cj.jdbc.ConnectionImpl.setNetworkTimeout(ConnectionImpl.java:2488) [mysql-connector-java-8.0.21.jar:8.0.21]\r\n        at org.apache.commons.dbcp2.Jdbc41Bridge.setNetworkTimeout(Jdbc41Bridge.java:386) [commons-dbcp2-2.7.0.jar:2.7.0]\r\n        at org.apache.commons.dbcp2.DelegatingConnection.setNetworkTimeout(DelegatingConnection.java:985) [commons-dbcp2-2.7.0.jar:2.7.0]\r\n        at org.apache.commons.dbcp2.Jdbc41Bridge.setNetworkTimeout(Jdbc41Bridge.java:386) [commons-dbcp2-2.7.0.jar:2.7.0]\r\n        at org.apache.commons.dbcp2.DelegatingConnection.setNetworkTimeout(DelegatingConnection.java:985) [commons-dbcp2-2.7.0.jar:2.7.0]\r\n        at org.apache.activemq.artemis.jdbc.store.drivers.JDBCConnectionProvider.getConnection(JDBCConnectionProvider.java:62) [artemis-jdbc-store-2.14.0-SNAPSHOT.jar:2.14.0-SNAPSHOT]\r\n        at org.apache.activemq.artemis.core.server.impl.jdbc.JdbcSharedStateManager.setup(JdbcSharedStateManager.java:193) [artemis-server-2.14.0-SNAPSHOT.jar:2.14.0-SNAPSHOT]\r\n        at org.apache.activemq.artemis.core.server.impl.jdbc.JdbcNodeManager.start(JdbcNodeManager.java:164) [artemis-server-2.14.0-SNAPSHOT.jar:2.14.0-SNAPSHOT]\r\n        at org.apache.activemq.artemis.core.server.impl.ActiveMQServerImpl.internalStart(ActiveMQServerImpl.java:580) [artemis-server-2.14.0-SNAPSHOT.jar:2.14.0-SNAPSHOT]\r\n        at org.apache.activemq.artemis.core.server.impl.ActiveMQServerImpl.start(ActiveMQServerImpl.java:526) [artemis-server-2.14.0-SNAPSHOT.jar:2.14.0-SNAPSHOT]\r\n        at org.apache.activemq.artemis.core.server.impl.SharedStoreBackupActivation$FailbackChecker$1.run(SharedStoreBackupActivation.java:230) [artemis-server-2.14.0-SNAPSHOT.jar:2.14.0-SNAPSHOT]\r\n        at java.lang.Thread.run(Thread.java:748) [rt.jar:1.8.0_265]\r\n\r\n2020-09-16 15:07:12,207 INFO  [org.apache.activemq.artemis.core.server] AMQ221000: backup Message Broker is starting with configuration Broker Configuration (clustered=true,jdbcDriverClassName=com.mysql.cj.jdbc.Driver,jdbcConnectionUrl=jdbc:mysql://localhost:3306/mysql,messageTableName=MESSAGE_TABLE,bindingsTableName=BINDINGS_TABLE,largeMessageTableName=LARGE_MESSAGES_TABLE,pageStoreTableName=PAGE_TABLE,)\r\n2020-09-16 15:07:14,586 INFO  [org.apache.activemq.artemis.core.server] AMQ221032: Waiting to become backup node\r\n2020-09-16 15:07:14,587 WARN  [org.apache.activemq.artemis.jdbc.store.drivers.JDBCConnectionProvider] Unable to set a network timeout on the JDBC connection: java.util.concurrent.RejectedExecutionException: Task org.apache.activemq.artemis.utils.actors.ProcessorBase$$Lambda$61/1394940518@15f4294c rejected from org.apache.activemq.artemis.utils.ActiveMQThreadPoolExecutor@69d38d0a[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 265]\r\n        at java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2063) [rt.jar:1.8.0_265]\r\n        at java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:830) [rt.jar:1.8.0_265]\r\n        at java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1379) [rt.jar:1.8.0_265]\r\n        at org.apache.activemq.artemis.utils.actors.ProcessorBase.onAddedTaskIfNotRunning(ProcessorBase.java:186) [artemis-commons-2.14.0-SNAPSHOT.jar:2.14.0-SNAPSHOT]\r\n        at org.apache.activemq.artemis.utils.actors.ProcessorBase.task(ProcessorBase.java:174) [artemis-commons-2.14.0-SNAPSHOT.jar:2.14.0-SNAPSHOT]\r\n        at org.apache.activemq.artemis.utils.actors.OrderedExecutor.execute(OrderedExecutor.java:54) [artemis-commons-2.14.0-SNAPSHOT.jar:2.14.0-SNAPSHOT]\r\n        at com.mysql.cj.jdbc.ConnectionImpl.setNetworkTimeout(ConnectionImpl.java:2488) [mysql-connector-java-8.0.21.jar:8.0.21]\r\n        at org.apache.commons.dbcp2.Jdbc41Bridge.setNetworkTimeout(Jdbc41Bridge.java:386) [commons-dbcp2-2.7.0.jar:2.7.0]\r\n        at org.apache.commons.dbcp2.DelegatingConnection.setNetworkTimeout(DelegatingConnection.java:985) [commons-dbcp2-2.7.0.jar:2.7.0]\r\n        at org.apache.commons.dbcp2.Jdbc41Bridge.setNetworkTimeout(Jdbc41Bridge.java:386) [commons-dbcp2-2.7.0.jar:2.7.0]\r\n        at org.apache.commons.dbcp2.DelegatingConnection.setNetworkTimeout(DelegatingConnection.java:985) [commons-dbcp2-2.7.0.jar:2.7.0]\r\n        at org.apache.activemq.artemis.jdbc.store.drivers.JDBCConnectionProvider.getConnection(JDBCConnectionProvider.java:62) [artemis-jdbc-store-2.14.0-SNAPSHOT.jar:2.14.0-SNAPSHOT]\r\n        at org.apache.activemq.artemis.core.server.impl.jdbc.JdbcLeaseLock.tryAcquire(JdbcLeaseLock.java:175) [artemis-server-2.14.0-SNAPSHOT.jar:2.14.0-SNAPSHOT]\r\n        at org.apache.activemq.artemis.core.server.impl.jdbc.LeaseLock.tryAcquire(LeaseLock.java:91) [artemis-server-2.14.0-SNAPSHOT.jar:2.14.0-SNAPSHOT]\r\n        at org.apache.activemq.artemis.core.server.impl.jdbc.LeaseLock.tryAcquire(LeaseLock.java:109) [artemis-server-2.14.0-SNAPSHOT.jar:2.14.0-SNAPSHOT]\r\n        at org.apache.activemq.artemis.core.server.impl.jdbc.JdbcNodeManager.lock(JdbcNodeManager.java:267) [artemis-server-2.14.0-SNAPSHOT.jar:2.14.0-SNAPSHOT]\r\n        at org.apache.activemq.artemis.core.server.impl.jdbc.JdbcNodeManager.startBackup(JdbcNodeManager.java:360) [artemis-server-2.14.0-SNAPSHOT.jar:2.14.0-SNAPSHOT]\r\n        at org.apache.activemq.artemis.core.server.impl.SharedStoreBackupActivation.run(SharedStoreBackupActivation.java:62) [artemis-server-2.14.0-SNAPSHOT.jar:2.14.0-SNAPSHOT]\r\n        at org.apache.activemq.artemis.core.server.impl.ActiveMQServerImpl$ActivationThread.run(ActiveMQServerImpl.java:3981) [artemis-server-2.14.0-SNAPSHOT.jar:2.14.0-SNAPSHOT]\r\n```"
            },
            {
              "comment_username": "franz1981",
              "comment_create_time": "2020-09-16T13:28:13Z",
              "comment_edit_time": "2020-09-16T13:29:06Z",
              "comment_text": "The problem on https://github.com/apache/activemq-artemis/pull/3204#issuecomment-693393537 seems related to: https://github.com/apache/activemq-artemis/blob/d55ec37195dff04377a43957cc0afa9d0de14b89/artemis-server/src/main/java/org/apache/activemq/artemis/core/server/impl/SharedStoreBackupActivation.java#L213-L222\r\n\r\nwhere the `nodeManager` is being used while the server (and the thread pools created on it) are stopped."
            },
            {
              "comment_username": "franz1981",
              "comment_create_time": "2020-09-17T08:07:00Z",
              "comment_edit_time": "2020-09-17T08:07:15Z",
              "comment_text": "@uomik \r\n\r\nOne question: the comment here is valid?\r\n```java\r\n   /**\r\n    * The DataSource to use to store Artemis data in the data store (can be {@code null} if {@code jdbcConnectionUrl} and {@code jdbcDriverClassName} are used instead).\r\n    *\r\n    * @return the DataSource used to store Artemis data in the JDBC data store.\r\n    */\r\n   private DataSource getDataSource() {\r\n      if (dataSource == null) {\r\n         if (dataSourceProperties.isEmpty()) {\r\n            addDataSourceProperty(\"driverClassName\", jdbcDriverClassName);\r\n            addDataSourceProperty(\"url\", jdbcConnectionUrl);\r\n            if (jdbcUser != null) {\r\n               addDataSourceProperty(\"username\", jdbcUser);\r\n            }\r\n            if (jdbcPassword != null) {\r\n               addDataSourceProperty(\"password\", jdbcPassword);\r\n            }\r\n         }\r\n         dataSource = JDBCDataSourceUtils.getDataSource(dataSourceClassName, dataSourceProperties);\r\n      }\r\n      return dataSource;\r\n   }\r\n```"
            },
            {
              "comment_username": "uomik",
              "comment_create_time": "2020-09-17T14:08:54Z",
              "comment_edit_time": "2020-09-17T14:08:54Z",
              "comment_text": "> One question: the comment here is valid?\r\n\r\nNo it's not. That's something I've forgotten to change. The datasource can be injected by underlying application server, but otherwise the default one is used"
            },
            {
              "comment_username": "franz1981",
              "comment_create_time": "2020-09-21T15:05:08Z",
              "comment_edit_time": "2020-09-21T15:05:08Z",
              "comment_text": "Hi @uomik !\r\nJust sent another PR to address some failures + dependences + threading issues on https://github.com/uomik/activemq-artemis/pull/2"
            },
            {
              "comment_username": "franz1981",
              "comment_create_time": "2020-09-21T15:18:52Z",
              "comment_edit_time": "2020-09-21T15:59:20Z",
              "comment_text": "With these last changes I believe it's nearly ok, I'm now working to check how it behaves with different databases.\r\nI will further work to make this better for the shared store use case too, similarly to what we do for NFS with file shared store (maybe).\r\n\r\nI've just one concern looking at the default pool configuration: the number of available JDBC connections in the pool and the behaviour when there are none available.\r\n\r\nI see that paging + large messsages can use an unbounded number of connections (it really depends by the amount of concurrent threads that perform page/large msg streaming operations), the 2 journals requires 1 connections each (binding + message journal) and the node manager requires 2 (the lease lock + the node manager shared state) connections.\r\nThat means that without paging or large messages a minimum of 2 + 2 = 4 database connections.\r\nThe problem I see is paging and large messages: if the are too many some of the other operations will hang, awaiting an available connection, that means that probably we need to add the critical analyzer there or just let users knows they can configure pool appropriately.\r\n"
            },
            {
              "comment_username": "franz1981",
              "comment_create_time": "2020-09-21T15:58:54Z",
              "comment_edit_time": "2020-09-21T15:58:54Z",
              "comment_text": "Please use the rebase to remove the last merge commit and try rebasing against `master` manually instead so the merge commit won't appear when the PR will be merged"
            },
            {
              "comment_username": "uomik",
              "comment_create_time": "2020-09-21T16:49:03Z",
              "comment_edit_time": "2020-09-21T16:59:12Z",
              "comment_text": "> Please use the rebase to remove the last merge commit and try rebasing against `master` manually instead so the merge commit won't appear when the PR will be merged\r\n\r\nI'll try, but it will take me a while to understand how I do that. \r\nEdit: I managed to completely mess up my local copy, so it will definitely take a while."
            },
            {
              "comment_username": "franz1981",
              "comment_create_time": "2020-09-21T17:31:57Z",
              "comment_edit_time": "2020-09-21T17:32:19Z",
              "comment_text": "@uomik don't worry try with\r\n\r\n```git reset --hard 76caeaccbc61f154cb9e26ed74e172534a1f4da0```\r\n\r\nthis would reset that branch to the commit right before the merge with master ;)"
            },
            {
              "comment_username": "uomik",
              "comment_create_time": "2020-09-21T18:12:17Z",
              "comment_edit_time": "2020-09-21T18:12:17Z",
              "comment_text": "@franz1981 I may have succeeded :) Hopefully I didn't mess anything up"
            },
            {
              "comment_username": "franz1981",
              "comment_create_time": "2020-09-24T14:25:58Z",
              "comment_edit_time": "2020-09-24T14:25:58Z",
              "comment_text": "Sorry @uomik we've had several fixes around JDBC, please try to fix the conflicts if you can so I would merge the PR: I'm still testing this with different DBMS :)"
            }
          ]
        },
        {
          "issue_url": "https://github.com/apache/activemq-artemis/issues/3198",
          "issue_title": "Closing the directbytebuffer locally to prevent leaks",
          "issue_number": 3198,
          "issue_text": "Related to the jira issue: \r\nhttps://issues.apache.org/jira/browse/ARTEMIS-2696\r\nAssuming this can be done locally, this could be a fix for the above issue.\r\n\r\n",
          "issue_comments": []
        },
        {
          "issue_url": "https://github.com/apache/activemq-artemis/issues/3124",
          "issue_title": "ARTEMIS-2614 Create queues based on FQQN for AMQP protocol",
          "issue_number": 3124,
          "issue_text": "This is my attempt to address the problem of the auto-creation of queues based on FQQN. The simple scenario seems to work, but some decisions need to be made before I can move forward with this. \r\n\r\n1) There is a problem with the `RoutingType` mismatch. Previously it didn't matter what `RoutingType` receiver tries to attach to, as the implementation assumed pre-existence of the queue. Now the routing type matters, as we need to create a properly configured queue if it doesn't exist. That's why `testQueueConsumerReceiveTopicUsingFQQN\r\n` test fails currently. To make it pass I would need to subscribe to a topic instead of a queue. I'm not sure but is this a breaking change or not? \r\n2) I'm not sure what is the expected behavior when durability configuration mismatches. The problem is described in a test `testAttachToPreConfiguredNonDurableQueueUsingDurableFQQN\r\n`. Should we reject the attempt to attach when the pre-configured queue is non-durable and attach request has the source with `TerminusDurability\r\n` of `UNSETTLED_STATE` or `CONFIGURATION`? And what about the opposite, pre-configured queue is durable and we received source with `NONE` value for `TerminusDurability`?\r\n\r\nI would greatly appreciate your help. :)\r\n",
          "issue_comments": [
            {
              "comment_username": "jbertram",
              "comment_create_time": "2020-05-11T16:48:13Z",
              "comment_edit_time": "2020-05-11T16:48:13Z",
              "comment_text": "> Previously it didn't matter what RoutingType receiver tries to attach to, as the implementation assumed pre-existence of the queue.\r\n\r\nThat's not technically true. The broker supports auto-creation of addresses and queues. When auto-creating resources the routing type is based on the following (in descending order of precedence):\r\n\r\n1. The terminus capability set on the link as described in section 5.2 of the [AMQP-to-JMS mapping document](https://www.oasis-open.org/committees/download.php/60574/amqp-bindmap-jms-v1.0.pdf).\r\n2. The [prefix](http://activemq.apache.org/components/artemis/documentation/latest/address-model.html#using-prefixes-to-determine-routing-type) set on the acceptor.\r\n3. The dynamic default routing-type set in the `address-settings`.\r\n4. The [static default routing-type](https://github.com/apache/activemq-artemis/blob/master/artemis-core-client/src/main/java/org/apache/activemq/artemis/api/config/ActiveMQDefaultConfiguration.java#L537) set the the code-base."
            },
            {
              "comment_username": "Havret",
              "comment_create_time": "2020-05-11T17:02:49Z",
              "comment_edit_time": "2020-05-11T17:03:45Z",
              "comment_text": "> That's not technically true. The broker supports auto-creation of addresses and queues. When auto-creating resources the routing type is based on the following (in descending order of precedence):\r\n\r\nThat's true, but not when you specify address+queue using FQQN. If you do, it will attach to any address+queue combination that matches irrespectively of the defined routing type. If the queue doesn't exist, it won't be auto-created. "
            },
            {
              "comment_username": "gemmellr",
              "comment_create_time": "2020-05-11T17:04:07Z",
              "comment_edit_time": "2020-05-11T17:04:07Z",
              "comment_text": "TerminusDurability is not the same thing as queue durability and so shouldn't be used for indicating such."
            },
            {
              "comment_username": "Havret",
              "comment_create_time": "2020-05-11T17:05:24Z",
              "comment_edit_time": "2020-05-11T17:05:24Z",
              "comment_text": "> TerminusDurability is not the same thing as queue durability and so shouldn't be used for indicating such.\r\n\r\nBut currently seems to be used that way, isn't it? "
            },
            {
              "comment_username": "jbertram",
              "comment_create_time": "2020-05-11T17:06:13Z",
              "comment_edit_time": "2020-05-11T17:06:13Z",
              "comment_text": "> That's true, but not when you specify address+queue using FQQN.\r\n\r\nOf course, but that wasn't my point. My point was simply that the resources didn't have to pre-exist in the first place (i.e. without FQQN) as you seemed to assert."
            },
            {
              "comment_username": "Havret",
              "comment_create_time": "2020-05-11T17:07:30Z",
              "comment_edit_time": "2020-05-11T17:07:30Z",
              "comment_text": "@jbertram Yes, but why should we consider not FQQN based scenarios? "
            },
            {
              "comment_username": "gemmellr",
              "comment_create_time": "2020-05-11T17:07:40Z",
              "comment_edit_time": "2020-05-11T17:07:40Z",
              "comment_text": "> > TerminusDurability is not the same thing as queue durability and so shouldn't be used for indicating such.\r\n> \r\n> But currently seems to be used that way, isn't it?\r\n\r\n\r\nNo. It is only used in one place, appropriately, to indicate the subscription to a topic node is durable or not. That is, the link/terminus is durable or not. Not the queue the broker happens to create (which is an implementation detail, there need not be any queue)."
            },
            {
              "comment_username": "Havret",
              "comment_create_time": "2020-05-11T17:17:47Z",
              "comment_edit_time": "2020-05-11T17:17:47Z",
              "comment_text": "@gemmellr I am not sure I follow. Isn't this just JMS limitation? What I'm aiming for here is to support programmatically what is already supported via broker configuration and via GUI.\r\n"
            },
            {
              "comment_username": "jbertram",
              "comment_create_time": "2020-05-11T17:18:36Z",
              "comment_edit_time": "2020-05-11T17:18:36Z",
              "comment_text": "@Havret, I suppose I misunderstood what you were asserting in your original comment when you were talking about how \"the implementation assumed pre-existence of the queue.\""
            },
            {
              "comment_username": "jbertram",
              "comment_create_time": "2020-05-11T17:19:58Z",
              "comment_edit_time": "2020-05-11T17:19:58Z",
              "comment_text": "> What I'm aiming for here is to support programmatically what is already supported via broker configuration and via GUI.\\r\\n\\r\\nBy \"programmatically\" you mean specifically from an AMQP client, right? You can already create the resources any client would need programmatically through the management API."
            },
            {
              "comment_username": "Havret",
              "comment_create_time": "2020-05-11T17:20:04Z",
              "comment_edit_time": "2020-05-11T17:44:07Z",
              "comment_text": "@jbertram No worries. I suppose I could've been more specific. :) \\r\\n\\r\\n> By \"programmatically\" you mean specifically from an AMQP client, right? You can already create the resources any client would need programmatically through the management API.\\r\\n\\r\\nYep, exactly that's my goal."
            },
            {
              "comment_username": "gemmellr",
              "comment_create_time": "2020-05-11T17:26:42Z",
              "comment_edit_time": "2020-05-11T17:26:42Z",
              "comment_text": "@Havret No its not related to JMS. TerminusDurability is simply not equal to queue durability.\r\n\r\nFor example, were you to conflate terminus and queue config like that, I'd note that the terminus expiry policy would then dictate the broker should be deleting your queues when you go away in almost all situations. Which is typically not going to be preferred behaviour for most folks, and fortunately isnt the case since terminus != queue. The core AMQP spec doesn't deal with creation of nodes (queues/topics/others), which again reinforces terminus durability isnt queue durability. Its expected other mechanisms are used for fine grained creation of nodes (queues/topics/others) such as layered management like Artemis own existing management abilities Justin notes, which can be used using AMQP messages also, or alternatively the AMQP Management layered [draft] spec."
            },
            {
              "comment_username": "Havret",
              "comment_create_time": "2020-05-11T17:42:35Z",
              "comment_edit_time": "2020-05-11T18:03:24Z",
              "comment_text": "@gemmellr \r\nBut it seems it's already conflated, as it is being used to create shared durable queues like that? And they aren't deleted?\r\n\r\n>  TerminusDurability is simply not equal to queue durability.\r\n\r\nIn Artemis Docs it is stated. \r\n> If the Terminus Durability is either UNSETTLED_STATE or CONFIGURATION then the queue will be made **durable** (similar to a JMS durable subscription) (...).\r\n\r\nThe whole effort I am making here is just a result of a discussion on the mailing list where Martyn Taylor [said](http://activemq.2283324.n4.nabble.com/Artemis-AMQP-create-queue-based-on-FQQN-tp4754240p4754529.html):\r\n\r\n> My understanding was that auto-creation of FQQN queues was always an\r\n> intention but was never implemented.  Enabling this in the AMQP (and other\r\n> protocols) *should* allow any JMS over AMQP clients to utilize this\r\n> feature.  There may be some details here in terms of order of\r\n> precedence when using topic/queue capabilities (as defined in the JMS AMQP\r\n> mapping) and what is specified in the FQQN, but could be implemented\r\n> relatively easily.\r\n\r\nRegarding the part:\r\n> Its expected other mechanisms are used for fine grained creation of nodes (queues/topics/others) such as layered management like Artemis own existing management abilities Justin notes, which can be used using AMQP messages also\r\n\r\nThis sounds really promising! It is almost like it is done in RabbitMQ. Could you please point me where I can find more regarding this API? \r\n "
            },
            {
              "comment_username": "jbertram",
              "comment_create_time": "2020-05-11T19:25:31Z",
              "comment_edit_time": "2020-05-11T19:25:31Z",
              "comment_text": "> Could you please point me where I can find more regarding this API?\r\n\r\nThe [management API](http://activemq.apache.org/components/artemis/documentation/latest/management.html) generally and [management messages](http://activemq.apache.org/components/artemis/documentation/latest/management.html#using-management-message-api) in particular are both discussed in the documentation. The documentation discusses this mainly in terms of core and JMS. The broker ships the [management example](https://github.com/apache/activemq-artemis/tree/master/examples/features/standard/management) where you can see management messages in action. You can also see an example of an AMQP client using management messages in [the test-suite](https://github.com/apache/activemq-artemis/blob/master/tests/integration-tests/src/test/java/org/apache/activemq/artemis/tests/integration/amqp/AmqpManagementTest.java)."
            },
            {
              "comment_username": "gemmellr",
              "comment_create_time": "2020-05-12T08:57:53Z",
              "comment_edit_time": "2020-05-12T08:57:53Z",
              "comment_text": "> @gemmellr\r\n> But it seems it's already conflated, as it is being used to create shared durable queues like that? And they aren't deleted?\r\n> \r\n\r\nThis isnt the same. The client is creating a subscription to a 'topic' node's address, it says nothing about queues or their durability. It asks for the link terminus to be retained or not. For the durable-subscription case it specifically sets a terminus expiration of never, that is the terminus (/subscription) should be retained when it goes away. In the 'shared' cases there is a further layered mechanism (which has been negotiated as supported) further governing the behaviour of 'shared', 'global', link names etc. In no case does the client ask for a specific queue to be made, durable or otherwise, its simply a broker implementation detail that the broker chooses to model its 'topic' subscriptions as a queue encoding details such as a particular name.\r\n\r\nIf nothing else you will break things for most existing usage if you were to now conflate terminus and queue durability at this point when they never have been before. Similarly if you conflate Terminus config with queue config, youd really also need to conflate terminus expiry with the queue lifecycle. As earlier, in most cases (i.e mostly everything except 'never', as above) thats going to result in a queue being deleted when you dont want.\r\n\r\nThe core spec does not deal with regular queue creation, so it also doesnt cater to their durability, terminus durability isnt queue durability. \r\n\r\n> \r\n> Could you please point me where I can find more regarding this API?\r\n\r\nIts not something I have used, but Justin looks to have already linked to the details I was aware of in his last post above. \r\n\r\nOne place I know it is actually used as part of doing Artemis management using AMQP messages is:\r\nhttps://github.com/EnMasseProject/enmasse/blob/0.31.1/agent/lib/artemis.js#L58"
            },
            {
              "comment_username": "Havret",
              "comment_create_time": "2020-05-12T09:09:56Z",
              "comment_edit_time": "2020-05-12T11:34:05Z",
              "comment_text": "@gemmellr\r\nI'm not sure I'd break anything as most jms based clients out there don't use FQQNs. \r\nCould you please refer to the points from documentation and Martyn's words? I believe he is responsible for the original implementation. \r\n\r\nI believe that your opinion is a little bit biased, as you're looking at AMQP as the implementation detail for JMS. I don't believe this is the case. Artemis as it is doesn't care about `shared`, `global`, `topics`, and `subscriptions`. These are JMS bits. It is modeled on addresses and queues and their properties."
            },
            {
              "comment_username": "gemmellr",
              "comment_create_time": "2020-05-12T10:00:04Z",
              "comment_edit_time": "2020-05-12T10:00:04Z",
              "comment_text": "I'm really not looking at it that way at all. I'd say I more typically advocate for not catering to JMS specifically, since AMQP is entirely separate from JMS, regardless whether a certain JMS client I work on uses it.\r\n\r\nThat I can recall, JMS hasn't really even entered my replies here except largely dismissing it as a factor when you keep bringing it up (one last time though: those bits you refer to are not only for JMS clients, other AMQP clients can and do use them). This discussion has zero dependence on JMS in my mind.\r\n\r\nI'm looking at this in terms of the AMQP spec itself, and having discussed this area previously with folks who actually wrote it. Terminus durability isnt queue durability. Again, the core spec does not cover queues or creating them, it simply attaches to nodes [configured out of band] with a given address.\r\n\r\nYou would similarly break behaviour for any AMQP client out there using FQQNs already by all of a sudden considering terminus durability to be queue durability."
            },
            {
              "comment_username": "Havret",
              "comment_create_time": "2020-05-13T22:17:09Z",
              "comment_edit_time": "2020-05-13T22:18:25Z",
              "comment_text": "@gemmellr I'm bringing up JMS because JMS-AMQP mapping was the first thing you referred me to. Speaking in terms of AMQP spec, words like `shared`, `global`, `topic`, and `subscription` doesn't appear there even once, and you keep bringing them up. I'm not an expert in this area, I've never spoken with the authors of the spec. The only resources I have are spec itself, Artemis Source Code, and its documentation. Forgive me my ignorance but if I read in docs for amqp connector that\r\n\r\n> If the Terminus Durability is either UNSETTLED_STATE or CONFIGURATION then the queue will be made durable (similar to a JMS durable subscription) (...).\r\n\r\nAnd I see this being used to implement JMS features I interpret it this way.\r\n\r\nShared Durable Subscription that you're recalling as the only feasible case to use TerminusDurability is not an AMQP feature. It's a way that JMS based clients can attach to multicast queues where topic maps to address and subscription name maps to durable queue name. The fact that the durable queue is created there, is not an implementation detail from my perspective. When I configure broker I'm not configuring it in terms of topics and subscriptions. When I use FQQN I don't even need to know that such things exist. I have a broker with configured addresses and queues and I want to attach to them. In the ideal world, it would be great if it was possible to do it without the need to pre-configure (for instance to make development easier). "
            },
            {
              "comment_username": "gemmellr",
              "comment_create_time": "2020-05-14T00:09:34Z",
              "comment_edit_time": "2020-05-14T00:09:34Z",
              "comment_text": "> \\r\\n> \\r\\n> @gemmellr I'm bringing up JMS because JMS-AMQP mapping was the first thing you referred me to. \\r\\n\\r\\nI didn't refer you to it. Someone else may have briefly linked to it.\\r\\n\\r\\n> Speaking in terms of AMQP spec, words like `shared`, `global`, `topic`, and `subscription` doesn't appear there even once, and you keep bringing them up. \\r\\n\\r\\nTopic and subscriptions aren't AMQP specific terms (or JMS specific), though are generally understood messaging concepts, but since the spec doesn't deal specifically with queues or topics, merely nodes, plus links, terminus etc, their lack of use isnt surprising to me.\\r\\n\\r\\nYou are correct that the 'shared' and 'global' terms arent in the draft JMS mapping document either, because I have yet to publish the update with them in it. I did only mention those terms after you brought up shared durable subscriptions however, and again I didn't actually direct you to that document.\\r\\n\\r\\nJMS has no special bearing in this discussion from my view, its purely AMQP from my perspective. \\r\\n\\r\\n> I'm not an expert in this area, I've never spoken with the authors of the spec. The only resources I have are spec itself, Artemis Source Code, and its documentation. \\r\\n\\r\\nThats fair enough. You could possibly include in that resource list, discussion from those who people might say are quite familiar with the area, and are indicating having discussed this very specific item with authors on more than one occasion (including yesterday).\\r\\n\\r\\n> Forgive me my ignorance but if I read in docs for amqp connector that\\r\\n> \\r\\n> > If the Terminus Durability is either UNSETTLED_STATE or CONFIGURATION then the queue will be made durable (similar to a JMS durable subscription) (...).\\r\\n> \\r\\n> And I see this being used to implement JMS features I interpret it this way.\\r\\n> \\r\\n\\r\\nSpecifically the doc section it appears you may be quoting from is \"AMQP and Multicast Addresses (Topics)\", i.e its about topics rather than queues, aligning with what I have said.\\r\\n\\r\\nThe broker models its 'topic' subscriptions (i.e consumer link attaching using address of, err, an address ) using a queue that it happens to expose. There is no need for an actual exposed queue here at all, its a topic subscription, the queue is essentially a synthetic creation. It is the link terminus attaching to the topic-like node (so, address), i.e effectively the subscription representation, that is being indicated as durable (and separately marked as never expiring, to ensure its contuing existance) in this case to retain it. This is quite a different situation than if the link terminus were intended for attaching directly to an actual queue.\\r\\n\\r\\n> Shared Durable Subscription that you're recalling as the only feasible case to use TerminusDurability is not an AMQP feature.\\r\\n\\r\\nI am not saying (and dont recall ever having done so here) that Shared Durable Subscriptions are the only feasible case for TerminusDurability. I have said topic-like subscriptions of any kind are the main use case for it (along with expiry in some cases). I have said that TerminusDurability isnt the same direct thing as queue durability. The spec doesnt deal with creating them and so doesn't deal with their durability.\\r\\n\\r\\nSeparately though, they are however an AMQP feature - a layered extension feature yes, but a fully negotiated one that any AMQP client or server can use doing regular AMQP interactions, and again already do.\\r\\n\\r\\nNot really important, but for historic clarity, as it happens shared AMQP subscriptions existed in actual use in non-JMS client and servers long before any work was done in that area for a JMS 2 mapping for use in AMQP JMS clients. That existing behaviour was used as a starting point of reference (and one of many many approaches looked at) and expanded slightly to cover other requirements.\\r\\n\\r\\n> It's a way that JMS based clients can attach to multicast queues where topic maps to address and subscription name maps to durable queue name.\\r\\n\\r\\nAll AMQP clients or servers attach to 'topic-like' nodes (in this case, the brokers 'addresses') in the same way, not just JMS ones. Again, the queue existing in this scenario is completely a broker implementation detail (an approach that others also choose, admittedly), and it is entirely possible to model that topic subscription without any exposed queue.\\r\\n\\r\\n\\r\\n> The fact that the durable queue is created there, is not an implementation detail from my perspective. When I configure broker I'm not configuring it in terms of topics and subscriptions. When I use FQQN I don't even need to know that such things exist. I have a broker with configured addresses and queues and I want to attach to them. In the ideal world, it would be great if it was possible to do it without the need to pre-configure (for instance to make development easier).\\r\\n\\r\\nI understand that it might be a nice side effect that you desire in certain cases, but that doesnt mean it isnt an impl detail, or that terminus durability is queue durability."
            },
            {
              "comment_username": "gemmellr",
              "comment_create_time": "2020-05-14T00:10:26Z",
              "comment_edit_time": "2020-05-14T00:10:26Z",
              "comment_text": "The way I know some other servers have facilitated things in this space is to enable pre-defining configuration that will apply to a queue/topic/other when things are being auto created, e.g allowing you to pattern match what configuration that applies when a link attaches to a given address. Possibly much like the the existing address config stuff can pre-configure address behaviour without actual addresses."
            },
            {
              "comment_username": "Havret",
              "comment_create_time": "2020-05-24T13:48:24Z",
              "comment_edit_time": "2020-05-24T16:39:46Z",
              "comment_text": "@jbertram, @gemmellr \\r\\n\\r\\nSo after all it wasn't too productive discussion. I debugged the code, and apparently methods `createSharedDurableQueue` and `createUnsharedDurableQueue` don't explicitly create durable queues. The queues are durable by default if it's not specified otherwise. So if I drop \"setDurable\" bits, FQQN queues will be created as durable which makes sense, and doesn't break anything.\\r\\n\\r\\nHowever, there are still some things that need to be cleared up regarding my first question. \\r\\n\\r\\n> There is a problem with the RoutingType mismatch. Previously it didn't matter what RoutingType receiver tries to attach to, as the implementation assumed pre-existence of the queue. Now the routing type matters, as we need to create a properly configured queue if it doesn't exist. That's why testQueueConsumerReceiveTopicUsingFQQN test fails currently. To make it pass I would need to subscribe to a topic instead of a queue. I'm not sure but is this a breaking change or not?\\r\\n\\r\\nSTOMP implementation that @jbertram recently added https://github.com/apache/activemq-artemis/pull/3018 seems to silently ignore passed routing type when the mismatch occurs. Should I do the same here? It would be the safest way I guess, but what do you guys think? \\r\\n"
            },
            {
              "comment_username": "michaelandrepearce",
              "comment_create_time": "2020-05-24T17:42:16Z",
              "comment_edit_time": "2020-05-24T17:42:16Z",
              "comment_text": "First off you shouldn't affect the existing logic, e.g. the original use case which was to allow a fqqn to bind to a queue matching the name ignoring the routing type. E.g. if a fqqn matches an existing queue it should bind to it regardless routingtype.\\r\\n\\r\\nFor stomp how its done is\"\\r\\n\\r\\nA\\xa0RoutingType\\xa0is passed to the\\xa0autoCreateDestinationIfPossible\\xa0method which is based on the frame's header or destination prefix. If that is\\xa0null\\xa0then it uses the default value specified in the address-settings."
            },
            {
              "comment_username": "michaelandrepearce",
              "comment_create_time": "2020-05-24T17:56:49Z",
              "comment_edit_time": "2020-05-24T17:56:49Z",
              "comment_text": "If you look in  ProtonServerSenderContext, there already is a field derived and caclulated \"routingTypeToUse\"\r\n\r\nAlso i would use the same logic  already in there thats working out if its a durable or non-durable queue (aka if its volatile)"
            },
            {
              "comment_username": "Havret",
              "comment_create_time": "2020-05-24T18:00:48Z",
              "comment_edit_time": "2020-05-24T18:04:50Z",
              "comment_text": "So the routing type is calculated there multiple times, using slightly different variations of the same logic. It's pretty messy tbh, maybe it would be a good idea to unify this, and extract one method to determine routing type. \r\n\r\nThe same story with fields `routingTypeToUse` and `multicast`. It is basically the same thing, isn't it? Or at least represent the same information. "
            },
            {
              "comment_username": "michaelandrepearce",
              "comment_create_time": "2020-05-24T18:08:32Z",
              "comment_edit_time": "2020-05-24T18:09:42Z",
              "comment_text": "To be honest i would expect for FQQN it to apply all the exact logic thats in there, except the where we call createQueueName you use queueNameToUse if it present instead, that was possibly extracted earlier in the composite address extraction\r\n\r\ne.g. line 413\r\n\r\n               queue = createQueueName(connection.isUseCoreSubscriptionNaming(), clientId, pubId, shared, global, false);\r\n\r\nto \r\n\r\n               queue = queueNameToUse == null ? createQueueName(connection.isUseCoreSubscriptionNaming(), clientId, pubId, shared, global, false) : queueNameToUse;\r\n\r\n\r\nAnd like wise line 438 extactly the same\r\n\r\nI wouldnt expect changes anywhere else to achieve what you wanted, e.g. no change to any other class or area of the code base, a very small extra if then else  around the queue name used"
            },
            {
              "comment_username": "Havret",
              "comment_create_time": "2020-05-24T18:25:17Z",
              "comment_edit_time": "2020-05-24T18:37:21Z",
              "comment_text": "I wish it was that simple. Unfortunately for anycast routing type we don't even get to this line. For multicast on the other hand,\r\n\r\n```\r\nqueue = getMatchingQueue(queueNameToUse, addressToUse, RoutingType.MULTICAST);\r\n```\r\nin the original line 414 will throw an exception as the queue doesn't exist. \r\n\r\nThere is quite a lot of code here, and a lot of non-obvious stuff is going on, that's why I wanted to break the flow and handle FQQN in separation to the old bits. "
            },
            {
              "comment_username": "michaelandrepearce",
              "comment_create_time": "2020-05-24T18:39:48Z",
              "comment_edit_time": "2020-05-24T18:46:03Z",
              "comment_text": "For anycast i wouldnt even bother, the anycast queue is expected to have been created when the queue address is, for JMS semantics anyhow. (E.g. the original remit of FQQN was allowing a JMS queue consumer bind to a mutlicat queue, to get around issues where a JMS client was still at 1.1 spec without shared sub support )\\r\\n\\r\\nre Topic (aka multicast)\\r\\n\\r\\nThats just original logic added for FQQN, getMatchingQueue, so just change line 515 e.g. this\\r\\n\\r\\n         if (!result.isExists()) {\\r\\n            throw new ActiveMQAMQPNotFoundException(\"Queue: '\" + queueName + \"' does not exist\");\\r\\n\\r\\nto return null, instead of throw exception (as like for non fqqn)\\r\\n\\r\\ndo keep the rest of it.\\r\\n\\r\\n"
            },
            {
              "comment_username": "Havret",
              "comment_create_time": "2020-05-24T19:03:40Z",
              "comment_edit_time": "2020-05-24T19:09:01Z",
              "comment_text": "So if I make these changes, it will work partially for multicast routing type (but all queues will be created using `createUnsharedDurableQueue` method. Not sure if this is desired. And it doesn't work at all for anycast, because of this if:\r\n\r\n![image](https://user-images.githubusercontent.com/9103861/82762546-e6a5c180-9e01-11ea-9733-e56794a18a40.png)\r\n\r\nAny suggestions? \r\n\r\n> For anycast i wouldnt even bother, the anycast queue is expected to have been created when the queue address is, for JMS semantics anyhow. (E.g. the original remit of FQQN was allowing a JMS queue consumer bind to a mutlicat queue, to get around issues where a JMS client was still at 1.1 spec without shared sub support )\r\n\r\nBut we would end up with the implementation that's not in sync with what's available for instance for STOMP. The whole point of this PR was to make it right, and easier to utilize for non-JMS AMQP based clients. "
            },
            {
              "comment_username": "michaelandrepearce",
              "comment_create_time": "2020-05-24T19:16:21Z",
              "comment_edit_time": "2020-05-24T19:25:01Z",
              "comment_text": "For AMQP clients they should really be using correct structures of AMQP not coding to FQQN. Its intent was solely for JMS legacy workaround where client has not yet upgraded to JMS 2 which supports shared subs etc. And in JMS 2.0 Amqp client like qpid they correctly set the amqp structures. This was focussed on jms 1.1 legacy usecases as a bridging mechanism where client upgrade to 2.0 wasnt yet feasible. For non jms usecase in amqp, then should advocating using the correct amqp structures.\r\n\r\nE.g. in your case such where NMS doesnt yet support 2.0 (wip) then youre restricted to fqqn for shared subs. Obviously the whole issue is removed when we get NMS 2.0 upto 2.0 spec. \r\n\r\n\r\n\r\nAs such i would just assume in code that if fqqn and queue not found it is a multicast you want to create on\r\n"
            },
            {
              "comment_username": "michaelandrepearce",
              "comment_create_time": "2020-05-24T20:03:02Z",
              "comment_edit_time": "2020-05-24T20:27:00Z",
              "comment_text": "Just to note and for the record, i am cautious of this change, as the FQQN was added for legacy stuff, making this change could be impacting to those expected behaviours. And building on it further encourages its use to proliferate. \r\n\r\nUnlike stomp, which you bring up, AMQP has a very rich messaging structure that supports this, as such its not like comparing stomp or openwire which are limited, in this area.\r\n\r\nPersonally i think for any higher level AMQP clients (e.g JMS/NMS) they would be better to expend time into adding support for the equivalent of jms shared sub in their abstractions, , so in case of NMS that would be to get that updated to  similar spec level with JMS 2.0\r\n\r\nI guess mapping would be for JMS 1.1 for fqqn genreated depending on the consumer the client made in jms api.\r\n\r\nJmsQueueConsumer (createConsumer(Queue)-> shared durable\r\nJmsTopicConsumer (createConsumer(Topic)) -> non durable shared\r\nJmsTopicConsumer (createDurableconsumer(Topic)) -> non shared durable\r\n\r\nFor nonshared, non durable a user would just not use FQQN.\r\n\r\nDef need tests for the above., and all the combinations.\r\n\r\nYou could  look to make use of ParameterisedAddress (re use, like CompositeAddress is) like in Core to get extra queue config for a queue off the name, to allow for more configuration to be able to be present in the fqqn address.\r\n"
            }
          ]
        },
        {
          "issue_url": "https://github.com/apache/activemq-artemis/issues/3053",
          "issue_title": "ARTEMIS-2697 Avoid using raw types for Persister<T>",
          "issue_number": 3053,
          "issue_text": "This commit, despite the big number of changes, is just fixing raw type usages for `Persister<T>`.",
          "issue_comments": [
            {
              "comment_username": "franz1981",
              "comment_create_time": "2020-04-01T15:42:39Z",
              "comment_edit_time": "2020-04-01T15:42:39Z",
              "comment_text": "The failure doesn't seem related to this PR but I will rebase and push -f it again to be sure of it"
            },
            {
              "comment_username": "clebertsuconic",
              "comment_create_time": "2020-04-02T18:20:14Z",
              "comment_edit_time": "2020-04-02T18:20:14Z",
              "comment_text": "can you push -f to force a new build please?"
            },
            {
              "comment_username": "michaelandrepearce",
              "comment_create_time": "2020-04-04T22:09:58Z",
              "comment_edit_time": "2020-04-04T22:09:58Z",
              "comment_text": "please raise a jira. "
            },
            {
              "comment_username": "franz1981",
              "comment_create_time": "2020-04-06T07:18:22Z",
              "comment_edit_time": "2020-04-06T07:18:22Z",
              "comment_text": "I'm going to create a JIRA and trigger a build of this again :)"
            },
            {
              "comment_username": "clebertsuconic",
              "comment_create_time": "2020-04-08T19:43:37Z",
              "comment_edit_time": "2020-04-08T19:43:37Z",
              "comment_text": "@franz1981 can you rebase this please?"
            },
            {
              "comment_username": "clebertsuconic",
              "comment_create_time": "2020-04-25T15:56:55Z",
              "comment_edit_time": "2020-04-25T15:56:55Z",
              "comment_text": "@franz1981 can you either rebase this or close it?"
            }
          ]
        },
        {
          "issue_url": "https://github.com/apache/activemq-artemis/issues/2982",
          "issue_title": "ARTEMIS-2620 Avoid Exception on client when large message file corrupted",
          "issue_number": 2982,
          "issue_text": "The broker should be more resilient so that it can avoid throwing an\r\nexception on the client side when it encounters an invalid or empty\r\nlarge message file(corrupted for any reason). It would be better to\r\ndeal with it within the broker and not allow the issue to manifest\r\nitself on the consuming client.\r\n\r\nAlso fixed an ActiveMQServerMessagePlugin issue where beforeSend(),\r\nafterSend(), sendOnException() are not get called with large\r\nmessages.",
          "issue_comments": [
            {
              "comment_username": "clebertsuconic",
              "comment_create_time": "2020-04-17T02:32:16Z",
              "comment_edit_time": "2020-04-17T02:32:16Z",
              "comment_text": "rebase?"
            },
            {
              "comment_username": "clebertsuconic",
              "comment_create_time": "2020-05-19T22:43:40Z",
              "comment_edit_time": "2020-05-19T22:43:40Z",
              "comment_text": "You have changed the ServerSessionHandler... if the large message is damaged at that point, any exceptions should definitely be sent to the client.\r\n\r\nI don't think we should change this.\r\n\r\nperhaps we should look closely to what was the original issue before this was raised?"
            },
            {
              "comment_username": "clebertsuconic",
              "comment_create_time": "2020-05-19T22:45:04Z",
              "comment_edit_time": "2020-05-19T22:45:04Z",
              "comment_text": "I agree with adding a plugin event.. \r\n\r\nbut I don't think we should change semantics on the sending exceptions to the client."
            },
            {
              "comment_username": "gaohoward",
              "comment_create_time": "2020-05-22T14:18:54Z",
              "comment_edit_time": "2020-05-22T14:18:54Z",
              "comment_text": "That was user's idea that the error should be detect early and not to send to client.\r\nI'd agree that we fix the original issue (so far we lack information as to details how the large message was corrupted and lack of a reproducer)."
            }
          ]
        },
        {
          "issue_url": "https://github.com/apache/activemq-artemis/issues/2978",
          "issue_title": "ARTEMIS-2563 Added option to create queues on all clustered nodes",
          "issue_number": 2978,
          "issue_text": "This is to help with redistribution as mentioned in this Jira:\r\nhttps://issues.apache.org/jira/browse/ARTEMIS-2563\r\n\r\nAddress option added to enable queue creation on all active nodes in a cluster, which happens on BINDING_ADDED. Since this creates a local binding for remote queues, a redistributor will also get created once a consumer registers",
          "issue_comments": [
            {
              "comment_username": "jbertram",
              "comment_create_time": "2020-02-10T17:49:06Z",
              "comment_edit_time": "2020-02-10T20:59:03Z",
              "comment_text": "I think there may be a simpler solution to this that would avoid adding a new configuration option. Currently `org.apache.activemq.artemis.core.server.impl.QueueImpl#addRedistributor` is only invoked when receiving a notification about a consumer being created or closed on another cluster member. I think it should be possible to check if there are consumers on other nodes when creating a queue and if so add a redistributor."
            },
            {
              "comment_username": "AntonRoskvist",
              "comment_create_time": "2020-02-10T20:40:48Z",
              "comment_edit_time": "2020-02-10T20:40:48Z",
              "comment_text": "Yeah, that sounds good... but wouldn't it be preferable to have it configurable, to not change the way the broker works without other users being aware of it? Or do you think it would that be safe enough to implement it like that anyway? I'll certainly look in to it in that case, any idea where such an addition should be made?"
            },
            {
              "comment_username": "jbertram",
              "comment_create_time": "2020-02-10T21:03:43Z",
              "comment_edit_time": "2020-02-10T21:03:43Z",
              "comment_text": "In my opinion the current behavior is a bug since it's a gap between the way pre-configured and auto-created queues work. Therefore, I don't think it needs to be configurable.\r\n\r\nLook at the callers of `org.apache.activemq.artemis.core.server.impl.QueueImpl#addRedistributor`. Some meta-data about the remote queues are stored there that you should be able to access when a queue gets created on the broker."
            },
            {
              "comment_username": "AntonRoskvist",
              "comment_create_time": "2020-02-11T08:56:48Z",
              "comment_edit_time": "2020-02-11T08:56:48Z",
              "comment_text": "Okay, sounds good. I will have a look at it.\r\n\r\nOne other thing though, Tests are failing for my current pr, but they all pass when I run them locally, I even reran the specific tests that was failing here with no issues. Have I set something up incorrectly in my env? I get some log4j errors in the log."
            },
            {
              "comment_username": "jbertram",
              "comment_create_time": "2020-02-11T16:27:37Z",
              "comment_edit_time": "2020-02-11T16:27:37Z",
              "comment_text": "The test failures look legitimate to me since you've modified `org.apache.activemq.artemis.core.settings.impl.AddressSettings` and `org.apache.activemq.artemis.tests.compatibility.JournalCompatibilityTest` is failing at the point where it's trying to decode address settings. It looks to me like the problem is in `org.apache.activemq.artemis.core.settings.impl.AddressSettings#decode(org.apache.activemq.artemis.api.core.ActiveMQBuffer, boolean)` because you're not checking `buffer.readableBytes() > 0` when trying to decode your new parameter which fails on an old journal which doesn't have that set."
            },
            {
              "comment_username": "jbertram",
              "comment_create_time": "2020-02-25T18:37:56Z",
              "comment_edit_time": "2020-02-25T18:37:56Z",
              "comment_text": "Just to be clear...\r\n\r\nAre you working on the alternative solution that I mentioned previously? I wouldn't merge this PR until it was clear that other potential solution wasn't viable."
            },
            {
              "comment_username": "AntonRoskvist",
              "comment_create_time": "2020-02-26T07:47:47Z",
              "comment_edit_time": "2020-02-26T07:47:47Z",
              "comment_text": "Sorry, I am looking in to the alternative solution but have gotten nowhere. I am pretty new to coding and have gotten completely swamped with other stuff so I am not making the progress I was hoping for.\r\n\r\nI have yet to find a good way to poll the cluster members for consumer information on individual queues. Once I find that I'll be good to go, but it might take a while considering my current other work load."
            },
            {
              "comment_username": "AntonRoskvist",
              "comment_create_time": "2020-02-28T12:48:50Z",
              "comment_edit_time": "2020-02-28T12:48:50Z",
              "comment_text": "Would it be safe to just add a redistributor on receiving core notification ADD_BINDING if distance on that notification message is 0? That is, add a redistributor (if redistributiondelay is greater than -1) on creation of all local bindings? I tried it and all tests (as far as i can tell) are passing..."
            }
          ]
        },
        {
          "issue_url": "https://github.com/apache/activemq-artemis/issues/2972",
          "issue_title": "Restart Sequence",
          "issue_number": 2972,
          "issue_text": "This document contains the restart sequence for the brokers under various situations.\r\nThese sequences can help to restart the brokers in a proper sequence so as to avoid the client applications to be restarted.",
          "issue_comments": [
            {
              "comment_username": "franz1981",
              "comment_create_time": "2020-02-05T06:42:36Z",
              "comment_edit_time": "2020-02-05T06:42:36Z",
              "comment_text": "I suggest to sign the apache ICLA (see https://www.apache.org/dev/new-committers-guide.html#submitting-your-individual-contributor-license-agreement-icla) and create an appropriate JIRA for this document change eg https://issues.apache.org/jira/browse/ARTEMIS-1730\r\nThe name of PR and the commit message should reflect the JIRA name to help tracking the motivation of the change :+1:  "
            },
            {
              "comment_username": "clebertsuconic",
              "comment_create_time": "2020-02-05T17:16:46Z",
              "comment_edit_time": "2020-02-05T17:16:46Z",
              "comment_text": "nice doc... it needs a JIRA"
            },
            {
              "comment_username": "jbertram",
              "comment_create_time": "2020-02-05T18:56:19Z",
              "comment_edit_time": "2020-02-05T18:56:19Z",
              "comment_text": "IMO this belongs in the \"High Availability\" chapter rather than its own chapter."
            },
            {
              "comment_username": "michaelpearce-gain",
              "comment_create_time": "2020-06-24T21:06:31Z",
              "comment_edit_time": "2020-06-24T21:06:31Z",
              "comment_text": "@jbertram if no objections in next few hours, i will merge this, we can then move it later if wanted. I think the doc is useful"
            },
            {
              "comment_username": "michaelpearce-gain",
              "comment_create_time": "2020-06-24T21:09:33Z",
              "comment_edit_time": "2020-06-24T21:12:08Z",
              "comment_text": "@clebertsuconic it seems this went quiet, but i think its a good contribution, to avoid issues with no Jira i have created one for it: https://issues.apache.org/jira/browse/ARTEMIS-2822, before merge i will make a branch update their commit (keeping author) to add, and then send a new pr, and merge that."
            }
          ]
        },
        {
          "issue_url": "https://github.com/apache/activemq-artemis/issues/2893",
          "issue_title": "ARTEMIS-1925 combine STRICT and OFF with redistribution",
          "issue_number": 2893,
          "issue_text": "This commit adds two new message-load-balancing types:\r\nSTRICT_WITH_REDISTRIBUTION and REDISTRIBUTION_ONLY. These allow\r\nredistribution with the semantics of the existing STRICT and OFF\r\nmessage-load-balancing types respectively. Previously you could only get\r\nredistribution if you were using ON_DEMAND. The semantics for STRICT and\r\nOFF were not changed so as not to impact existing users.",
          "issue_comments": [
            {
              "comment_username": "jbertram",
              "comment_create_time": "2019-11-15T02:43:14Z",
              "comment_edit_time": "2019-11-15T02:43:14Z",
              "comment_text": "@AntonRoskvist, can you give this branch a test?"
            },
            {
              "comment_username": "AntonRoskvist",
              "comment_create_time": "2019-11-15T14:51:37Z",
              "comment_edit_time": "2019-11-15T14:51:37Z",
              "comment_text": "Sure can, @jbertram .\\r\\n\\r\\nInitial tests are looking great! I will keep this running in a \"busy\" test environment over the weekend to weed out any potential issues, but for the past few hours it's looking good. The system is _mostly_ behaving as expected, with the load balancing type \"REDISTRIBUTION_ONLY\" configured. Forwards are WAY down and the system is running much smoother.\\r\\n\\r\\nThe only odd behavior I noticed  was in regards to queues that require redistribution. As an example: If a consumer is connected to broker \"1\" on a new queue and then messages get produced to broker \"2\", the messages will not get forwarded but rather backs up on broker \"2\". If I disconnect and reconnect the consumer on broker \"1\", then redistribution starts and keeps on going as expected.\\r\\n\\r\\nIf messages are produced on broker \"2\" first and a consumer later connects to broker \"1\", then all works as expected. Otherwise it's looking fantastic!\\r\\n\\r\\nBr,\\r\\nAnton"
            },
            {
              "comment_username": "jbertram",
              "comment_create_time": "2019-11-18T21:23:13Z",
              "comment_edit_time": "2019-11-18T21:23:13Z",
              "comment_text": "@AntonRoskvist so the \"odd\" behavior your seeing isn't specifically related to this PR. It is simply the way the broker deals with auto-created queues in a cluster at the moment. I'll try to explain...\\r\\n\\r\\nEach broker has its own set of addresses and queues - even in a cluster. When a queue is created on one broker in a cluster the broker on which the queue is created creates what we call a \"local\" binding. It then informs all the other nodes in the cluster about this binding at which point the other nodes create a \"remote\" binding. Therefore, at this point all the nodes in the cluster know about the queue. Likewise, when a consumer is created on a queue on one broker all the other nodes are informed about it. *If* another broker in the queue has a matching _local_ binding with the same name (i.e. the queue has actually been created on that broker) then a \"redistributor\" will be created on that queue so that messages sent to it can be redistributed if necessary. In your use-case there is no local binding so the redistributor isn't created.\\r\\n\\r\\nAs I mentioned previously, this issue wasn't caused by this PR. IMO a new JIRA should be opened and a new PR should be sent to address it. In the mean-time you can work-around the issue by pre-creating all the queues for which you need redistribution."
            },
            {
              "comment_username": "AntonRoskvist",
              "comment_create_time": "2019-11-19T12:28:21Z",
              "comment_edit_time": "2019-11-19T12:28:21Z",
              "comment_text": "Oh, okay. In that case I think this PR is looking good. I have been running it through heavy load tests continuously since Friday and I have seen no issues except the one I mentioned above.\r\n\r\nI'd be happy to publish a new Jira for the other minor problem, but I am not quite sure how to formulate it... This is the expected behavior right now and we might want to alter how or when redistributors are formed, is that right?\r\n\r\nUnfortunately I can not configure static queues as I am working with several hundreds of them, in several sites that are created ad-hoc by external teams/components that I have little to no control over. But I'll work out some temporary solution for that, like forcing disconnects for consumers on the queues that are not currently getting redistributions.\r\n\r\nBr,\r\nAnton"
            },
            {
              "comment_username": "clebertsuconic",
              "comment_create_time": "2019-12-19T15:53:55Z",
              "comment_edit_time": "2019-12-19T15:58:57Z",
              "comment_text": "My vote is to just respect redistribution-delay with STRICT.\r\n\r\nWhen we wrote STRICT, we thought we didn't need redistribution, as with forward-when-no-consumers=off, it would mean the queue would only receive messages if there was a consumer.\r\n\r\n\r\nso, if in fact redistribution-delay and STRICT makes sense, just change STRICT. There's no sense on adding a third option.\r\n\r\nif you have a strong opinion about it.. call it something else.. I suggest... ROUND_ROBIN, make it the default, and deprecate STRICT. I see no value on STRICT after the change though.\r\n\r\n\r\nI was bitten by this once or two actually when I was using the broker as an user, trying to simulate a non related issue with redistribution, thinking it was supposed to redistribute and I had to read the documentation to figure it out. at one time I spent half day figuring out.. and I worked on the code myself before. definitely not intuitive."
            },
            {
              "comment_username": "clebertsuconic",
              "comment_create_time": "2019-12-19T16:00:36Z",
              "comment_edit_time": "2019-12-19T16:01:26Z",
              "comment_text": "ROUND_ROBIN is actually not a bad name if you want to keep a third option.\r\n\r\nIf you keep it, make a log.warn(\"STRICT at this point is considered a deprecated option... blablabla\")\r\n"
            },
            {
              "comment_username": "clebertsuconic",
              "comment_create_time": "2019-12-19T16:03:01Z",
              "comment_edit_time": "2019-12-19T16:03:01Z",
              "comment_text": "ha! I just googled.. and most other-brokers I just found actually call that kind of distribution... guess what? ... \"ROUND ROBIN\""
            },
            {
              "comment_username": "jbertram",
              "comment_create_time": "2019-12-19T21:01:05Z",
              "comment_edit_time": "2019-12-19T21:01:05Z",
              "comment_text": "All things being equal I would agree with you, @clebertsuconic. However, as noted in the commit message, I decided not to change the semantics of `STRICT` and `OFF` as I didn't want anyone using those settings along with a non-zero `redistribution-delay` to suddenly start having redistribution when up until this commit the `redistribution-delay` would just be ignored in those cases. Certainly an argument could be made that such a configuration is rare and might even be considered \"wrong.\" However, the fact remains that using `STRICT` or `OFF` with a non-zero `redistribution-delay` has always been a technically valid configuration and the documentation explicitly states that one must use `ON_DEMAND` to enable redistribution. I just think changing semantics in a minor version like you're suggesting is an objectively bad thing.\\r\\n\\r\\nPerhaps this should just wait until the next major release where we can properly break stuff. What do you think?"
            },
            {
              "comment_username": "michaelandrepearce",
              "comment_create_time": "2020-01-22T21:12:39Z",
              "comment_edit_time": "2020-01-22T21:12:39Z",
              "comment_text": "@jbertram just reading discussion on this, i agree with you until next major version, we should avoid breaking changes. If you're leaving this parked, any objections if we mark the PR with the \"Do not merge yet label\" just so its clear and gets skipped when reviewing active PR's?"
            },
            {
              "comment_username": "jbertram",
              "comment_create_time": "2020-01-22T21:35:31Z",
              "comment_edit_time": "2020-01-22T21:35:31Z",
              "comment_text": "@michaelandrepearce, I don't have any objections to that. I added the label."
            },
            {
              "comment_username": "AntonRoskvist",
              "comment_create_time": "2020-01-23T08:03:13Z",
              "comment_edit_time": "2020-01-23T08:03:13Z",
              "comment_text": "Just out of curiosity... when can I expect to see this implemented in a released version then? Is there a schedule for major releases?"
            },
            {
              "comment_username": "jbertram",
              "comment_create_time": "2020-01-23T18:17:12Z",
              "comment_edit_time": "2020-01-23T18:17:12Z",
              "comment_text": "@AntonRoskvist, it depends on the outcome of this discussion. I'm still waiting to hear back from @clebertsuconic. \r\n\r\nAs far as major releases go, there is no schedule."
            },
            {
              "comment_username": "AntonRoskvist",
              "comment_create_time": "2020-08-04T14:13:25Z",
              "comment_edit_time": "2020-08-04T14:13:25Z",
              "comment_text": "Hi,\r\n\r\nI don't really know if this is expected behavior or not, but I have run this PR in one environment since it was created and just noticed something. When running with REDISTRIBUTION_ONLY, topics (multicast) are not forwarded across the cluster, so given a cluster of broker A and B, messages produced to broker A only get delivered to subscribers directly connected to broker A. Subscribers connected to broker B gets none. If no consumers are connected to broker A then the message is lost (using non-durable topics). Using ON_DEMAND the redistribution seem to work as expected, but then all anycast queues get distributed as well, which is what I was hoping to avoid in the first place.\r\n\r\nI guess this might relate to the other issue I had where remote bindings don't get created unless there is also a local one present?\r\n\r\nBesides this I have had no issue running this PR, even ported into the latest version of the broker\r\n\r\nBr,\r\nAnton"
            }
          ]
        },
        {
          "issue_url": "https://github.com/apache/activemq-artemis/issues/2845",
          "issue_title": "ARTEMIS-2336 Use zero copy to replicate journal/page/large message file (AGAIN)",
          "issue_number": 2845,
          "issue_text": "I've opened this PR for discussion: I would like to re-introduce ARTEMIS-2336, but I've allowed wildfly or any user that doesn't want/can to use zero copy to be able to use the existing artemis code.\\r\\n\\r\\nI've opened https://github.com/netty/netty/pull/9592 too to \"enhance\" `ChunkedNioFile` in order to solve a bug on our implementation: in the meantime I've \"shadowed\" my solution on Netty directly into `AbsoluteChunkedNioFile` to not rely on any specific Netty version.\\r\\n\\r\\nThis PR could make use for InVM connection the same optimization sent on https://github.com/apache/activemq-artemis/pull/2844 while reading file (RandomAccessFile) on `ReplicationSyncFileMessage`.",
          "issue_comments": [
            {
              "comment_username": "franz1981",
              "comment_create_time": "2019-09-22T11:49:13Z",
              "comment_edit_time": "2019-09-22T13:40:22Z",
              "comment_text": "@ehsavoie @clebertsuconic FYI this version is not failing anymore the wildfly tests on https://issues.apache.org/jira/browse/ARTEMIS-2496"
            },
            {
              "comment_username": "franz1981",
              "comment_create_time": "2019-09-22T13:18:03Z",
              "comment_edit_time": "2019-09-22T13:18:19Z",
              "comment_text": "@wy96f I remember you've provided some numbers for ARTEMIS-2336 using one of your load generator: it would be nice to compare the results with this pr using `-Dio.netty.file.region=true|false` and vs master to verify that `-Dio.netty.file.region=false` is not worst then master."
            },
            {
              "comment_username": "clebertsuconic",
              "comment_create_time": "2019-09-22T14:04:46Z",
              "comment_edit_time": "2019-09-22T14:04:46Z",
              "comment_text": "lets wait the release I'm doing on monday before we can start considering this?\r\nWe will need to test it within Wildfly to make sure it won't cause an issue in there."
            },
            {
              "comment_username": "franz1981",
              "comment_create_time": "2019-09-22T14:14:53Z",
              "comment_edit_time": "2019-09-22T14:15:08Z",
              "comment_text": "@clebertsuconic yes, agree and I would like to run a soak test + wait @wy96f results as well +1\r\nThe wildfly tests seems pretty stable with this, but we have many of them to be tried first "
            },
            {
              "comment_username": "franz1981",
              "comment_create_time": "2019-09-25T12:32:44Z",
              "comment_edit_time": "2019-09-25T12:33:02Z",
              "comment_text": "@wy96f \\r\\n\\r\\n> . So only non file packet accounts for the pending write bytes in channel, and flowControl is not working very well in this case, will this have a negative impact?\\r\\n\\r\\nI see that `DefaultMessageSizeEstimator` for `FileRegion` isn't handled nor `ChannelOutboundBuffer::decrementPendingOutboundBytes` is called afted `sendFile` succeed: it means that our \"lazy\" backpressure is quite limiting while using `FileRegion`s.\\r\\nFor `ChunkedNioFile` is a different story, but equally interesting: `DefaultMessageSizeEstimator` is not able to recognize `ChunkedNioFile` , but `ChannelOutboundBuffer::decrementPendingOutboundBytes` is correctly handling it because `ChunkedWriteHandler` is transparently handling the files as `ByteBuf`s.\\r\\nI think that is time to simplify `blockUntilWritable` to make it more lazy ie to just leverage on Netty `isWritable` instead of trying to calculate exactly the pending bytes :) \\r\\nI'm adding a commit to address that :)"
            },
            {
              "comment_username": "wy96f",
              "comment_create_time": "2019-09-26T04:02:28Z",
              "comment_edit_time": "2019-09-26T04:02:28Z",
              "comment_text": "@franz1981 I find new problem with -Dio.netty.file.region=false. I generated 48GB files with load generator. In the case of -Dio.netty.file.region=true and master, log(something like this `2019-09-26 11:02:49,348 DEBUG [org.apache.activemq.artemis.core.replication.ReplicationManager] sending 1048576 bytes on file xxxx`) showed it took about 7 minutes to transfer files, then synchronization done message sent. However in the case of -Dio.netty.file.region=false, log showed it took about about 40 seconds to transfer files, then sync done message sent. The fact is flow control didn't work and it took 40 seconds to build up PendingWrite in the queue rather than to transfer files. This leads to sync done message timeouts."
            },
            {
              "comment_username": "franz1981",
              "comment_create_time": "2019-09-26T08:26:28Z",
              "comment_edit_time": "2019-09-26T08:28:33Z",
              "comment_text": "@wy96f\\r\\nIt's strange that the back-pressure (writability) propagation fix isn't working: have you used the last version that include that fix? \\r\\nFlow control on Netty for chunked nio files should increase/decrease outbound pending bytes as with \"normal\" ByteBuf writes...can you check if the writeability changes are correctly propagated to ChunkedWriteHandler?\\r\\nI will be in vacation from today so don't have access for about 1 month to my computer: I will do my best to help as I can on my return, but we are near to fix this :)"
            },
            {
              "comment_username": "franz1981",
              "comment_create_time": "2019-09-26T08:31:23Z",
              "comment_edit_time": "2019-09-26T08:31:23Z",
              "comment_text": "Maybe on ActiveMQChannelHandler there are others events that are not correctly propagated through the pipeline and that would wake up the chunk writer?"
            },
            {
              "comment_username": "wy96f",
              "comment_create_time": "2019-09-26T10:21:40Z",
              "comment_edit_time": "2019-09-26T10:21:40Z",
              "comment_text": "@franz1981 Have fun in vacation :)\r\n\r\nThere is no problem with writability propagation, it works very well. when I set initial-replication-sync-timeout to a big value(E.g. 7 minutes), all of the queued up messages were sent and replication succeeded.\r\n\r\nThe packet sending process with -Dio.netty.file.region=true or master is:\r\n1. channel.writeAndFlush(artemis thread)\r\n2. add bytebuf in outboundbuffer -- increase size, flush it -- decrease size(netty thread)\r\n\r\nThe message sending process with -Dio.netty.file.region=false is:\r\n1. channel.writeAndFlush(artemis thread)\r\n2. add message in queue in ChunkedWriteHandler(netty thread)\r\n3. if channel writable, add bytebuf in outboundbuffer -- increase size and flush it -- decrease size(netty thread)\r\n4. If channel state transfers from unwritable to writable, call step3(netty thread)\r\n\r\nFor -Dio.netty.file.region=false, given message will be first put into queue then put in `outboundbuffer` only when channel writable, size in `outboundbuffer` will be limited to highWaterMark(default 128k). When flush proceeds and size drops to lowWaterMark(default 32k), channel is writable again, over and over again. I guess `flowControl` often sees channel writable(actually lots of queued up messages in ChunkedWriteHandler's queue)  so it's not limiting well. In the end, sync done message would not be delivered in time due to too many messages queued up.\r\n\r\nFor -Dio.netty.file.region=true or master, size in `outboundbuffer` will continue to grow(netty thread is running all the time). When it exceeds 128k meaning channel not writable and `flowControl` triggers, broker(artemis thread) will not send packet until data in `outboundbuffer` is flushed. So there will be not much data queued up. Make sense?"
            },
            {
              "comment_username": "franz1981",
              "comment_create_time": "2019-09-26T10:32:51Z",
              "comment_edit_time": "2019-09-26T15:42:15Z",
              "comment_text": "@wy96f \r\nGood analysis :)\r\nYep, so the simplest possible solution I see is to tune differently `highWaterMark`  > 1 MB to allow the chunk writer queue to continue to be drained. \r\n\r\nBoth ChunkedInput and FileRegions are missing (on Netty) a correct size estimation on ChannelOutboundBuffer and this would imply senders to push many of them in burst: what makes them behave differently is that FileRegion is getting backpressured only by TCP while ChunkedInputs start to getting backpressured by Netty itself *into* ChunkedWriterHandler, given that any read ByteBuf is being accounted into ChannelOutboundBuffer thus preventing subsequent pending writes to proceed due to the small high watermark limit (if compared to the chunk size)."
            },
            {
              "comment_username": "wy96f",
              "comment_create_time": "2019-09-27T04:10:54Z",
              "comment_edit_time": "2019-09-27T04:11:54Z",
              "comment_text": "@franz1981 Hi, I made tests with writeBufferHighWaterMark=2MB, 10MB, 100MB, 200MB,  the replication still failed(shocked).\r\nAfter some analysis, i think results might be reasonable. Whatever writeBufferHighWaterMark value we tune to, the total time with channel writable is similar(If using big value, it would take long to saturate channel; If using small value, it would take more times to saturate channel although shorter time). Considering netty thread would read chunk file to add ByteBuf in outboundbuffer(size will be added) and artemis thread just put the packets into netty executor(that's more fast), packets will definitely build up in the chunk writer queue."
            },
            {
              "comment_username": "franz1981",
              "comment_create_time": "2019-09-27T05:00:38Z",
              "comment_edit_time": "2019-09-27T05:03:06Z",
              "comment_text": "@wy96f thanks to have tried! It sounds strange to me: I was thinking the reason why was taking more was due to being continuosly stopped/being awaken and sending short chunks to the network (ie more syscalls with less data). I have a strong feeling that sendFile send a 1 MB chunk *directly* without using the TCP buffer at all....if is the case, it means we should increase the chunkSize (1 MB or at least 100K) and the TCP buffer accordingly (that's very small, by default afaik).\r\nDid you observe that the network was saturated in both cases?\r\nIf you use async-profiler you can check what the kernel does and where most the cost is for both cases...."
            },
            {
              "comment_username": "wy96f",
              "comment_create_time": "2019-09-27T05:50:37Z",
              "comment_edit_time": "2019-09-27T05:54:44Z",
              "comment_text": "@franz1981 \r\n```\r\n2019-09-27 13:47:01,943 DEBUG [org.apache.activemq.artemis.core.replication.ReplicationManager] sending 1048576 bytes on file 000002541.page\r\n2019-09-27 13:47:01,943 DEBUG [org.apache.activemq.artemis.core.replication.ReplicationManager] sending 1048576 bytes on file 000002541.page\r\n2019-09-27 13:47:01,943 DEBUG [org.apache.activemq.artemis.core.replication.ReplicationManager] sending 1048496 bytes on file 000002541.page\r\n2019-09-27 13:47:01,945 DEBUG [org.apache.activemq.artemis.core.replication.ReplicationManager] sending 0 bytes on file 000002541.page\r\n\r\n\r\n\r\n^C\r\n[artemis@windqpstdb05 bin]$ sar -n DEV 1\r\nLinux 2.6.32-279.19.1.el6_sn.12.x86_64 (windqpstdb05) \t09/27/2019 \t_x86_64_\t(8 CPU)\r\n\r\n01:47:37 PM     IFACE   rxpck/s   txpck/s    rxkB/s    txkB/s   rxcmp/s   txcmp/s  rxmcst/s\r\n01:47:38 PM        lo      0.00      0.00      0.00      0.00      0.00      0.00      0.00\r\n01:47:38 PM      eth0   4548.00   2576.00    294.39 108552.23      0.00      0.00      0.00\r\n\r\n01:47:38 PM     IFACE   rxpck/s   txpck/s    rxkB/s    txkB/s   rxcmp/s   txcmp/s  rxmcst/s\r\n01:47:39 PM        lo      0.00      0.00      0.00      0.00      0.00      0.00      0.00\r\n01:47:39 PM      eth0   4520.00   2528.00    292.59 108009.43      0.00      0.00      0.00\r\n\r\n01:47:39 PM     IFACE   rxpck/s   txpck/s    rxkB/s    txkB/s   rxcmp/s   txcmp/s  rxmcst/s\r\n01:47:40 PM        lo      0.00      0.00      0.00      0.00      0.00      0.00      0.00\r\n01:47:40 PM      eth0   4497.00   2588.00    291.05 107394.71      0.00      0.00      0.00\r\n\r\n01:47:40 PM     IFACE   rxpck/s   txpck/s    rxkB/s    txkB/s   rxcmp/s   txcmp/s  rxmcst/s\r\n01:47:41 PM        lo      0.00      0.00      0.00      0.00      0.00      0.00      0.00\r\n01:47:41 PM      eth0   4483.00   2561.00    290.18 106670.38      0.00      0.00      0.00\r\n\r\n01:47:41 PM     IFACE   rxpck/s   txpck/s    rxkB/s    txkB/s   rxcmp/s   txcmp/s  rxmcst/s\r\n01:47:42 PM        lo      0.00      0.00      0.00      0.00      0.00      0.00      0.00\r\n01:47:42 PM      eth0   4494.00   2584.00    290.85 107486.57      0.00      0.00      0.00\r\n\r\n```\r\nYes, i saw the network was saturated(initial-replication-sync-timeout was set to 300000, otherwise replication failed due to timeout). \r\nNote that log showed `13:47:01,945` last page sent, and sar showed `01:47:37 PM` queued up data still transferring and saturating network.\r\n\r\nBTW,`<acceptor name=\"artemis\">tcp://10.244.201.200:61616?;tcpSendBufferSize=1048576;tcpReceiveBufferSize=1048576;protocols=CORE,AMQP,STOMP,HORNETQ,MQTT,OPENWIRE;useEpoll=true;amqpCredits=1000;amqpLowCredits=300</acceptor>\r\n`\r\n I used this in broker.xml."
            },
            {
              "comment_username": "franz1981",
              "comment_create_time": "2019-09-27T06:13:19Z",
              "comment_edit_time": "2019-09-27T06:25:41Z",
              "comment_text": "If both cases (with/without file region) are saturating the network, why the latter will take more time? The total amount of data sent should be the same...\r\nDo you have tried master as well? \r\nI'm start to think that the pipelining happening while copying data in a non-netty thread and a separate Netty thread sending them across network is beneficial to improve the overall throughput, because we can do something (ie reading the file) while Netty is taking care to send data across network. But that means that if we don't use file regions I expect that network is not saturated 100% of the time and sometime wait ChunkInput to finish reading data and saturate it again: sar shows averages over sampling intervals so I believe that we can't spot those spikes...\r\nI don't know if your acceptor configuration of TCP buffer is working,need to inspect the logs...\r\nAnd profiling could be helpful as well..."
            },
            {
              "comment_username": "wy96f",
              "comment_create_time": "2019-09-27T06:30:15Z",
              "comment_edit_time": "2019-09-27T06:30:15Z",
              "comment_text": "@franz1981 \r\n> If both cases (with/without file region) are saturating the network, why the latter will take more time? The total amount of data sent should be the same...\r\n\r\nThey're taking same time(~7 mins). As i said,\r\n``\r\nIn the case of -Dio.netty.file.region=true and master, log(something like this 2019-09-26 11:02:49,348 DEBUG [org.apache.activemq.artemis.core.replication.ReplicationManager] sending 1048576 bytes on file xxxx) showed it took about 7 minutes to transfer files, then synchronization done message sent. However in the case of -Dio.netty.file.region=false, log showed it took about about 40 seconds to transfer files, then sync done message sent. \r\n``\r\n``\r\nYes, i saw the network was saturated(initial-replication-sync-timeout was set to 300000, otherwise replication failed due to timeout). Note that log showed 13:47:01,945 last page sent, and sar showed 01:47:37 PM queued up data still transferring and saturating network.\r\n``\r\nSaturation lasted for ~7 mins.\r\n\r\n> Do you have tried master as well?\r\n\r\nThe same. \r\n\r\n"
            },
            {
              "comment_username": "wy96f",
              "comment_create_time": "2019-09-27T06:50:01Z",
              "comment_edit_time": "2019-09-27T06:53:21Z",
              "comment_text": "@franz1981 \r\nprofiling without file region:\r\nhttps://filebin.net/r9o4bupoym9zxwk9/netty_false.svg?t=ld5f197t\r\nNote I profiled after 40s(after log showed last page sent) so most of samples were about netty.\r\n\r\nprofiling with file region:\r\nhttps://filebin.net/r9o4bupoym9zxwk9/netty_true.svg?t=mo3tho33\r\n\r\nprofiling master:\r\nhttps://filebin.net/r9o4bupoym9zxwk9/profiler_master.svg?t=mo3tho33"
            },
            {
              "comment_username": "franz1981",
              "comment_create_time": "2019-09-27T07:26:36Z",
              "comment_edit_time": "2019-09-27T07:26:36Z",
              "comment_text": "Sorry I have missed that the file region=true version was failing on timeout as well: tbh I'm not sure it makes sense to have to have such small timeout in that case, given that depends by disk speed/network bandwidth and total transferred size....\r\nJust one advice: try collecting samples with -t..I wasn't expecting blockUntiWritable to be that costy...."
            },
            {
              "comment_username": "wy96f",
              "comment_create_time": "2019-09-27T07:46:39Z",
              "comment_edit_time": "2019-09-27T07:46:39Z",
              "comment_text": "\r\n> tbh I'm not sure it makes sense to have to have such small timeout in that case, given that depends by disk speed/network bandwidth and total transferred size....\r\n\r\nDo you mean initial-replication-sync-timeout? If so, it's ok with default(30000) in master and file region=true, but not in file region=false, so i set it to 3000000 for profiling.\r\n\r\n> Just one advice: try collecting samples with -t..I wasn't expecting blockUntiWritable to be that costy....\r\n\r\n`./profiler.sh -d 60 -t -f xx.svg pid` like this, profiling for master,file region=false/true? For file region=false, should i collect samples from beginning or after 40s(after log showed last page sent)?"
            },
            {
              "comment_username": "franz1981",
              "comment_create_time": "2019-09-27T08:02:29Z",
              "comment_edit_time": "2019-09-27T08:02:29Z",
              "comment_text": "To make profiling traces comparable in number of samples you should need to limit the duration and make them equals eg -d 10 (seconds) \r\nOk to do it after 40 seconds depending which part of the catching up we are interested in...\r\nRe the timeout I need to re-read your previous answers: you said that both the approaches (file region/chunked files) max out network and take the same time, but only the latter will fail on replication timeout?"
            },
            {
              "comment_username": "wy96f",
              "comment_create_time": "2019-09-27T08:52:27Z",
              "comment_edit_time": "2019-09-27T08:52:27Z",
              "comment_text": "> Re the timeout I need to re-read your previous answers: you said that both the approaches (file region/chunked files) max out network and take the same time, but only the latter will fail on replication timeout?\r\n\r\nYes. In file region, sync done msg was sent and received by backup 7 mins later. In chunked file, sync done msg was sent 40 seconds later and received by backup 7 min later because of packets queued up on live server side.\r\n\r\ncollecting samples with -t. see https://filebin.net/1lyef9lv6rkew9ks\r\n\r\n"
            },
            {
              "comment_username": "franz1981",
              "comment_create_time": "2019-09-27T09:16:41Z",
              "comment_edit_time": "2019-09-27T09:16:41Z",
              "comment_text": "@wy96f probably the size estimator of Netty or/and the pending writes count is not working as I was expecting with file regions..."
            },
            {
              "comment_username": "franz1981",
              "comment_create_time": "2019-10-13T05:05:31Z",
              "comment_edit_time": "2019-10-13T05:05:31Z",
              "comment_text": "@wy96f I'll look better into this on my back, but please take a look yourself if you have any idea.\r\nFrom what I've understood we can:\r\n- fix https://github.com/netty/netty/blob/ff7a9fa091a8bf2e10020f83fc4df1c44098bbbb/transport/src/main/java/io/netty/channel/DefaultMessageSizeEstimator.java#L45 in order to account for chunked files correctly and flow control them while being enqueued into chunk writer \r\n- change how sync done timeout is being calculated and consider the total time to replicate (from the first file region/chunk sent) instead of the time from when the sync done is being enqueued in Netty \r\n\r\nwdyt?"
            },
            {
              "comment_username": "wy96f",
              "comment_create_time": "2019-10-14T03:44:34Z",
              "comment_edit_time": "2019-10-14T03:44:34Z",
              "comment_text": "@franz1981\r\n\r\n> fix https://github.com/netty/netty/blob/ff7a9fa091a8bf2e10020f83fc4df1c44098bbbb/transport/src/main/java/io/netty/channel/DefaultMessageSizeEstimator.java#L45 in order to account for chunked files correctly and flow control them while being enqueued into chunk writer\r\n\r\nNot sure. I think netty is just limiting data in memory so not accounting for chunked file and file region?\r\n\r\n> change how sync done timeout is being calculated and consider the total time to replicate (from the first file region/chunk sent) instead of the time from when the sync done is being enqueued in Netty\r\n\r\nWe block all journal related operations during sync done interval to make sure replica has received all data, so we‘d better not change it.\r\n\r\nMaybe we can:\r\n\r\n* We use original implementation when broker doesn't support zero region. IMO the chunked file way is more or less same with original one. At some point original version might be better bcs it reads file data in non-netty thread that can be beneficial where netty threads is busy taking care of new sent packets.\r\n\r\n* Another possible alternative is to use rate limiter like guava instead of netty flow control to work around the problem?\r\n\r\n"
            },
            {
              "comment_username": "franz1981",
              "comment_create_time": "2019-10-14T03:58:08Z",
              "comment_edit_time": "2019-10-14T03:58:08Z",
              "comment_text": "@wy96f sadly the rate limiter algorithm in guava is not very good fore fast paced networks and if with small token size, despite what the doc says about it :)\\r\\n\\r\\nNetty is considering 0 sized the FileRegion sent on the outbound buffer afaik and just \"unknown\" the one based on chunked files, but the point is that the latter will be accumulated and won't back-pressure the channel.becauee are sent through a background accumulator (the additional outbound channel in the pipeline) ie the solution to provide a custom estimator makes sense imho, but I need my computer at hand to try it.\\r\\n\\r\\nMy point on the existing sync done is that is not actually correct because it assume that the previous files are being sent before sending that sync done while is not in any case, because Netty is asynchronous...the chunked file case just make it more evident..\\r\\nMy 2 cents :)\\r\\n"
            },
            {
              "comment_username": "wy96f",
              "comment_create_time": "2019-10-14T10:34:45Z",
              "comment_edit_time": "2019-10-14T10:34:45Z",
              "comment_text": "@franz1981 Ok, I see your points of views. Maybe we can try with a custom estimator :)"
            },
            {
              "comment_username": "franz1981",
              "comment_create_time": "2019-10-28T17:16:29Z",
              "comment_edit_time": "2019-10-28T17:17:24Z",
              "comment_text": "@wy96f \r\nOne quick question: on https://github.com/apache/activemq-artemis/pull/2845#issuecomment-535440070 I see that you've said:\r\n> with -Dio.netty.file.region=true or master\r\n\r\nBut actually master and  `-Dio.netty.file.region=true` are very different!\r\nmaster has the feature with zero copy not enabled and `ByteBuf` are correctly estimated, while  `-Dio.netty.file.region=true` on this PR is using custom `FileRegion`s that doesn't seems correctly estimated according to https://github.com/netty/netty/blob/ff7a9fa091a8bf2e10020f83fc4df1c44098bbbb/transport/src/main/java/io/netty/channel/DefaultMessageSizeEstimator.java#L45.\r\nIn theory `-Dio.netty.file.region=true` should time out due to the wrong estimation (like `-Dio.netty.file.region=false`): do you have tested `-Dio.netty.file.region=true` on this PR with your long replication backlog test?"
            },
            {
              "comment_username": "wy96f",
              "comment_create_time": "2019-10-29T02:45:17Z",
              "comment_edit_time": "2019-10-29T02:45:42Z",
              "comment_text": "I was dubious about it too, but after some analysis, I somewhat understood it.\r\nIn fact size estimator is limiting non file packets in file region=true.\r\nThe log showed flow control triggered every ~450 times send. This make senses as each non file packet occupies ~300 bytes, and total size of them just over highWaterMark(default 128k). And flow control waited for ~4 seconds to allow 450 non file and 450 file packet to be drained(~450*1MB chunk size=~450MB data transferred in ~4 seconds, i.e. ~100MB/s).\r\nI uploaded a file containing a small section of artemis.log, see https://filebin.net/emt7r3h7yg30zq8e/artemis_section.log?t=r9f67qpl\r\n"
            },
            {
              "comment_username": "franz1981",
              "comment_create_time": "2019-10-29T06:25:42Z",
              "comment_edit_time": "2019-10-29T19:07:06Z",
              "comment_text": "Ok, now it is more clear, thanks! \r\nSo ByteBufs start to be accumulated due to FileRegions time to send.\r\nI will take another look to the file region = false case, but probably accounting correctly with a custom msg estimator could make the back-pressure to be triggered before, but i'm not sure...\r\n\r\nIn the worst case I could just use something similar to what we were using before, maybe cleaning it up a lil more given that we are touching these bits "
            },
            {
              "comment_username": "franz1981",
              "comment_create_time": "2019-10-31T19:18:17Z",
              "comment_edit_time": "2019-10-31T19:18:17Z",
              "comment_text": "@wy96f I've tried to understand what's happening for ChunkedNioFile and probably it needs to be fixed Netty-side (see https://github.com/netty/netty/issues/3413#issuecomment-548501688), let's see."
            },
            {
              "comment_username": "franz1981",
              "comment_create_time": "2019-11-04T10:45:17Z",
              "comment_edit_time": "2019-11-04T10:49:07Z",
              "comment_text": "@wy96f I've added an additional commit to control the backlog of unflushed Netty writes, to improve the precision of the initial replication timeout performed right after \"sending\" all the replicated files: due to how Netty works (the previous comment has the relevant info about it), probably is the safer way to fail fast due to slow network/dead connections on replication. Let me know if your test won't be too slow due to this (with both zero copy or not) and if it seems correct now (without changing the default timeout on initial replication). "
            }
          ]
        },
        {
          "issue_url": "https://github.com/apache/activemq-artemis/issues/2839",
          "issue_title": "[ARTEMIS-2490] Prevent NumberFormatExc when reading large message",
          "issue_number": 2839,
          "issue_text": "",
          "issue_comments": []
        },
        {
          "issue_url": "https://github.com/apache/activemq-artemis/issues/2793",
          "issue_title": "ARTEMIS-2452 group-name ignored in shared store colocated setup",
          "issue_number": 2793,
          "issue_text": "",
          "issue_comments": [
            {
              "comment_username": "franz1981",
              "comment_create_time": "2019-08-13T15:38:16Z",
              "comment_edit_time": "2019-08-13T15:38:16Z",
              "comment_text": "This PR is adding `BackupRequestMessage`'s `nodeID` information for shared store colocated setup, to enable proper validation of backup group names: I don't know yet how to handle the case where old brokers are communicating with new ones, but throwing an exception + logging a warn message. I'm opened to other approaches ;)\r\nI will add some tests on my PTO return, but I've left this here to make it available for comment/reviewes :)"
            },
            {
              "comment_username": "franz1981",
              "comment_create_time": "2019-08-13T15:42:00Z",
              "comment_edit_time": "2019-08-13T15:42:00Z",
              "comment_text": "@howardgao @jbertram wdyt?"
            },
            {
              "comment_username": "michaelandrepearce",
              "comment_create_time": "2019-08-15T06:41:51Z",
              "comment_edit_time": "2019-08-15T06:41:51Z",
              "comment_text": "Its important broker can be rolling upgraded, e.g. old broker instance can talk with new without issue so zero downtime upgrades can occur. I think the issue you called out must be addressed before this can merge, if it breaks that."
            },
            {
              "comment_username": "franz1981",
              "comment_create_time": "2019-08-15T07:50:38Z",
              "comment_edit_time": "2019-08-15T07:50:38Z",
              "comment_text": "@michaelandrepearce Agree, but nonetheless my comment on the code hold:\r\n```\r\n// Older versions of artemis don't send the nodeID in BackupRequestMessage:\r\n      // in this case we cannot trust the request, making the requesting server\r\n      // to pair with this server in any case.\r\n```\r\nOld servers cannot be trusted, because they don't provide enough information to pair with new ones ie the colocated shared store with group name wasn't a feature correctly implemented.\r\nIf I let them behave as before it can break a new cluster: atm i can't see solutions to preserve a partial cluster upgrade."
            },
            {
              "comment_username": "michaelandrepearce",
              "comment_create_time": "2019-08-15T07:58:32Z",
              "comment_edit_time": "2019-08-15T07:59:34Z",
              "comment_text": "Why not have a setting that makes it behave as old version, then once all servers uograded the setting can change to strict..making the flag dynamic would also mean the last change can be done without need for further bounces \r\n"
            },
            {
              "comment_username": "michaelandrepearce",
              "comment_create_time": "2019-08-15T08:05:51Z",
              "comment_edit_time": "2019-08-15T08:07:22Z",
              "comment_text": "Also why would having old logic break new? Another option is to version the message, and if you get v1 do old logic, if you get new v2 message do new. Then no need even for a flag. This way in a brand new setup it will all be good, but ln an old / partially transitioned setup you get same guarentees as old, until all fully upgraded after which you're all good."
            },
            {
              "comment_username": "franz1981",
              "comment_create_time": "2019-08-15T08:14:34Z",
              "comment_edit_time": "2019-08-15T08:14:58Z",
              "comment_text": "The version proposal is probably the best solution indeed: the reason why the old logic breaks the new one is that the old one just ignores the group names and let any requesting server that arrive first to be able to form a pair on any group name regardless both the requesting server group name and the target server. It was just totally broken..."
            },
            {
              "comment_username": "michaelandrepearce",
              "comment_create_time": "2019-08-15T08:16:23Z",
              "comment_edit_time": "2019-08-15T08:17:09Z",
              "comment_text": "Well it can't be totally broken, theres users out there using it (mostly because they are forming single groups or ensuring clusters are rign fenced, therefor group name isnt really mandatory there, yes theres a bug if you need multiple, and  thats not ideal, but its not totally broken as you say,"
            },
            {
              "comment_username": "franz1981",
              "comment_create_time": "2019-08-15T08:19:39Z",
              "comment_edit_time": "2019-08-15T08:19:39Z",
              "comment_text": "The feature of shared store colocated is not totally broken, but the shared store colocation using group names, yes.\r\nBut is normal, if the were not using at all group names in their logic I believe that's ok.."
            },
            {
              "comment_username": "franz1981",
              "comment_create_time": "2019-08-15T08:24:53Z",
              "comment_edit_time": "2019-08-15T08:24:53Z",
              "comment_text": "I understand your comment now, my comment was related to the code itself,not the feature as a whole: I see that there exists cases where it can work.."
            },
            {
              "comment_username": "michaelandrepearce",
              "comment_create_time": "2019-08-15T08:30:04Z",
              "comment_edit_time": "2019-08-15T08:30:04Z",
              "comment_text": "@franz1981 i assume you're going to have a think on this and re-work bits? e.g. no further comments needed for now right?"
            },
            {
              "comment_username": "franz1981",
              "comment_create_time": "2019-08-15T08:39:00Z",
              "comment_edit_time": "2019-08-15T08:39:16Z",
              "comment_text": "Yes, sure and will provide proper test coverage too as it deserves :)\r\nThanks for the suggestions Michael!!"
            },
            {
              "comment_username": "clebertsuconic",
              "comment_create_time": "2019-08-22T02:05:50Z",
              "comment_edit_time": "2019-08-22T02:05:50Z",
              "comment_text": "can you rebase this? it will trigger a new build after fixed."
            },
            {
              "comment_username": "michaelandrepearce",
              "comment_create_time": "2019-08-23T15:47:23Z",
              "comment_edit_time": "2019-08-23T15:51:06Z",
              "comment_text": "@clebertsuconic its not ready for merge anyhow. Waiting on @franz1981 atm"
            },
            {
              "comment_username": "clebertsuconic",
              "comment_create_time": "2019-08-30T13:28:43Z",
              "comment_edit_time": "2019-08-30T13:28:43Z",
              "comment_text": "@michaelandrepearce I'm marking as DO-NOT-MERGE-YET... it helps me to not mess up."
            }
          ]
        },
        {
          "issue_url": "https://github.com/apache/activemq-artemis/issues/2792",
          "issue_title": "NO-JIRA: Javadoc for ActiveMQServerMessagePlugin interface",
          "issue_number": 2792,
          "issue_text": "",
          "issue_comments": [
            {
              "comment_username": "jirkadanek",
              "comment_create_time": "2019-08-12T11:41:31Z",
              "comment_edit_time": "2019-08-12T11:41:31Z",
              "comment_text": "I still need to try this with a broker cluster, to see what exactly happens there."
            },
            {
              "comment_username": "jbertram",
              "comment_create_time": "2019-08-13T17:11:29Z",
              "comment_edit_time": "2019-08-13T17:11:29Z",
              "comment_text": "I think this would be better in the User Guide rather than the JavaDoc for this class."
            },
            {
              "comment_username": "jirkadanek",
              "comment_create_time": "2019-08-15T07:46:05Z",
              "comment_edit_time": "2019-08-15T07:46:05Z",
              "comment_text": "Ok, I can move it there.\r\n\r\nI saw some java classes with quite extensive Javadoc, and I felt that explaining how user provided methods are called belongs in a Javadoc."
            },
            {
              "comment_username": "jirkadanek",
              "comment_create_time": "2019-08-15T15:20:29Z",
              "comment_edit_time": "2019-08-15T15:20:29Z",
              "comment_text": "(Does it look reasonable so far, what I wrote? Especially that using ThreadLocal is a good idea?)"
            },
            {
              "comment_username": "jbertram",
              "comment_create_time": "2019-08-15T15:46:21Z",
              "comment_edit_time": "2019-08-15T15:46:21Z",
              "comment_text": "I think what you wrote looks good. I wouldn't go as far as saying that using a `ThreadLocal` is a *good* idea, but it should work nonetheless. :)"
            }
          ]
        },
        {
          "issue_url": "https://github.com/apache/activemq-artemis/issues/2643",
          "issue_title": "ARTEMIS-2320 Update javac-errorprone to 2.8.5 and error_prone_core to 2.3.3",
          "issue_number": 2643,
          "issue_text": "",
          "issue_comments": [
            {
              "comment_username": "clebertsuconic",
              "comment_create_time": "2019-04-25T15:47:46Z",
              "comment_edit_time": "2019-04-25T15:47:46Z",
              "comment_text": "lets wait the release before merging this please!"
            },
            {
              "comment_username": "jbertram",
              "comment_create_time": "2020-01-10T18:22:01Z",
              "comment_edit_time": "2020-01-10T18:22:01Z",
              "comment_text": "This has been sitting for awhile, and now there is a conflict. @jdanekrh, can you resolve the conflict and we can get this merged once the 2.11.0 release is done."
            },
            {
              "comment_username": "jirkadanek",
              "comment_create_time": "2020-01-15T13:07:21Z",
              "comment_edit_time": "2020-01-15T13:07:21Z",
              "comment_text": "Conflict fixed. When this is merged, I'll do a 2.3.4 update. The release notes of error_prone promise better overall compile performance, which sounds attractive."
            },
            {
              "comment_username": "jirkadanek",
              "comment_create_time": "2020-01-23T14:44:44Z",
              "comment_edit_time": "2020-01-23T14:45:19Z",
              "comment_text": "(Tried that 2.3.4 and it is not any faster, partially given that time to run tests dominates any build ;("
            }
          ]
        },
        {
          "issue_url": "https://github.com/apache/activemq-artemis/issues/2220",
          "issue_title": "[ARTEMIS-1946] Cluster with allow-direct-connections-only=\"true\" and …",
          "issue_number": 2220,
          "issue_text": "…localAddress in netty connections with allow-direct-connections-only=\"true\" prevents core bridge to be created.\\r\\n\\r\\nIssue: https://issues.apache.org/jira/browse/ARTEMIS-1946\\r\\n\\r\\nLocalAddress and localPort have  to be removed from org.apache.activemq.artemis.core.server.cluster.impl.ClusterConnectionImpl.allowableConnections in order to compare correctly TransportConfiguration during detection of direct connection (these TransportConfiguration don't contain local addresses/ports other nodes)",
          "issue_comments": []
        }
      ]
    }
  },
  {
    "github_url": "https://github.com/elastic/elasticsearch",
    "github_info": {
      "name": "elastic/elasticsearch",
      "language": "Java"
    },
    "github_branches": {
      "branch_datas": [
        {
          "branch_version": "0.12",
          "branch_url": "https://github.com/elastic/elasticsearch/tree/0.12",
          "branch_download_url": "https://github.com/elastic/elasticsearch/archive/0.12.zip"
        },
        {
          "branch_version": "0.13",
          "branch_url": "https://github.com/elastic/elasticsearch/tree/0.13",
          "branch_download_url": "https://github.com/elastic/elasticsearch/archive/0.13.zip"
        },
        {
          "branch_version": "0.14",
          "branch_url": "https://github.com/elastic/elasticsearch/tree/0.14",
          "branch_download_url": "https://github.com/elastic/elasticsearch/archive/0.14.zip"
        },
        {
          "branch_version": "0.15",
          "branch_url": "https://github.com/elastic/elasticsearch/tree/0.15",
          "branch_download_url": "https://github.com/elastic/elasticsearch/archive/0.15.zip"
        },
        {
          "branch_version": "0.16",
          "branch_url": "https://github.com/elastic/elasticsearch/tree/0.16",
          "branch_download_url": "https://github.com/elastic/elasticsearch/archive/0.16.zip"
        },
        {
          "branch_version": "0.17",
          "branch_url": "https://github.com/elastic/elasticsearch/tree/0.17",
          "branch_download_url": "https://github.com/elastic/elasticsearch/archive/0.17.zip"
        },
        {
          "branch_version": "0.18",
          "branch_url": "https://github.com/elastic/elasticsearch/tree/0.18",
          "branch_download_url": "https://github.com/elastic/elasticsearch/archive/0.18.zip"
        },
        {
          "branch_version": "0.19",
          "branch_url": "https://github.com/elastic/elasticsearch/tree/0.19",
          "branch_download_url": "https://github.com/elastic/elasticsearch/archive/0.19.zip"
        },
        {
          "branch_version": "0.20",
          "branch_url": "https://github.com/elastic/elasticsearch/tree/0.20",
          "branch_download_url": "https://github.com/elastic/elasticsearch/archive/0.20.zip"
        },
        {
          "branch_version": "0.90",
          "branch_url": "https://github.com/elastic/elasticsearch/tree/0.90",
          "branch_download_url": "https://github.com/elastic/elasticsearch/archive/0.90.zip"
        },
        {
          "branch_version": "1.0",
          "branch_url": "https://github.com/elastic/elasticsearch/tree/1.0",
          "branch_download_url": "https://github.com/elastic/elasticsearch/archive/1.0.zip"
        },
        {
          "branch_version": "1.1",
          "branch_url": "https://github.com/elastic/elasticsearch/tree/1.1",
          "branch_download_url": "https://github.com/elastic/elasticsearch/archive/1.1.zip"
        },
        {
          "branch_version": "1.2",
          "branch_url": "https://github.com/elastic/elasticsearch/tree/1.2",
          "branch_download_url": "https://github.com/elastic/elasticsearch/archive/1.2.zip"
        },
        {
          "branch_version": "1.3",
          "branch_url": "https://github.com/elastic/elasticsearch/tree/1.3",
          "branch_download_url": "https://github.com/elastic/elasticsearch/archive/1.3.zip"
        },
        {
          "branch_version": "1.4",
          "branch_url": "https://github.com/elastic/elasticsearch/tree/1.4",
          "branch_download_url": "https://github.com/elastic/elasticsearch/archive/1.4.zip"
        },
        {
          "branch_version": "1.5",
          "branch_url": "https://github.com/elastic/elasticsearch/tree/1.5",
          "branch_download_url": "https://github.com/elastic/elasticsearch/archive/1.5.zip"
        },
        {
          "branch_version": "1.6",
          "branch_url": "https://github.com/elastic/elasticsearch/tree/1.6",
          "branch_download_url": "https://github.com/elastic/elasticsearch/archive/1.6.zip"
        },
        {
          "branch_version": "1.7",
          "branch_url": "https://github.com/elastic/elasticsearch/tree/1.7",
          "branch_download_url": "https://github.com/elastic/elasticsearch/archive/1.7.zip"
        },
        {
          "branch_version": "2.0",
          "branch_url": "https://github.com/elastic/elasticsearch/tree/2.0",
          "branch_download_url": "https://github.com/elastic/elasticsearch/archive/2.0.zip"
        },
        {
          "branch_version": "2.1",
          "branch_url": "https://github.com/elastic/elasticsearch/tree/2.1",
          "branch_download_url": "https://github.com/elastic/elasticsearch/archive/2.1.zip"
        },
        {
          "branch_version": "2.2",
          "branch_url": "https://github.com/elastic/elasticsearch/tree/2.2",
          "branch_download_url": "https://github.com/elastic/elasticsearch/archive/2.2.zip"
        },
        {
          "branch_version": "2.3",
          "branch_url": "https://github.com/elastic/elasticsearch/tree/2.3",
          "branch_download_url": "https://github.com/elastic/elasticsearch/archive/2.3.zip"
        },
        {
          "branch_version": "2.4",
          "branch_url": "https://github.com/elastic/elasticsearch/tree/2.4",
          "branch_download_url": "https://github.com/elastic/elasticsearch/archive/2.4.zip"
        },
        {
          "branch_version": "5.0",
          "branch_url": "https://github.com/elastic/elasticsearch/tree/5.0",
          "branch_download_url": "https://github.com/elastic/elasticsearch/archive/5.0.zip"
        },
        {
          "branch_version": "5.1",
          "branch_url": "https://github.com/elastic/elasticsearch/tree/5.1",
          "branch_download_url": "https://github.com/elastic/elasticsearch/archive/5.1.zip"
        },
        {
          "branch_version": "5.2",
          "branch_url": "https://github.com/elastic/elasticsearch/tree/5.2",
          "branch_download_url": "https://github.com/elastic/elasticsearch/archive/5.2.zip"
        },
        {
          "branch_version": "5.3",
          "branch_url": "https://github.com/elastic/elasticsearch/tree/5.3",
          "branch_download_url": "https://github.com/elastic/elasticsearch/archive/5.3.zip"
        },
        {
          "branch_version": "5.4",
          "branch_url": "https://github.com/elastic/elasticsearch/tree/5.4",
          "branch_download_url": "https://github.com/elastic/elasticsearch/archive/5.4.zip"
        },
        {
          "branch_version": "5.5",
          "branch_url": "https://github.com/elastic/elasticsearch/tree/5.5",
          "branch_download_url": "https://github.com/elastic/elasticsearch/archive/5.5.zip"
        },
        {
          "branch_version": "5.6",
          "branch_url": "https://github.com/elastic/elasticsearch/tree/5.6",
          "branch_download_url": "https://github.com/elastic/elasticsearch/archive/5.6.zip"
        }
      ]
    },
    "github_pull_requests": {
      "pull_datas": [
        {
          "pull_number": 63024,
          "pull_title": "[ML] Use registry for transform templates",
          "pull_version": "elastic:master",
          "pull_version_url": "https://github.com/elastic/elasticsearch/tree/master"
        },
        {
          "pull_number": 63023,
          "pull_title": "[DOCS] Add experimental tag to inference processor and aggregation",
          "pull_version": "elastic:master",
          "pull_version_url": "https://github.com/elastic/elasticsearch/tree/master"
        },
        {
          "pull_number": 63022,
          "pull_title": "[ML] Add timeouts to named pipe connections",
          "pull_version": "elastic:7.x",
          "pull_version_url": "https://github.com/elastic/elasticsearch/tree/7.x"
        },
        {
          "pull_number": 63021,
          "pull_title": "[DOCS] Adds limitation item about using scripts in transforms",
          "pull_version": "elastic:master",
          "pull_version_url": "https://github.com/elastic/elasticsearch/tree/master"
        },
        {
          "pull_number": 63020,
          "pull_title": "[7.9] [DOCS] Backport normalize aggregation fix (#63017)",
          "pull_version": "elastic:7.9",
          "pull_version_url": "https://github.com/elastic/elasticsearch/tree/7.9"
        },
        {
          "pull_number": 63018,
          "pull_title": "Convert test field mappers to parametrized forms",
          "pull_version": "elastic:master",
          "pull_version_url": "https://github.com/elastic/elasticsearch/tree/master"
        },
        {
          "pull_number": 63016,
          "pull_title": "[DOCS] Adds data stream and ILM related limitation items to transforms",
          "pull_version": "elastic:master",
          "pull_version_url": "https://github.com/elastic/elasticsearch/tree/master"
        },
        {
          "pull_number": 63012,
          "pull_title": "[ML] Add data frame analytics bwc testing",
          "pull_version": "elastic:7.x",
          "pull_version_url": "https://github.com/elastic/elasticsearch/tree/7.x"
        },
        {
          "pull_number": 63006,
          "pull_title": "[Transform] fix issue in TransformIndexerStateTests.testStopAtCheckpoint",
          "pull_version": "elastic:master",
          "pull_version_url": "https://github.com/elastic/elasticsearch/tree/master"
        },
        {
          "pull_number": 63001,
          "pull_title": "[ML] has a datafeed task to wait for assignment if no indices are available and allow_no_indices is true",
          "pull_version": "elastic:master",
          "pull_version_url": "https://github.com/elastic/elasticsearch/tree/master"
        },
        {
          "pull_number": 62995,
          "pull_title": "InternalClusterInfoService should not ignore hidden indices",
          "pull_version": "elastic:master",
          "pull_version_url": "https://github.com/elastic/elasticsearch/tree/master"
        },
        {
          "pull_number": 62992,
          "pull_title": "Convert dense/sparse vector field mappers to Parametrized form",
          "pull_version": "elastic:master",
          "pull_version_url": "https://github.com/elastic/elasticsearch/tree/master"
        },
        {
          "pull_number": 62985,
          "pull_title": "Introduce yamlRestCompatTest task and plugin. ",
          "pull_version": "elastic:master",
          "pull_version_url": "https://github.com/elastic/elasticsearch/tree/master"
        },
        {
          "pull_number": 62984,
          "pull_title": "System index auto-creation should not be disabled by user settings",
          "pull_version": "elastic:master",
          "pull_version_url": "https://github.com/elastic/elasticsearch/tree/master"
        },
        {
          "pull_number": 62983,
          "pull_title": "Make Rounding.nextRoundingValue consistent",
          "pull_version": "elastic:master",
          "pull_version_url": "https://github.com/elastic/elasticsearch/tree/master"
        },
        {
          "pull_number": 62974,
          "pull_title": "Move FieldMapper#valueFetcher to MappedFieldType",
          "pull_version": "elastic:master",
          "pull_version_url": "https://github.com/elastic/elasticsearch/tree/master"
        },
        {
          "pull_number": 62968,
          "pull_title": "Use services for archive and file operations in tasks",
          "pull_version": "elastic:master",
          "pull_version_url": "https://github.com/elastic/elasticsearch/tree/master"
        },
        {
          "pull_number": 62966,
          "pull_title": "[7.x] [ML] Set parent task Id on ml expired data removers (#62854)",
          "pull_version": "elastic:7.x",
          "pull_version_url": "https://github.com/elastic/elasticsearch/tree/7.x"
        },
        {
          "pull_number": 62960,
          "pull_title": "[ML] Delete dest index and reindex if incompatible",
          "pull_version": "elastic:master",
          "pull_version_url": "https://github.com/elastic/elasticsearch/tree/master"
        },
        {
          "pull_number": 62953,
          "pull_title": "Adding script support for version parts",
          "pull_version": "elastic:master",
          "pull_version_url": "https://github.com/elastic/elasticsearch/tree/master"
        },
        {
          "pull_number": 62947,
          "pull_title": "Async search status",
          "pull_version": "elastic:master",
          "pull_version_url": "https://github.com/elastic/elasticsearch/tree/master"
        },
        {
          "pull_number": 62944,
          "pull_title": "Fix Race in ClusterApplierService Shutdown",
          "pull_version": "elastic:master",
          "pull_version_url": "https://github.com/elastic/elasticsearch/tree/master"
        },
        {
          "pull_number": 62943,
          "pull_title": "SQL: Fix exception when using CAST on inexact field",
          "pull_version": "elastic:master",
          "pull_version_url": "https://github.com/elastic/elasticsearch/tree/master"
        },
        {
          "pull_number": 62938,
          "pull_title": "Convert all FieldMappers in mapper-extras to parametrized form",
          "pull_version": "elastic:master",
          "pull_version_url": "https://github.com/elastic/elasticsearch/tree/master"
        },
        {
          "pull_number": 62937,
          "pull_title": "Cleanup on integtest distribution setup",
          "pull_version": "elastic:master",
          "pull_version_url": "https://github.com/elastic/elasticsearch/tree/master"
        },
        {
          "pull_number": 62936,
          "pull_title": "SQL: Allow skip of bwc tests on `check` task",
          "pull_version": "elastic:master",
          "pull_version_url": "https://github.com/elastic/elasticsearch/tree/master"
        },
        {
          "pull_number": 62932,
          "pull_title": "EQL: Allow escaped backquote in identifiers",
          "pull_version": "elastic:master",
          "pull_version_url": "https://github.com/elastic/elasticsearch/tree/master"
        },
        {
          "pull_number": 62930,
          "pull_title": "Add equals and hashcode implementation to KnownCardinalityUpperBound",
          "pull_version": "elastic:master",
          "pull_version_url": "https://github.com/elastic/elasticsearch/tree/master"
        },
        {
          "pull_number": 62922,
          "pull_title": "Aggregations: Implement linear interpolation for  percentiles_bucket",
          "pull_version": "elastic:master",
          "pull_version_url": "https://github.com/elastic/elasticsearch/tree/master"
        },
        {
          "pull_number": 62917,
          "pull_title": "Fix API key role descriptors rewrite bug for upgraded clusters",
          "pull_version": "elastic:master",
          "pull_version_url": "https://github.com/elastic/elasticsearch/tree/master"
        }
      ]
    },
    "github_issues": {
      "issue_datas": [
        {
          "issue_url": "https://github.com/elastic/elasticsearch/issues/63024",
          "issue_title": "[ML] Use registry for transform templates",
          "issue_number": 63024,
          "issue_text": "",
          "issue_comments": []
        },
        {
          "issue_url": "https://github.com/elastic/elasticsearch/issues/63023",
          "issue_title": "[DOCS] Add experimental tag to inference processor and aggregation",
          "issue_number": 63023,
          "issue_text": "This PR adds the appropriate tags to [Inference bucket aggregation](https://www.elastic.co/guide/en/elasticsearch/reference/master/search-aggregations-pipeline-inference-bucket-aggregation.html) and [Inference processor](https://www.elastic.co/guide/en/elasticsearch/reference/master/inference-processor.html) documentation.",
          "issue_comments": []
        },
        {
          "issue_url": "https://github.com/elastic/elasticsearch/issues/63022",
          "issue_title": "[ML] Add timeouts to named pipe connections",
          "issue_number": 63022,
          "issue_text": "This PR adds timeouts to the named pipe connections of the\r\nautodetect, normalize and data_frame_analyzer processes.\r\nThis argument requires the changes of elastic/ml-cpp#1514 in\r\norder to work, so that PR will be merged before this one.\r\n(The controller process already had a different mechanism,\r\ntied to the ES JVM lifetime.)\r\n\r\nBackport of #62993",
          "issue_comments": []
        },
        {
          "issue_url": "https://github.com/elastic/elasticsearch/issues/63021",
          "issue_title": "[DOCS] Adds limitation item about using scripts in transforms",
          "issue_number": 63021,
          "issue_text": "## Overview\r\n\r\nThis PR adds some notes on how to work with scripts in transforms.\r\n\r\n### Preview\r\n\r\n[Using scripts in transforms - limitation](https://elasticsearch_63021.docs-preview.app.elstc.co/guide/en/elasticsearch/reference/master/transform-limitations.html#transform-painless-imitation)",
          "issue_comments": [
            {
              "comment_username": "elasticmachine",
              "comment_create_time": "2020-09-29T15:27:52Z",
              "comment_edit_time": "2020-09-29T15:27:52Z",
              "comment_text": "Pinging @elastic/ml-core (:ml/Transform)"
            },
            {
              "comment_username": "elasticmachine",
              "comment_create_time": "2020-09-29T15:27:52Z",
              "comment_edit_time": "2020-09-29T15:27:52Z",
              "comment_text": "Pinging @elastic/es-docs (>docs)"
            }
          ]
        },
        {
          "issue_url": "https://github.com/elastic/elasticsearch/issues/63020",
          "issue_title": "[7.9] [DOCS] Backport normalize aggregation fix (#63017)",
          "issue_number": 63020,
          "issue_text": "Backports the following commits to 7.9:\n - [DOCS] Backport normalize aggregation fix (#63017)",
          "issue_comments": []
        },
        {
          "issue_url": "https://github.com/elastic/elasticsearch/issues/63019",
          "issue_title": "Dry run configs like index templates",
          "issue_number": 63019,
          "issue_text": "We plan to store index templates in Phabricator and would like to validate a diff before landing it and HTTP PUT to ES.\r\n\r\nWithout dry-run support, we would have to either hope that the new config does not break ES, or find a way to hack some local ES instance to apply the config to.\r\n\r\nA dry-run param in the HTTP PUT request should be easy to implement and would make API users' life much easier.",
          "issue_comments": [
            {
              "comment_username": "dakrone",
              "comment_create_time": "2020-09-29T15:20:35Z",
              "comment_edit_time": "2020-09-29T15:21:25Z",
              "comment_text": "@shenwan maybe you can clarify, when you say \"does not break ES\", how does ES break when putting a template in place?\r\n\r\nAlso, what would a dry-run parameter actually do? We already have an API to simulate adding an index template: https://www.elastic.co/guide/en/elasticsearch/reference/7.9/indices-simulate-template.html"
            },
            {
              "comment_username": "elasticmachine",
              "comment_create_time": "2020-09-29T15:21:36Z",
              "comment_edit_time": "2020-09-29T15:21:36Z",
              "comment_text": "Pinging @elastic/es-core-features (:Core/Features/Indices APIs)"
            }
          ]
        },
        {
          "issue_url": "https://github.com/elastic/elasticsearch/issues/63018",
          "issue_title": "Convert test field mappers to parametrized forms",
          "issue_number": 63018,
          "issue_text": "Relates to #62988 ",
          "issue_comments": [
            {
              "comment_username": "elasticmachine",
              "comment_create_time": "2020-09-29T15:04:02Z",
              "comment_edit_time": "2020-09-29T15:04:02Z",
              "comment_text": "Pinging @elastic/es-search (:Search/Mapping)"
            },
            {
              "comment_username": "romseygeek",
              "comment_create_time": "2020-09-29T15:28:19Z",
              "comment_edit_time": "2020-09-29T15:28:19Z",
              "comment_text": "@elasticmachine run elasticsearch-ci/packaging-sample-windows "
            }
          ]
        },
        {
          "issue_url": "https://github.com/elastic/elasticsearch/issues/63016",
          "issue_title": "[DOCS] Adds data stream and ILM related limitation items to transforms",
          "issue_number": 63016,
          "issue_text": "## Overview\r\n\r\nThis PR adds two limitation items to the transform-related limitations section:\r\n* Data streams as destination indices are not supported\r\n* ILM as destination index may cause duplicated documents\r\n\r\n### Preview\r\n\r\n[Limitations - data streams](https://elasticsearch_63016.docs-preview.app.elstc.co/guide/en/elasticsearch/reference/master/transform-limitations.html#transform-data-streams-destination)\r\n[Limitations – ILM](https://elasticsearch_63016.docs-preview.app.elstc.co/guide/en/elasticsearch/reference/master/transform-limitations.html#transform-ilm-destination)",
          "issue_comments": [
            {
              "comment_username": "elasticmachine",
              "comment_create_time": "2020-09-29T14:18:52Z",
              "comment_edit_time": "2020-09-29T14:18:52Z",
              "comment_text": "Pinging @elastic/ml-core (:ml/Transform)"
            },
            {
              "comment_username": "elasticmachine",
              "comment_create_time": "2020-09-29T14:18:52Z",
              "comment_edit_time": "2020-09-29T14:18:52Z",
              "comment_text": "Pinging @elastic/es-docs (>docs)"
            },
            {
              "comment_username": "hendrikmuhs",
              "comment_create_time": "2020-09-29T15:39:56Z",
              "comment_edit_time": "2020-09-29T15:39:56Z",
              "comment_text": "Related issue regarding data stream support: https://github.com/elastic/elasticsearch/issues/62712"
            }
          ]
        },
        {
          "issue_url": "https://github.com/elastic/elasticsearch/issues/63012",
          "issue_title": "[ML] Add data frame analytics bwc testing",
          "issue_number": 63012,
          "issue_text": "This commit adds bwc testing for data frame analytics.\r\n\r\nThe bwc tests only go back to the 7.9.0. \r\nMeaning, initially only rolling upgrades from 7.9.x -> 7.10.0 are tested.\r\n\r\nSince the feature was experimental in < 7.9.0, this is acceptable.",
          "issue_comments": [
            {
              "comment_username": "elasticmachine",
              "comment_create_time": "2020-09-29T14:01:46Z",
              "comment_edit_time": "2020-09-29T14:01:46Z",
              "comment_text": "Pinging @elastic/ml-core (:ml)"
            },
            {
              "comment_username": "benwtrent",
              "comment_create_time": "2020-09-29T14:53:01Z",
              "comment_edit_time": "2020-09-29T14:53:01Z",
              "comment_text": "run elasticsearch-ci/2"
            }
          ]
        },
        {
          "issue_url": "https://github.com/elastic/elasticsearch/issues/63006",
          "issue_title": "[Transform] fix issue in TransformIndexerStateTests.testStopAtCheckpoint",
          "issue_number": 63006,
          "issue_text": "fix a test issue by improving counting the number of times the deferred listener is called\r\n\r\nfixes #62996\r\n\r\nNote: The test failed for a certain seed that called `_stop_at_checkpoint=false` all 6 times. I run this test with `-Dtests.iters` and could not spot another corner case",
          "issue_comments": [
            {
              "comment_username": "elasticmachine",
              "comment_create_time": "2020-09-29T13:15:53Z",
              "comment_edit_time": "2020-09-29T13:15:53Z",
              "comment_text": "Pinging @elastic/ml-core (:ml/Transform)"
            },
            {
              "comment_username": "hendrikmuhs",
              "comment_create_time": "2020-09-29T15:27:00Z",
              "comment_edit_time": "2020-09-29T15:27:00Z",
              "comment_text": "run elasticsearch-ci/packaging-sample-windows"
            }
          ]
        },
        {
          "issue_url": "https://github.com/elastic/elasticsearch/issues/63005",
          "issue_title": "Unexpected behaviour for size in UpdateByQuery",
          "issue_number": 63005,
          "issue_text": "**Elasticsearch version**\\r\\n\\r\\nVersion: 7.9.2, Build: default/tar/d34da0ea4a966c4e49417f2da2f244e3e97b4e6e/2020-09-23T00:45:33.626720Z, JVM: 15\\r\\n\\r\\n**JVM version**\\r\\n\\r\\njavac 1.8.0_172\\r\\n\\r\\n**Description of the problem including expected versus actual behavior**:\\r\\n\\r\\nIn 7.x the `size` parameter was deprecated in favour of `max_docs`.\\r\\nWhen `size` is passed as a query parameter, Elasticsearch gives a deprecation message, but it still takes `size` into account.\\r\\n\\r\\nHowever, when `size` is passed in the body of the request, the behaviour is completely different.\\r\\nNo deprecation message is trigerred.\\r\\nFurthermore Elasticsearch just updates all the documents that match the query.\\r\\nA closer look also shows that in this case `size` acts more like `scroll_size`, because it seems to dictate the batch size.\\r\\n\\r\\nWhen `size` is passed in the query, it works as expected:\\r\\n```\\r\\nPOST my-index/_update_by_query?size=10\\r\\n{\\r\\n  \"query\": {\\r\\n    \"term\": {\\r\\n      \"year$string\": 2007\\r\\n    }\\r\\n  },\\r\\n  \"script\": {\\r\\n    \"source\": \"ctx._source['year$string'] = 3000;\"\\r\\n  }\\r\\n}\\r\\n\\r\\n#! Deprecation: Deprecated field [size] used, expected [max_docs] instead\\r\\n{\\r\\n  \"took\" : 23,\\r\\n  \"timed_out\" : false,\\r\\n  \"total\" : 10,\\r\\n  \"updated\" : 10,\\r\\n  \"deleted\" : 0,\\r\\n  \"batches\" : 1,\\r\\n  \"version_conflicts\" : 0,\\r\\n  \"noops\" : 0,\\r\\n  \"retries\" : {\\r\\n    \"bulk\" : 0,\\r\\n    \"search\" : 0\\r\\n  },\\r\\n  \"throttled_millis\" : 0,\\r\\n  \"requests_per_second\" : -1.0,\\r\\n  \"throttled_until_millis\" : 0,\\r\\n  \"failures\" : [ ]\\r\\n}\\r\\n```\\r\\n\\r\\nWhen `size` is passed in the body, all docs are updated and `size` seems to act more like the batch size:\\r\\n\\r\\n```\\r\\nPOST my-index/_update_by_query\\r\\n{\\r\\n  \"query\": {\\r\\n    \"term\": {\\r\\n      \"year$string\": 2007\\r\\n    }\\r\\n  },\\r\\n  \"script\": {\\r\\n    \"source\": \"ctx._source['year$string'] = 3000;\"\\r\\n  },\\r\\n  \"size\": 10\\r\\n}\\r\\n\\r\\n{\\r\\n  \"took\" : 488,\\r\\n  \"timed_out\" : false,\\r\\n  \"total\" : 354,\\r\\n  \"updated\" : 354,\\r\\n  \"deleted\" : 0,\\r\\n  \"batches\" : 36,\\r\\n  \"version_conflicts\" : 0,\\r\\n  \"noops\" : 0,\\r\\n  \"retries\" : {\\r\\n    \"bulk\" : 0,\\r\\n    \"search\" : 0\\r\\n  },\\r\\n  \"throttled_millis\" : 0,\\r\\n  \"requests_per_second\" : -1.0,\\r\\n  \"throttled_until_millis\" : 0,\\r\\n  \"failures\" : [ ]\\r\\n}\\r\\n\\r\\n```\\r\\n\\r\\nIs this the expected behaviour?\\r\\nI'd expect to at least see a deprecation message.\\r\\nIt would also be interesting to see whether in 8.x you can still pass `size` in the body, since its usage should no longer be supported.\\r\\n",
          "issue_comments": []
        },
        {
          "issue_url": "https://github.com/elastic/elasticsearch/issues/63002",
          "issue_title": "Elastic freezes after scheduled [ML] maintenance tasks",
          "issue_number": 63002,
          "issue_text": "<!--\r\nGitHub is reserved for bug reports and feature requests; it is not the place\r\nfor general questions. If you have a question or an unconfirmed bug , please\r\nvisit the [forums](https://discuss.elastic.co/c/elasticsearch).  Please also\r\ncheck your OS is [supported](https://www.elastic.co/support/matrix#show_os).\r\nIf it is not, the issue is likely to be closed.\r\n\r\nFor security vulnerabilities please only send reports to security@elastic.co.\r\nSee https://www.elastic.co/community/security for more information.\r\n\r\nPlease fill in the following details to help us reproduce the bug:\r\n-->\r\n\r\n**Elasticsearch version** (`bin/elasticsearch --version`): 6.8.2 and 6.8.12 (at least tested on those)\r\n\r\n**Plugins installed**: none, standard Elasticsearch\r\n\r\n**JVM version** (`java -version`): 1.8.0_265\r\n\r\n**OS version** (`uname -a` if on a Unix-like system): Ubuntu 20.04.1 LTS\r\n\r\n**Description of the problem including expected versus actual behavior**: Elastic (very little load, 36 open indices plus about 70 explicitly closed ones, largest 15mb) freezes after scheduled [ML] maintenance tasks did run (once a day, in this case around 2:40am). There is no load at all on Elastic at that time, just the maintenance task running. After that, socket connections are no longer served until the JVM is killed and elastic is restarted. Does happen on a single-instance installation, both with 5 shards and with one, both with less than 1000 and a little less than 1500 attributes (latter is desired config, still not outrageous). There is no info in the log about what does happen. Below is the little it does give: \r\n\r\n```\r\n[2020-09-21T02:29:00,000][INFO ][o.e.x.m.MlDailyMaintenanceService] [es.magentoshop2020.stage.es03] triggering scheduled [ML] maintenance tasks\r\n[2020-09-21T02:29:00,003][INFO ][o.e.x.m.a.TransportDeleteExpiredDataAction] [es.magentoshop2020.stage.es03] Deleting expired data\r\n[2020-09-21T02:29:00,026][INFO ][o.e.x.m.a.TransportDeleteExpiredDataAction] [es.magentoshop2020.stage.es03] Completed deletion of expired ML data\r\n[2020-09-21T02:29:00,027][INFO ][o.e.x.m.MlDailyMaintenanceService] [es.magentoshop2020.stage.es03] Successfully completed [ML] maintenance tasks\r\n[2020-09-21T02:33:27,385][WARN ][o.e.c.InternalClusterInfoService] [es.magentoshop2020.stage.es03] Failed to update node information for ClusterInfoUpdateJob within 15s timeout\r\n[2020-09-21T02:33:42,386][WARN ][o.e.c.InternalClusterInfoService] [es.magentoshop2020.stage.es03] Failed to update shard information for ClusterInfoUpdateJob within 15s timeout\r\n[2020-09-21T02:34:27,387][WARN ][o.e.c.InternalClusterInfoService] [es.magentoshop2020.stage.es03] Failed to update node information for ClusterInfoUpdateJob within 15s timeout\r\n[2020-09-21T02:34:42,387][WARN ][o.e.c.InternalClusterInfoService] [es.magentoshop2020.stage.es03] Failed to update shard information for ClusterInfoUpdateJob within 15s timeout\r\n[2020-09-21T02:35:27,388][WARN ][o.e.c.InternalClusterInfoService] [es.magentoshop2020.stage.es03] Failed to update node information for ClusterInfoUpdateJob within 15s timeout\r\n[2020-09-21T02:35:42,388][WARN ][o.e.c.InternalClusterInfoService] [es.magentoshop2020.stage.es03] Failed to update shard information for ClusterInfoUpdateJob within 15s timeout\r\n```\r\n\r\nNow we check and restart - however: any hint or idea is certainly appreciated. \r\n\r\n\r\n**Steps to reproduce**:\r\n\r\nPlease include a *minimal* but *complete* recreation of the problem,\r\nincluding (e.g.) index creation, mappings, settings, query etc.  The easier\r\nyou make for us to reproduce it, the more likely that somebody will take the\r\ntime to look at it.\r\n\r\n 1. Set up elasticsearch and create indices\r\n 2. Freeze during maintanance (not every day, either)\r\n\r\n**Provide logs (if relevant)**: see above\r\n\r\n\r\n",
          "issue_comments": []
        },
        {
          "issue_url": "https://github.com/elastic/elasticsearch/issues/63001",
          "issue_title": "[ML] has a datafeed task to wait for assignment if no indices are available and allow_no_indices is true",
          "issue_number": 63001,
          "issue_text": "It is conceivable that a datafeed and job should be created but the source indices are yet to be created.\r\n\r\nThe downside of this scenario is that the eager validations of the source indices are not possible.\r\n\r\nBut, once the datafeed task is assigned, those validations will effectively run again and have the datafeed fail.\r\n\r\nThis is an appropriate response as requesting that allowing the datafeed to start with no indices existing is an\r\nadvanced setting.",
          "issue_comments": []
        },
        {
          "issue_url": "https://github.com/elastic/elasticsearch/issues/62998",
          "issue_title": "[CI] BulkProcessorIT.testBulkProcessorConcurrentRequestsReadOnlyIndex",
          "issue_number": 62998,
          "issue_text": "<!--\\r\\nPlease fill out the following information, and ensure you have attempted\\r\\nto reproduce locally\\r\\n-->\\r\\n\\r\\n**Build scan**:\\r\\n\\r\\nhttps://gradle-enterprise.elastic.co/s/k77zfxjlbccki\\r\\n\\r\\n**Repro line**:\\r\\n\\r\\n```\\r\\n./gradlew ':client:rest-high-level:asyncIntegTest' --tests \"org.elasticsearch.client.BulkProcessorIT.testBulkProcessorConcurrentRequestsReadOnlyIndex\" \\\\\\r\\n  -Dtests.seed=E2CF2B7CFC578FF \\\\\\r\\n  -Dtests.security.manager=true \\\\\\r\\n  -Dtests.locale=it-IT \\\\\\r\\n  -Dtests.timezone=Etc/GMT-7 \\\\\\r\\n  -Druntime.java=11\\r\\n```\\r\\n\\r\\n**Reproduces locally?**:\\r\\n\\r\\nYes\\r\\n\\r\\n**Applicable branches**:\\r\\n\\r\\n`master`\\r\\n\\r\\n**Failure history**:\\r\\n<!--\\r\\nLink to build stats and possible indication of when this started failing and how often it fails\\r\\n<https://build-stats.elastic.co/app/kibana>\\r\\n-->\\r\\n**Failure excerpt**:\\r\\n\\r\\n```\\r\\norg.elasticsearch.client.BulkProcessorIT > testBulkProcessorConcurrentRequestsReadOnlyIndex FAILED\\r\\n    org.elasticsearch.action.ActionRequestValidationException: Validation Failed: 1: no documents to get;\\r\\n        at __randomizedtesting.SeedInfo.seed([E2CF2B7CFC578FF:8887DDCB6306302F]:0)\\r\\n        at org.elasticsearch.action.ValidateActions.addValidationError(ValidateActions.java:26)\\r\\n        at org.elasticsearch.action.get.MultiGetRequest.validate(MultiGetRequest.java:277)\\r\\n        at org.elasticsearch.client.RestHighLevelClient.performRequest(RestHighLevelClient.java:1594)\\r\\n        at org.elasticsearch.client.RestHighLevelClient.performRequestAndParseEntity(RestHighLevelClient.java:1568)\\r\\n        at org.elasticsearch.client.RestHighLevelClient.mget(RestHighLevelClient.java:831)\\r\\n        at org.elasticsearch.client.BulkProcessorIT.testBulkProcessorConcurrentRequestsReadOnlyIndex(BulkProcessorIT.java:278)\\r\\n```\\r\\n\\r\\n",
          "issue_comments": [
            {
              "comment_username": "elasticmachine",
              "comment_create_time": "2020-09-29T10:34:05Z",
              "comment_edit_time": "2020-09-29T10:34:05Z",
              "comment_text": "Pinging @elastic/es-core-features (:Core/Features/Java High Level REST Client)"
            }
          ]
        },
        {
          "issue_url": "https://github.com/elastic/elasticsearch/issues/62997",
          "issue_title": "[CI] UnsignedLongTests.testSortDifferentFormatsShouldFail",
          "issue_number": 62997,
          "issue_text": "<!--\\r\\nPlease fill out the following information, and ensure you have attempted\\r\\nto reproduce locally\\r\\n-->\\r\\n\\r\\n**Build scan**:\\r\\n\\r\\nhttps://gradle-enterprise.elastic.co/s/3oc3h67psze7o\\r\\n\\r\\n**Repro line**:\\r\\n\\r\\n```\\r\\n./gradlew ':x-pack:plugin:mapper-unsigned-long:test' --tests \"org.elasticsearch.xpack.unsignedlong.UnsignedLongTests.testSortDifferentFormatsShouldFail\" \\\\\\r\\n  -Dtests.seed=7FEBE4CC348595AE \\\\\\r\\n  -Dtests.security.manager=true \\\\\\r\\n  -Dtests.locale=zh-TW \\\\\\r\\n  -Dtests.timezone=America/Edmonton \\\\\\r\\n  -Druntime.java=11\\r\\n```\\r\\n\\r\\n**Reproduces locally?**:\\r\\n\\r\\nYes\\r\\n\\r\\n**Applicable branches**:\\r\\n\\r\\n`master`\\r\\n\\r\\n**Failure history**:\\r\\n<!--\\r\\nLink to build stats and possible indication of when this started failing and how often it fails\\r\\n<https://build-stats.elastic.co/app/kibana>\\r\\n-->\\r\\n**Failure excerpt**:\\r\\n\\r\\n```\\r\\norg.elasticsearch.xpack.unsignedlong.UnsignedLongTests > testSortDifferentFormatsShouldFail FAILED\\r\\n    junit.framework.AssertionFailedError: Expected exception SearchPhaseExecutionException but no exception was thrown\\r\\n        at __randomizedtesting.SeedInfo.seed([7FEBE4CC348595AE:BF14AAAD39F59DC9]:0)\\r\\n        at org.apache.lucene.util.LuceneTestCase.expectThrows(LuceneTestCase.java:2744)\\r\\n        at org.apache.lucene.util.LuceneTestCase.expectThrows(LuceneTestCase.java:2734)\\r\\n        at org.elasticsearch.xpack.unsignedlong.UnsignedLongTests.testSortDifferentFormatsShouldFail(UnsignedLongTests.java:280)\\r\\n```\\r\\n",
          "issue_comments": [
            {
              "comment_username": "elasticmachine",
              "comment_create_time": "2020-09-29T10:29:22Z",
              "comment_edit_time": "2020-09-29T10:29:22Z",
              "comment_text": "Pinging @elastic/es-search (:Search/Mapping)"
            }
          ]
        },
        {
          "issue_url": "https://github.com/elastic/elasticsearch/issues/62996",
          "issue_title": "[CI] TransformIndexerStateTests.testStopAtCheckpoint",
          "issue_number": 62996,
          "issue_text": "<!--\\r\\nPlease fill out the following information, and ensure you have attempted\\r\\nto reproduce locally\\r\\n-->\\r\\n\\r\\n**Build scan**:\\r\\n\\r\\nhttps://gradle-enterprise.elastic.co/s/yatg7ybt2srg6\\r\\n\\r\\n**Repro line**:\\r\\n\\r\\n```\\r\\n./gradlew ':x-pack:plugin:transform:test' --tests \"org.elasticsearch.xpack.transform.transforms.TransformIndexerStateTests.testStopAtCheckpoint\" \\\\\\r\\n  -Dtests.seed=A036379B9D4C4E02 \\\\\\r\\n  -Dtests.security.manager=true \\\\\\r\\n  -Dtests.locale=vi \\\\\\r\\n  -Dtests.timezone=Etc/GMT-12 \\\\\\r\\n  -Druntime.java=11\\r\\n```\\r\\n\\r\\n**Reproduces locally?**:\\r\\n\\r\\nYes\\r\\n\\r\\n**Applicable branches**:\\r\\n\\r\\n`master`\\r\\n\\r\\n**Failure history**:\\r\\n<!--\\r\\nLink to build stats and possible indication of when this started failing and how often it fails\\r\\n<https://build-stats.elastic.co/app/kibana>\\r\\n-->\\r\\n**Failure excerpt**:\\r\\n\\r\\n```\\r\\norg.elasticsearch.xpack.transform.transforms.TransformIndexerStateTests > testStopAtCheckpoint FAILED\\r\\n    java.lang.AssertionError:\\r\\n    Expected: a value equal to or greater than <1>\\r\\n         but: <0> was less than <1>\\r\\n        at __randomizedtesting.SeedInfo.seed([A036379B9D4C4E02:A6AC16D3A43F3513]:0)\\r\\n        at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:18)\\r\\n        at org.junit.Assert.assertThat(Assert.java:956)\\r\\n        at org.junit.Assert.assertThat(Assert.java:923)\\r\\n        at org.elasticsearch.xpack.transform.transforms.TransformIndexerStateTests.testStopAtCheckpoint(TransformIndexerStateTests.java:439)\\r\\n```\\r\\n",
          "issue_comments": [
            {
              "comment_username": "elasticmachine",
              "comment_create_time": "2020-09-29T10:23:52Z",
              "comment_edit_time": "2020-09-29T10:23:52Z",
              "comment_text": "Pinging @elastic/ml-core (:ml/Transform)"
            }
          ]
        },
        {
          "issue_url": "https://github.com/elastic/elasticsearch/issues/62995",
          "issue_title": "InternalClusterInfoService should not ignore hidden indices",
          "issue_number": 62995,
          "issue_text": "Today `InternalClusterInfoService` ignores hidden indices when retrieving shard stats of the cluster. This can lead to suboptimal shard allocation decisions as the size of shards are taken into account when allocating new shards or rebalancing existing shards.",
          "issue_comments": [
            {
              "comment_username": "elasticmachine",
              "comment_create_time": "2020-09-29T10:14:13Z",
              "comment_edit_time": "2020-09-29T10:14:13Z",
              "comment_text": "Pinging @elastic/es-distributed (:Distributed/Allocation)"
            },
            {
              "comment_username": "tlrx",
              "comment_create_time": "2020-09-29T10:15:47Z",
              "comment_edit_time": "2020-09-29T10:15:47Z",
              "comment_text": "Note that #52423 moved ML indices to be `hidden`"
            },
            {
              "comment_username": "tlrx",
              "comment_create_time": "2020-09-29T14:00:09Z",
              "comment_edit_time": "2020-09-29T14:01:56Z",
              "comment_text": "> I wonder if there is a similar issue for system indices? Or are those captured by this change?\r\n\r\n@henningandersen System indices should be correctly resolved using the previous indices options (as long as they are not hidden too), but just in case I pushed https://github.com/elastic/elasticsearch/pull/62995/commits/99b262fb03ca6bb93aff05d7493445693a413b4d so that the test is also randomly executed on a system index. Let me know what you think please"
            },
            {
              "comment_username": "tlrx",
              "comment_create_time": "2020-09-29T15:05:40Z",
              "comment_edit_time": "2020-09-29T15:05:40Z",
              "comment_text": "@elasticmachine update branch"
            }
          ]
        },
        {
          "issue_url": "https://github.com/elastic/elasticsearch/issues/62992",
          "issue_title": "Convert dense/sparse vector field mappers to Parametrized form",
          "issue_number": 62992,
          "issue_text": "Also adds a proper MapperTestCase test for dense vectors.\r\n\r\nRelates to #62988 ",
          "issue_comments": [
            {
              "comment_username": "elasticmachine",
              "comment_create_time": "2020-09-29T09:35:36Z",
              "comment_edit_time": "2020-09-29T09:35:36Z",
              "comment_text": "Pinging @elastic/es-search (:Search/Mapping)"
            },
            {
              "comment_username": "romseygeek",
              "comment_create_time": "2020-09-29T14:30:10Z",
              "comment_edit_time": "2020-09-29T14:30:10Z",
              "comment_text": "@elasticmachine run elasticsearch-ci/2 (failure in ML, doesn't look related)"
            }
          ]
        },
        {
          "issue_url": "https://github.com/elastic/elasticsearch/issues/62988",
          "issue_title": "Convert remaining FieldMappers to parametrized form",
          "issue_number": 62988,
          "issue_text": "The following field mappers still remain to be converted:\r\n\r\n- [ ] AbstractGeometryFieldMapper\r\n- [ ] AbstractPointGeometryFieldMapper\r\n- [ ] AbstractShapeGeometryFieldMapper\r\n- [ ] AnnotatedTextFieldMapper\r\n- [ ] DenseVectorFieldMapper #62992 \r\n- [ ] ExternalMapper\r\n- [ ] FakeFieldMapper in DocumentFieldMapperTests\r\n- [ ] FakeStringFieldMapper\r\n- [ ] FlatObjectFieldMapper\r\n- [x] HistogramFieldMapper #63004\r\n- [ ] ICUCollationKeywordFieldMapper\r\n- [ ] MetaJoinFieldMapper\r\n- [ ] MockFieldMapper\r\n- [x] Murmur3FieldMapper #63004\r\n- [ ] ParentIdFieldMapper\r\n- [ ] ParentJoinFieldMapper\r\n- [x] PercolatorFieldMapper #63004\r\n- [ ] PhraseFieldMapper in TextFieldMapper\r\n- [ ] PrefixFieldMapper in SearchAsYouTypeFieldMapper #62938\r\n- [ ] PrefixFieldMapper in TextFieldMapper\r\n- [ ] RankFeatureFieldMapper #62938\r\n- [ ] RankFeaturesFieldMapper #62938\r\n- [ ] SearchAsYouTypeFieldMapper #62938\r\n- [ ] ShingleFieldMapper in SearchAsYouTypeFieldMapper #62938\r\n- [ ] SparseVectorFieldMapper #62992 \r\n- [ ] TextFieldMapper\r\n- [ ] TokenCountFieldMapper #62938\r\n- [ ] WildcardFieldMapper",
          "issue_comments": [
            {
              "comment_username": "elasticmachine",
              "comment_create_time": "2020-09-29T08:50:29Z",
              "comment_edit_time": "2020-09-29T08:50:29Z",
              "comment_text": "Pinging @elastic/es-search (:Search/Mapping)"
            }
          ]
        },
        {
          "issue_url": "https://github.com/elastic/elasticsearch/issues/62986",
          "issue_title": "EQL: `allow_no_indices` behavior",
          "issue_number": 62986,
          "issue_text": "Currently EQL requires at least one index to match in order to validate the query. However just like [search](https://www.elastic.co/guide/en/elasticsearch/reference/7.9/search-search.html#search-search-api-query-params), the `allow_no_indices` option is permitted which leads to confusing behavior since it can be true, yet EQL will throw an exception:\r\n```\r\n### A \r\n\r\nGET foo*/_search\r\n# success\r\n\r\nGET foo*/_eql/search\r\n{\r\n  \"size\": 0,\r\n  \"query\": \"process where 1 == 1\"\r\n}\r\n# \"type\" : \"index_not_found_exception\",\r\n# \"reason\" : \"no such index [foo*]\",\r\n\r\n\r\n### B\r\n\r\nGET foo/_search?ignore_unavailable=true\r\n# success\r\n\r\nGET foo/_eql/search?ignore_unavailable=true\r\n{\r\n  \"size\": 0,\r\n  \"query\": \"process where 1 == 1\"\r\n}\r\n# \"type\" : \"index_not_found_exception\",\r\n# \"reason\" : \"no such index [[foo]]\",\r\n\r\n\r\n### C \r\n\r\nGET foo/_search\r\n# \"type\" : \"index_not_found_exception\",\r\n# \"reason\" : \"no such index [foo]\",\r\n\r\nGET foo/_eql/search\r\n{\r\n  \"size\": 0,\r\n  \"query\": \"process where 1 == 1\"\r\n}\r\n# \"type\" : \"mapping_exception\",\r\n# \"reason\" : \"Unknown index [foo]\",\r\n\r\n### D \r\n\r\nGET foo*/_search?allow_no_indices=false\r\n# \"type\" : \"index_not_found_exception\",\r\n# \"reason\" : \"no such index [foo*]\",\r\n\r\nGET foo*/_eql/search?allow_no_indices=false\r\n{\r\n  \"size\": 0,\r\n  \"query\": \"process where 1 == 1\"\r\n}\r\n# \"type\" : \"index_not_found_exception\",\r\n# \"reason\" : \"no such index [foo*]\",\r\n\r\nGET foo*/_eql/search?allow_no_indices=true\r\n{\r\n  \"size\": 0,\r\n  \"query\": \"process where 1 == 1\"\r\n}\r\n# \"type\" : \"mapping_exception\",\r\n# \"reason\" : \"Unknown index [*,-*]\"\r\n```\r\n\r\nConsidering the similarities to the search API, EQL should either provide a similar behavior or not accept this parameter.",
          "issue_comments": [
            {
              "comment_username": "elasticmachine",
              "comment_create_time": "2020-09-29T07:09:10Z",
              "comment_edit_time": "2020-09-29T07:09:10Z",
              "comment_text": "Pinging @elastic/es-ql (:Query Languages/EQL)"
            },
            {
              "comment_username": "costin",
              "comment_create_time": "2020-09-29T07:09:40Z",
              "comment_edit_time": "2020-09-29T07:09:40Z",
              "comment_text": "/cc @rylnd "
            },
            {
              "comment_username": "rylnd",
              "comment_create_time": "2020-09-29T15:33:51Z",
              "comment_edit_time": "2020-09-29T15:33:51Z",
              "comment_text": "@costin in addition to the above discrepancies, I wanted to note that `allow_no_indices` does provide behavior on which EQL rules currently depend: it overrides the default behavior of a query with multiple indexes failing if **any** of them do not resolve.\r\n\r\n```js\r\nGET auditbeat-*,foo*/_eql/search\r\n// index_not_found_exception: no such index [foo*]\r\n\r\nGET auditbeat-*,foo*/_eql/search?allow_no_indices=true\r\n// success\r\n```\r\n\r\nSince a default detection rule will be populated with security solution's default indices (some of which may or may not exist/be visible for the rule writer), and since the default behavior for an ES search is to ignore these unmatched patterns, the ability to replicate this behavior in EQL would be ideal."
            }
          ]
        },
        {
          "issue_url": "https://github.com/elastic/elasticsearch/issues/62985",
          "issue_title": "Introduce yamlRestCompatTest task and plugin. ",
          "issue_number": 62985,
          "issue_text": "This commit introduces a plugin to run REST compatibility tests.\r\nThe plugin is not currently applied to any projects with this commit, \r\nbut it establishes the foundation for testing REST compatibility. \r\n\r\nThis plugin will be applied to all core, plugin, and module projects \r\nthat currently have YAML based REST tests. This plugin, when applied\r\nwill do the following:\r\n\r\n* Create a new sourceset that extends from the yamlRestTest sourceset\r\n* Ensure the default distribution is used to execute the tests\r\n* Checkout bwc:minor version of the source code\r\n* Copy the rest api and rest tests from bwc:minor to the new sourceset\r\n* Create the test task, but manipulate the classpath so it uses the \r\n  \"normal\" YAML test runner, but the \"compat\" set of tests\r\n* wire up dependencies, ide, and check\r\n\r\nThe implementation here depends on :distribution:bwc:minor:checkoutBwcBranch\r\nwhich is a convenient way to git checkout the most recent prior branch. \r\nThis approach works but is a bit fragile and long term would like a more \r\nrobust way to checkout arbitrary prior branches. This would be a fairly large\r\neffort and is not included in this commit. \r\n\r\nAlso not included in this commit is the code necessary to inject the\r\ncompatible header, any customization to running the compatibility tests, \r\nor any of the logic to override specific tests. Future commits will address\r\nthose concerns. \r\n\r\n",
          "issue_comments": [
            {
              "comment_username": "elasticmachine",
              "comment_create_time": "2020-09-29T13:42:30Z",
              "comment_edit_time": "2020-09-29T13:42:30Z",
              "comment_text": "Pinging @elastic/es-core-infra (:Core/Infra/Build)"
            }
          ]
        },
        {
          "issue_url": "https://github.com/elastic/elasticsearch/issues/62984",
          "issue_title": "System index auto-creation should not be disabled by user settings",
          "issue_number": 62984,
          "issue_text": "By default, Elasticsearch auto-creates indices when a document is submitted to a non-existent index. There is a setting that allows users to disable this behavior. However, this setting should not apply to system indices, so that Elasticsearch modules and plugins are able to use auto-create behavior whether or not it is exposed to users.\r\n\r\nThis commit constructs the AutoCreateIndex object with a reference to the SystemIndices object so that we bypass the check for the user-facing autocreate setting when it's a system index that is being autocreated.\r\n\r\nWe also modify the logic in TransportBulkAction to make sure that if a system index is included in a bulk request, we don't skip the autocreation step.\r\n\r\nFixes #61642",
          "issue_comments": [
            {
              "comment_username": "elasticmachine",
              "comment_create_time": "2020-09-28T21:13:02Z",
              "comment_edit_time": "2020-09-28T21:13:02Z",
              "comment_text": "Pinging @elastic/es-core-infra (:Core/Infra/Core)"
            }
          ]
        },
        {
          "issue_url": "https://github.com/elastic/elasticsearch/issues/62983",
          "issue_title": "Make Rounding.nextRoundingValue consistent",
          "issue_number": 62983,
          "issue_text": "\"interval\" style roundings were implementing `nextRoundingValue` in a\\r\\nfairly inconsistent way - it'd produce a value, but sometimes that\\r\\nvalue would be the same as the previous rounding value. This makes it\\r\\nconsistently the next value that `rounding` would make.\\r\\n",
          "issue_comments": [
            {
              "comment_username": "elasticmachine",
              "comment_create_time": "2020-09-29T15:08:12Z",
              "comment_edit_time": "2020-09-29T15:08:12Z",
              "comment_text": "Pinging @elastic/es-analytics-geo (:Analytics/Aggregations)"
            }
          ]
        },
        {
          "issue_url": "https://github.com/elastic/elasticsearch/issues/62974",
          "issue_title": "Move FieldMapper#valueFetcher to MappedFieldType",
          "issue_number": 62974,
          "issue_text": "For runtime fields, we will want to do all search-time interaction with\r\na field definition via a MappedFieldType, rather than a FieldMapper, to\r\navoid interfering with the logic of document parsing.  Currently, fetching\r\nvalues for runtime scripts and for building top hits responses need to\r\ncall a method on FieldMapper.  This commit moves this method to\r\nMappedFieldType, incidentally simplifying the current call sites and freeing\r\nus up to implement runtime fields as pure MappedFieldType objects.",
          "issue_comments": [
            {
              "comment_username": "elasticmachine",
              "comment_create_time": "2020-09-28T17:41:40Z",
              "comment_edit_time": "2020-09-28T17:41:40Z",
              "comment_text": "Pinging @elastic/es-search (:Search/Mapping)"
            },
            {
              "comment_username": "romseygeek",
              "comment_create_time": "2020-09-28T17:43:14Z",
              "comment_edit_time": "2020-09-28T17:43:14Z",
              "comment_text": "Some more refactoring from me :). We are getting some fairly unwieldy telescoping MappedFieldType constructors now, and there are also quite a few places where we end up sharing object parsing logic between the FieldMapper and the MappedFieldType which I think can be cleaned up, but I wanted to keep this PR as simple as possible and so this is just doing the absolute basics.  Further improvements will follow."
            }
          ]
        },
        {
          "issue_url": "https://github.com/elastic/elasticsearch/issues/62973",
          "issue_title": "Support IBM Cloud Object Storage for S3 snapshots",
          "issue_number": 62973,
          "issue_text": "[IBM Cloud Object Storage](https://cloud.ibm.com/docs/cloud-object-storage?topic=cloud-object-storage-compatibility-api) supports a subset of the S3 API.  \r\n\r\nEven if it does work today, there are a few risks\r\n- In the future,  we might change something in Elasticsearch that requires a part of the S3 API that IBM's version doesn't support\r\n- There might be edge cases that will break the snapshot/restore implementation today\r\n\r\nThis is a request for us to start officially testing against IBM Cloud Object Storage like we do for S3 and Minio so we can make it an officially supported storage option for our S3 snapshot/restore functionality.",
          "issue_comments": [
            {
              "comment_username": "elasticmachine",
              "comment_create_time": "2020-09-28T17:17:05Z",
              "comment_edit_time": "2020-09-28T17:17:05Z",
              "comment_text": "Pinging @elastic/es-distributed (:Distributed/Snapshot/Restore)"
            }
          ]
        },
        {
          "issue_url": "https://github.com/elastic/elasticsearch/issues/62968",
          "issue_title": "Use services for archive and file operations in tasks",
          "issue_number": 62968,
          "issue_text": "- Referencing a project instance during task execution is discouraged by\r\nGradle and should be avoided. E.g. It is incompatible with Gradles\r\nincubating configuration cache. Instead there are services available to handle\r\narchive and filesystem operations in task actions.\r\n\r\n- Brings us one step closer to #57918",
          "issue_comments": [
            {
              "comment_username": "elasticmachine",
              "comment_create_time": "2020-09-29T07:46:27Z",
              "comment_edit_time": "2020-09-29T07:46:27Z",
              "comment_text": "Pinging @elastic/es-core-infra (:Core/Infra/Build)"
            }
          ]
        },
        {
          "issue_url": "https://github.com/elastic/elasticsearch/issues/62966",
          "issue_title": "[7.x] [ML] Set parent task Id on ml expired data removers (#62854)",
          "issue_number": 62966,
          "issue_text": "Backports the following commits to 7.x:\n - [ML] Set parent task Id on ml expired data removers (#62854)",
          "issue_comments": [
            {
              "comment_username": "elasticmachine",
              "comment_create_time": "2020-09-28T16:36:45Z",
              "comment_edit_time": "2020-09-28T16:36:45Z",
              "comment_text": "Pinging @elastic/ml-core (:ml)"
            }
          ]
        },
        {
          "issue_url": "https://github.com/elastic/elasticsearch/issues/62960",
          "issue_title": "[ML] Delete dest index and reindex if incompatible",
          "issue_number": 62960,
          "issue_text": "Data frame analytics results format changed in version `7.10.0`.\r\nIf existing jobs that were not completed are restarted, it is\r\npossible the destination index had already been created. That index's\r\nmappings are not suitable for the new results format.\r\n\r\nThis commit checks the version of the destination index and deletes\r\nit when the version is outdated. The job will then continue by\r\nrecreating the destination index and reindexing.\r\n\r\nRelates https://github.com/elastic/elasticsearch/issues/61325",
          "issue_comments": [
            {
              "comment_username": "elasticmachine",
              "comment_create_time": "2020-09-28T14:24:46Z",
              "comment_edit_time": "2020-09-28T14:24:46Z",
              "comment_text": "Pinging @elastic/ml-core (:ml)"
            },
            {
              "comment_username": "dimitris-athanasiou",
              "comment_create_time": "2020-09-28T14:28:46Z",
              "comment_edit_time": "2020-09-28T14:28:46Z",
              "comment_text": "run elasticsearch-ci/packaging-sample-windows"
            },
            {
              "comment_username": "dimitris-athanasiou",
              "comment_create_time": "2020-09-29T10:23:34Z",
              "comment_edit_time": "2020-09-29T10:23:34Z",
              "comment_text": "@benwtrent @przemekwitek  I had to rework this quite a bit as the tests caught that I was deleting custom dest indices the user could have created (which might not contain `_meta` at all). Could you please take another look? I think I cleaned up the code a bit more too in the process."
            },
            {
              "comment_username": "dimitris-athanasiou",
              "comment_create_time": "2020-09-29T13:30:47Z",
              "comment_edit_time": "2020-09-29T13:30:47Z",
              "comment_text": "run packaging-sample-windows"
            },
            {
              "comment_username": "dimitris-athanasiou",
              "comment_create_time": "2020-09-29T14:38:59Z",
              "comment_edit_time": "2020-09-29T14:38:59Z",
              "comment_text": "run elasticsearch-ci/packaging-sample-windows"
            }
          ]
        },
        {
          "issue_url": "https://github.com/elastic/elasticsearch/issues/62959",
          "issue_title": "[ILM] Reflecting where data lives in the ILM policy UI",
          "issue_number": 62959,
          "issue_text": "<!--\r\nPlease first search existing issues for the feature you are requesting;\r\nit may already exist, even as a closed issue.\r\n-->\r\n\r\n<!--\r\nDescribe the feature.\r\n\r\nPlease give us as much context as possible about the feature. For example,\r\nyou could include a story about a time when you wanted to use the feature,\r\nand also tell us what you had to do instead. The last part is helpful\r\nbecause it gives us an idea of how much harder your life is without the\r\nfeature.\r\n\r\n-->\r\n\r\n## Summary\r\n\r\nWith the introduction of data tiers the ILM policy UI is being redesigned.The new UI will visually map a phase to a data tier. In many scenarios, the mapping will be 1:1. Hot phase will send data to the hot tier, warm to warm and so on. This is the default behaviour of ILM. However, there are a number of variables that can affect where data is ultimately moved in a phase, we want the UI to keep in step with the runtime behaviour of ILM.\r\n\r\n## Background\r\n\r\nIt is possible that a cluster may not have a data tier available in which case ILM needs to allocate data somewhere else. ILM has “tier preferences” that act as a fallback to prevent ILM from getting stuck when a data tier does not exist in the cluster. This is generated at runtime and is a function of:\r\n\r\n1. Behaviour built into ILM (like tier preferences)\r\n2. ILM Policy settings\r\n3. Index level settings (tier preferences can be overridden in index settings)\r\n\r\nUsers can also set node attributes to determine where data is going to be stored which provide an entirely different set of behaviours regarding migration of data as it moves through different phases.\r\n\r\n## Solution\r\n\r\nThe ILM policy configuration UI can use a yet-to-be-designed simulate API. This API will describe all of the actions performed by a policy, per phase. Most importantly, the \"migrate\" action. In this way, the UI can consult all of the migration actions to determine which tiers (and nodes) a policy will assign data to given a policy and current cluster state.\r\n\r\nA benefit here is that Kibana will be as uncoupled from ES implementation details as possible. The alternate solution is that the Kibana ILM plugin mimics what ILM would do to show a user where data is going to be moved which will involve capturing the behaviour of tier allocation and node attribute based allocation.\r\n\r\n### Proposed design of API\r\n\r\nRequest:\r\n\r\n```jsonc\r\nPUT _ilm/policy/simulate\r\n{\r\n  policy: { ... } // the policy that could submitted to the \"PUT _ilm/policy/<policy_id>\" endpoint\r\n  index: {...} // optionally send index object so that index-specific settings can be considered\r\n}\r\n```\r\n\r\nResponse:\r\n\r\n```jsonc\r\n{\r\n  // hot will always exist so:\r\n  hot: {\r\n    actions: [ { [action_name]: { ...action_specific_result } ] // A sequential array of any actions taken in this phase\r\n  },\r\n  // add warm if warm is in policy\r\n  warm: {\r\n    actions: [ { [action_name]: { ...action_specific_result } ] // A sequential array of any actions taken in this phase\r\n  },\r\n  ...\r\n}\r\n```\r\n\r\nEach ILM action response will need to designed individually, but the migration action will be the most useful for the UI. For instance, the actions result of a migration action might look something like:\r\n\r\n```jsonc\r\n{ migrate: { from: { nodes: [nodeIds], tier: \"hot\" }, to: { nodes: [nodeIds], tier: \"warm\" } } }\r\n```\r\n\r\nand if no data tiers exist at \"from\" nodes, but do exist at \"to\" nodes:\r\n\r\n```jsonc\r\n{ migrate: { from: { nodes: [nodeIds] }, to: { nodes: [nodeIds], tier: \"cold\" } } }\r\n```\r\n\r\n## Additional information\r\n\r\n* This endpoint should be able to report data migrations without reference data tiers because certain policy configurations might user node attrs to allocate data, or disable the data migration step entirely.\r\n\r\nCC @cjcenizal @dakrone @andreidan \r\n",
          "issue_comments": [
            {
              "comment_username": "elasticmachine",
              "comment_create_time": "2020-09-28T14:37:47Z",
              "comment_edit_time": "2020-09-28T14:37:47Z",
              "comment_text": "Pinging @elastic/es-core-features (:Core/Features/ILM+SLM)"
            }
          ]
        },
        {
          "issue_url": "https://github.com/elastic/elasticsearch/issues/62956",
          "issue_title": "Open API to allow creating a SearchContext from an existing ShardSearchContextId",
          "issue_number": 62956,
          "issue_text": "At the moment in _master_, it is not possible to create a `SearchContext` from an existing `ShardSearchContextId`. The methods required to do so are both `private` (see below). This would allow to leverage the [new reader context logic](https://github.com/elastic/elasticsearch/commit/879279c9b46b5a9606dfca96075e005624f0785d) within the [Federate plugin](https://siren.io/federate/) of siren.\r\nTherefore, both methods below should be `public` in order to allow this:\r\n\r\nhttps://github.com/elastic/elasticsearch/blob/860997b08c8452bee70a8d3ca63e977c1639bd29/server/src/main/java/org/elasticsearch/search/SearchService.java#L614\r\n\r\nhttps://github.com/elastic/elasticsearch/blob/860997b08c8452bee70a8d3ca63e977c1639bd29/server/src/main/java/org/elasticsearch/search/SearchService.java#L720",
          "issue_comments": []
        }
      ]
    }
  },
  {
    "github_url": "https://github.com/isucon/isucon5-qualify",
    "github_info": {
      "name": "isucon/isucon5-qualify",
      "language": "Java"
    },
    "github_branches": {
      "branch_datas": [
        {
          "branch_version": "master",
          "branch_url": "https://github.com/isucon/isucon5-qualify/tree/master",
          "branch_download_url": "https://github.com/isucon/isucon5-qualify/archive/master.zip"
        }
      ]
    },
    "github_pull_requests": { "pull_datas": [] },
    "github_issues": { "issue_datas": [] }
  },
  {
    "github_url": "https://github.com/floodlight/floodlight",
    "github_info": {
      "name": "floodlight/floodlight",
      "language": "Java"
    },
    "github_branches": {
      "branch_datas": [
        {
          "branch_version": "fix-compile",
          "branch_url": "https://github.com/floodlight/floodlight/tree/fix-compile",
          "branch_download_url": "https://github.com/floodlight/floodlight/archive/fix-compile.zip"
        },
        {
          "branch_version": "gh-pages",
          "branch_url": "https://github.com/floodlight/floodlight/tree/gh-pages",
          "branch_download_url": "https://github.com/floodlight/floodlight/archive/gh-pages.zip"
        },
        {
          "branch_version": "hot-failover",
          "branch_url": "https://github.com/floodlight/floodlight/tree/hot-failover",
          "branch_download_url": "https://github.com/floodlight/floodlight/archive/hot-failover.zip"
        },
        {
          "branch_version": "master-green",
          "branch_url": "https://github.com/floodlight/floodlight/tree/master-green",
          "branch_download_url": "https://github.com/floodlight/floodlight/archive/master-green.zip"
        },
        {
          "branch_version": "master-prejava8",
          "branch_url": "https://github.com/floodlight/floodlight/tree/master-prejava8",
          "branch_download_url": "https://github.com/floodlight/floodlight/archive/master-prejava8.zip"
        },
        {
          "branch_version": "master",
          "branch_url": "https://github.com/floodlight/floodlight/tree/master",
          "branch_download_url": "https://github.com/floodlight/floodlight/archive/master.zip"
        },
        {
          "branch_version": "perf-test",
          "branch_url": "https://github.com/floodlight/floodlight/tree/perf-test",
          "branch_download_url": "https://github.com/floodlight/floodlight/archive/perf-test.zip"
        },
        {
          "branch_version": "release.asplus.bvs",
          "branch_url": "https://github.com/floodlight/floodlight/tree/release.asplus.bvs",
          "branch_download_url": "https://github.com/floodlight/floodlight/archive/release.asplus.bvs.zip"
        },
        {
          "branch_version": "release.asplus",
          "branch_url": "https://github.com/floodlight/floodlight/tree/release.asplus",
          "branch_download_url": "https://github.com/floodlight/floodlight/archive/release.asplus.zip"
        },
        {
          "branch_version": "revert-555-master",
          "branch_url": "https://github.com/floodlight/floodlight/tree/revert-555-master",
          "branch_download_url": "https://github.com/floodlight/floodlight/archive/revert-555-master.zip"
        },
        {
          "branch_version": "revert-559-revert-555-master",
          "branch_url": "https://github.com/floodlight/floodlight/tree/revert-559-revert-555-master",
          "branch_download_url": "https://github.com/floodlight/floodlight/archive/revert-559-revert-555-master.zip"
        },
        {
          "branch_version": "revert-594-master",
          "branch_url": "https://github.com/floodlight/floodlight/tree/revert-594-master",
          "branch_download_url": "https://github.com/floodlight/floodlight/archive/revert-594-master.zip"
        },
        {
          "branch_version": "v0.8",
          "branch_url": "https://github.com/floodlight/floodlight/tree/v0.8",
          "branch_download_url": "https://github.com/floodlight/floodlight/archive/v0.8.zip"
        },
        {
          "branch_version": "v0.82",
          "branch_url": "https://github.com/floodlight/floodlight/tree/v0.82",
          "branch_download_url": "https://github.com/floodlight/floodlight/archive/v0.82.zip"
        },
        {
          "branch_version": "v0.85",
          "branch_url": "https://github.com/floodlight/floodlight/tree/v0.85",
          "branch_download_url": "https://github.com/floodlight/floodlight/archive/v0.85.zip"
        },
        {
          "branch_version": "v0.90",
          "branch_url": "https://github.com/floodlight/floodlight/tree/v0.90",
          "branch_download_url": "https://github.com/floodlight/floodlight/archive/v0.90.zip"
        },
        {
          "branch_version": "v0.91",
          "branch_url": "https://github.com/floodlight/floodlight/tree/v0.91",
          "branch_download_url": "https://github.com/floodlight/floodlight/archive/v0.91.zip"
        },
        {
          "branch_version": "v1.0",
          "branch_url": "https://github.com/floodlight/floodlight/tree/v1.0",
          "branch_download_url": "https://github.com/floodlight/floodlight/archive/v1.0.zip"
        },
        {
          "branch_version": "v1.1",
          "branch_url": "https://github.com/floodlight/floodlight/tree/v1.1",
          "branch_download_url": "https://github.com/floodlight/floodlight/archive/v1.1.zip"
        },
        {
          "branch_version": "v1.2",
          "branch_url": "https://github.com/floodlight/floodlight/tree/v1.2",
          "branch_download_url": "https://github.com/floodlight/floodlight/archive/v1.2.zip"
        },
        {
          "branch_version": "wallaby",
          "branch_url": "https://github.com/floodlight/floodlight/tree/wallaby",
          "branch_download_url": "https://github.com/floodlight/floodlight/archive/wallaby.zip"
        },
        {
          "branch_version": "xenon",
          "branch_url": "https://github.com/floodlight/floodlight/tree/xenon",
          "branch_download_url": "https://github.com/floodlight/floodlight/archive/xenon.zip"
        },
        {
          "branch_version": "yeti-st",
          "branch_url": "https://github.com/floodlight/floodlight/tree/yeti-st",
          "branch_download_url": "https://github.com/floodlight/floodlight/archive/yeti-st.zip"
        },
        {
          "branch_version": "yeti",
          "branch_url": "https://github.com/floodlight/floodlight/tree/yeti",
          "branch_download_url": "https://github.com/floodlight/floodlight/archive/yeti.zip"
        }
      ]
    },
    "github_pull_requests": {
      "pull_datas": [
        {
          "pull_number": 842,
          "pull_title": "VLAN supported OFPacketOut for DHCP server",
          "pull_version": "floodlight:master",
          "pull_version_url": "https://github.com/floodlight/floodlight/tree/master"
        },
        {
          "pull_number": 837,
          "pull_title": "[SECURITY] Use HTTPS to resolve dependencies in Maven Build",
          "pull_version": "floodlight:master",
          "pull_version_url": "https://github.com/floodlight/floodlight/tree/master"
        },
        {
          "pull_number": 827,
          "pull_title": "Support building Eclipse project on Ant Windows",
          "pull_version": "floodlight:master",
          "pull_version_url": "https://github.com/floodlight/floodlight/tree/master"
        },
        {
          "pull_number": 799,
          "pull_title": "Add Generic Support for Multicasting",
          "pull_version": "floodlight:master",
          "pull_version_url": "https://github.com/floodlight/floodlight/tree/master"
        },
        {
          "pull_number": 759,
          "pull_title": "Fixed bug at dropFilter(...) method.",
          "pull_version": "floodlight:master",
          "pull_version_url": "https://github.com/floodlight/floodlight/tree/master"
        },
        {
          "pull_number": 758,
          "pull_title": "Java 8 Streams for OFChannelHandler.ChannelRead0",
          "pull_version": "floodlight:master",
          "pull_version_url": "https://github.com/floodlight/floodlight/tree/master"
        },
        {
          "pull_number": 749,
          "pull_title": "Integration of new EW interface based on communication mode called CIDC for multiple controller deployment",
          "pull_version": "floodlight:master",
          "pull_version_url": "https://github.com/floodlight/floodlight/tree/master"
        },
        {
          "pull_number": 639,
          "pull_title": "Fix compatibility with Mikrotik an OF1.0",
          "pull_version": "floodlight:master",
          "pull_version_url": "https://github.com/floodlight/floodlight/tree/master"
        },
        {
          "pull_number": 632,
          "pull_title": "Fixing squid:S1596 -\\xa0 Collections.emptyList() should be used instead of ",
          "pull_version": "floodlight:master",
          "pull_version_url": "https://github.com/floodlight/floodlight/tree/master"
        }
      ]
    },
    "github_issues": {
      "issue_datas": [
        {
          "issue_url": "https://github.com/floodlight/floodlight/issues/847",
          "issue_title": "Listening for openflow messages",
          "issue_number": 847,
          "issue_text": "",
          "issue_comments": [
            {
              "comment_username": "hnm6500",
              "comment_create_time": "2020-08-20T15:31:46Z",
              "comment_edit_time": "2020-08-20T15:34:24Z",
              "comment_text": "I am using project Floodlight to understand the software defined networking for my personal project. My custom module uses floodlightProvider controller module and IOFMessageListener to listen for openflow messages. I can successfully listen to PACKET_IN openflow messages, however I am not able to listen to any synchronous openflow messages(HELLO, EQHO_REQUEST, etc). I followed the documentation and if I am not mistaken, openflow messages can be listened by using the floodlightProvider controller module. Can anyone assist me as I am very curious. The program for listening for messages is given on floodlight tutorial under \"how to write a module\" . However, I am editing the startup method and the receive method. Those are as follows:\r\n\r\n@Override\r\npublic void startUp(FloodlightModuleContext context) {\r\n    floodlightProvider.addOFMessageListener(OFType.PACKET_IN, this);\r\n    floodlightProvider.addOFMessageListener(OFType.HELLO, this);\r\n}\r\n\r\n@Override\r\n   public net.floodlightcontroller.core.IListener.Command receive(IOFSwitch sw, OFMessage msg, FloodlightContext cntx) {\r\n        switch(msg.getType()){\r\n            case PACKET_IN:\r\n             // print the openflow message in console\r\n                break;\r\n            case HELLO:\r\n            // print the openflow message in console\r\n               break;\r\n            default:\r\n               break;\r\n}\r\n        return Command.CONTINUE;\r\n    }"
            }
          ]
        },
        {
          "issue_url": "https://github.com/floodlight/floodlight/issues/846",
          "issue_title": "link bandwidth limit",
          "issue_number": 846,
          "issue_text": "Does anyone know how to use floodlight to limit link bandwidth?\r\n",
          "issue_comments": []
        },
        {
          "issue_url": "https://github.com/floodlight/floodlight/issues/845",
          "issue_title": "ant fail when build the project",
          "issue_number": 845,
          "issue_text": "*Env\r\nMy system is: ubuntu 16.04LTS on vmware\r\nmy jdk version is: 8u251\r\n\r\n*And I try to follow the steps on the https://floodlight.atlassian.net/wiki/spaces/floodlightcontroller/pages/1343544/Installation+Guide\r\n\r\n*Everything goes well except ant.\r\n*When I try to do this, I got the following message\r\n    [javac]   Symbol:   class AsyncProcessFunction\r\n    [javac]   location: package org.apache.thrift\r\n    [javac] /home/jj/floodlight/lib/gen-java/net/floodlightcontroller/packetstreamer/thrift/PacketStreamer.java:438: error: symbol can't found\r\n    [javac]     private static <I extends AsyncIface> java.util.Map<java.lang.String,  org.apache.thrift.AsyncProcessFunction<I, ? extends  org.apache.thrift.TBase,?>> getProcessMap(java.util.Map<java.lang.String,  org.apache.thrift.AsyncProcessFunction<I, ? extends  org.apache.thrift.TBase, ?>> processMap) {\r\n    [javac]                                                                     \r\n(ps:here are only part of the error message, and I try to translate it into English)\r\n\r\nIs there any suggestions?\r\nThank you for any help!!\r\n\r\n",
          "issue_comments": [
            {
              "comment_username": "wilton-tang",
              "comment_create_time": "2020-08-19T09:20:02Z",
              "comment_edit_time": "2020-08-19T09:20:02Z",
              "comment_text": "did you solve the problem? i have the same problem with you ."
            },
            {
              "comment_username": "PercolateJj",
              "comment_create_time": "2020-08-19T11:13:46Z",
              "comment_edit_time": "2020-08-19T11:13:46Z",
              "comment_text": "\nSry, I give up solving it. But v0.90 can work, although still doesn’t work on eclipse.\n\n> 在 2020年8月19日，17:20，wilton-tang <notifications@github.com> 写道：\n> \n> ﻿\n> did you solve the problem? i have the same problem with you .\n> \n> —\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub, or unsubscribe.\n"
            },
            {
              "comment_username": "wilton-tang",
              "comment_create_time": "2020-08-20T02:58:54Z",
              "comment_edit_time": "2020-08-20T02:58:54Z",
              "comment_text": "> \r\n> Sry, I give up solving it. But v0.90 can work, although still doesn’t work on eclipse.\r\n> […](#)\r\n> 在 2020年8月19日，17:20，wilton-tang ***@***.***> 写道： ﻿ did you solve the problem? i have the same problem with you . — You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub, or unsubscribe.\r\n\r\nthanks for your reply, i have some  questions about configure the ovs .do you have some suggestion about add the ethernet port to the ovs-bridge, and another virtual port inside the ovs-bridge can access outside networking(example: google or biying) by the ethernet port. best regards."
            }
          ]
        },
        {
          "issue_url": "https://github.com/floodlight/floodlight/issues/842",
          "issue_title": "VLAN supported OFPacketOut for DHCP server",
          "issue_number": 842,
          "issue_text": "## Description\r\n\r\nThe change include addition of VLAN tag in OFPacketOut created by DHCP server module. This would help client with vlan-id configured to get IP Address from DHCP server.\r\n\r\n## Motivation and Context\r\n\r\n- Hosts with vlan-id configured interface were not able to get IP address from DHCP server because host and DHCP replies were in different VLAN. This change enables host with different VLAN-ID to get IP address from DHCP server.\r\n\r\n## How Has This Been Tested?\r\n\r\n- Code has been tested using OVS and mininet.\r\n\r\n- Floodlight had DHCP module enabled and 2 hosts were connected with ovs. One of host was without any vlan configuration and other was with vlan id configured as 1.\r\n\r\n- DHCP instance was configured using a separate floodlight module.\r\n\r\n- dhcpcd was used to request for IP address from server and both hosts were able to get the IP addresses\r\n\r\n## Types of changes\r\n<!--- What types of changes does your code introduce? Put an `x` in all the boxes that apply: -->\r\n- [x] Bug fix (non-breaking change which fixes an issue)\r\n\r\n## Checklist\r\n<!--- Go over all the following points, and put an `x` in all the boxes that apply. -->\r\n<!--- If you're unsure about any of these, don't hesitate to ask. We're here to help! -->\r\n- [x] My code follows the code style of this project.\r\n- [x] I have read the **CONTRIBUTING** document.",
          "issue_comments": []
        },
        {
          "issue_url": "https://github.com/floodlight/floodlight/issues/841",
          "issue_title": "Controller is forwarding ICMP packets on multiple links",
          "issue_number": 841,
          "issue_text": "**Description**\r\nMy current setup consists of a floodlight controller and a mininet topology executing a basic fat-tree topology. Since I'm going to propose my very own load balancing script, I removed the default floodlight load balancer. The issue is when for example pinging from h8 to h2. The ICMP is being forwarded to multiple links even after all hosts are discovered (pingall). A design depicting the issue is available where the blue links show where the ICMP is being forwarded to and from.\r\n\r\nDesign: https://i.stack.imgur.com/WsIJW.png\r\nMy current floodlight default.properties: https://i.stack.imgur.com/bUgVa.png\r\n\r\n\r\n**To Reproduce**\r\nSteps to reproduce the behaviour:\r\n1. Remove the load balancer module and run floodlight.\r\n2. Execute the  topology on mininet.\r\n3. Ping from h8 to h2\r\n4. Making use of wireshark you will notice that the packet is being forwarded on multiple links.\r\n\r\n**Expected behavior**\r\nThe ICMP packet should be forwarded on one link, since the module was removed.\r\n\r\n**Screenshots**\r\nThe py script on mininet:![image](https://user-images.githubusercontent.com/62202473/78335164-62ffef00-758d-11ea-84c6-a37169a1570c.png)\r\nDesign: https://i.stack.imgur.com/WsIJW.png\r\nMy current floodlight default.properties: https://i.stack.imgur.com/bUgVa.png\r\n\r\n**Desktop (please complete the following information):**\r\n - Floodlight Version v1.2 (latest)\r\n - OS: Windows 10\r\n- Mininet on a separate vm, connection was established and topology was integrated accordingly. (Latest image)\r\n\r\n",
          "issue_comments": []
        },
        {
          "issue_url": "https://github.com/floodlight/floodlight/issues/840",
          "issue_title": "[CSRF] Attacker can insert or delete the element of Access Control Lists",
          "issue_number": 840,
          "issue_text": "**Describe the bug**\\r\\nAttacker can insert or delete the element of Access Control Lists.\\r\\n\\r\\n**To Reproduce**\\r\\nSteps to reproduce the behavior:\\r\\n1. Open the HTML file and submit the form from the browser.\\r\\n```html\\r\\n<html>\\r\\n<meta charset=\"utf-8\">\\r\\n<body>\\r\\n<form action=\"http://127.0.0.1:8080/wm/acl/rules/json\" method=\"POST\" enctype='text/plain'>\\r\\n  <input name='{\"nw-proto\": \"TCP\",\"tp-dst\": \"\",\"dst-ip\": \"1.0.0.2/10\",\"src-ip\": \"1.0.0.1/10\",\"action\": \"ALLOW\"}' value = ''>\\r\\n  <input type = 'submit' >\\r\\n</form>\\r\\n</body>\\r\\n</html>\\r\\n```\\r\\n\\r\\n**Expected behavior**\\r\\nHI I found csrf bug on ACL inserting and deleting. It can be dangerous for admin. Cause attacker can insert or delete arbitrary ACL elements without admin permission.\\r\\nThis attack is an example because Floodlight itself is vulnerable to CSRF attacks.\\r\\nIn fact, for any function that sets Floodlight controller using the “POST“ method of the REST API, we can make arbitrary malicious settings using CSRF attack.\\r\\nIf an administrator accesses an arbitrary web page created by an attacker, Floodlight can be configured with arbitrary settings\\r\\nIt is also possible to assume that Floodlight is on the internal network and the administrator accesses this page from the internal network.\\r\\n\\r\\n**Screenshots**\\r\\n1. Attack procedure\\r\\n<img width=\"1440\" alt=\"스크린샷 2020-03-03 오후 9 27 07\" src=\"https://user-images.githubusercontent.com/26154873/75775833-27a7b000-5d96-11ea-998a-bd787fdff415.png\">\\r\\n2. Attack result\\r\\n<img width=\"900\" alt=\"스크린샷 2020-03-03 오후 9 27 30\" src=\"https://user-images.githubusercontent.com/26154873/75775837-2a0a0a00-5d96-11ea-86c8-ec4bc90980c9.png\">\\r\\n\\r\\n\\r\\n**Desktop**\\r\\n - Floodlight Version v1.2 and master\\r\\n - OS: Ubuntu 18.04.1 LTS\\r\\n - Browser: Firefox 71.0\\r\\n\\r\\n**Additional context**\\r\\n- Reason of This Vulnerability: Floodlight rest api server just accept POST method with `text/plain` Content-Type. Due to the `text/plain` Content-Type, a simple request is sent at the cross origin instead of a pre-flight request. The server is not prepared for this. \\r\\n- Solution:\\r\\n  - Ultimate solution: add a CSRF-token to the post form and check CSRF-tokens in all POST request\\r\\n  - Secondary solution: do not accept POST method with `application/x-www-form-urlencoded`, `multipart/form-data`, and `text/plain` Content-Type (How about responding with `415 Unsupported Media Type`?)\\r\\n",
          "issue_comments": []
        },
        {
          "issue_url": "https://github.com/floodlight/floodlight/issues/838",
          "issue_title": "One demand",
          "issue_number": 838,
          "issue_text": "I want to use floodlight to implement the application as follow:\r\n\r\nThe example topology is:\r\n```\r\n     s3\r\n    / \\\r\nh1—s1—s2—h2\r\n```\r\n\r\nThen h1 send some packages to h2, I want to control s1 that:\r\nif flow size < 100MB, then s1 forward to s2;\r\nelse let 95% packages forwarding to s2 and let 5% packages forwarding  to s3;\r\n\r\nCan someone help me or can floodlight implement?",
          "issue_comments": []
        },
        {
          "issue_url": "https://github.com/floodlight/floodlight/issues/837",
          "issue_title": "[SECURITY] Use HTTPS to resolve dependencies in Maven Build",
          "issue_number": 837,
          "issue_text": "[![mitm_build](https://user-images.githubusercontent.com/1323708/59226671-90645200-8ba1-11e9-8ab3-39292bef99e9.jpeg)](https://medium.com/@jonathan.leitschuh/want-to-take-over-the-java-ecosystem-all-you-need-is-a-mitm-1fc329d898fb?source=friends_link&sk=3c99970c55a899ad9ef41f126efcde0e)\n\n- [Want to take over the Java ecosystem? All you need is a MITM!](https://medium.com/@jonathan.leitschuh/want-to-take-over-the-java-ecosystem-all-you-need-is-a-mitm-1fc329d898fb?source=friends_link&sk=3c99970c55a899ad9ef41f126efcde0e)\n- [Update: Want to take over the Java ecosystem? All you need is a MITM!](https://medium.com/bugbountywriteup/update-want-to-take-over-the-java-ecosystem-all-you-need-is-a-mitm-d069d253fe23?source=friends_link&sk=8c8e52a7d57b98d0b7e541665688b454)\n\n---\n\nThis is a security fix for a  vulnerability in your [Apache Maven](https://maven.apache.org/) `pom.xml` file(s).\n\nThe build files indicate that this project is resolving dependencies over HTTP instead of HTTPS.\nThis leaves your build vulnerable to allowing a [Man in the Middle](https://en.wikipedia.org/wiki/Man-in-the-middle_attack) (MITM) attackers to execute arbitrary code on your or your computer or CI/CD system.\n\nThis vulnerability has a CVSS v3.0 Base Score of [8.1/10](https://nvd.nist.gov/vuln-metrics/cvss/v3-calculator?vector=AV:N/AC:H/PR:N/UI:N/S:U/C:H/I:H/A:H).\n\n[POC code](https://max.computer/blog/how-to-take-over-the-computer-of-any-java-or-clojure-or-scala-developer/) has existed since 2014 to maliciously compromise a JAR file in-flight.\nMITM attacks against HTTP are [increasingly common](https://security.stackexchange.com/a/12050), for example [Comcast is known to have done it to their own users](https://thenextweb.com/insights/2017/12/11/comcast-continues-to-inject-its-own-code-into-websites-you-visit/#).\n\nThis contribution is a part of a submission to the [GitHub Security Lab](https://securitylab.github.com/) Bug Bounty program.\n\n## Detecting this and Future Vulnerabilities\n\nThis vulnerability was automatically detected by [LGTM.com](https://lgtm.com) using this [CodeQL Query](https://lgtm.com/rules/1511115648721/).\n\nAs of September 2019 LGTM.com and Semmle are [officially a part of GitHub](https://github.blog/2019-09-18-github-welcomes-semmle/).\n\nYou can automatically detect future vulnerabilities like this by enabling the free (for open-source) [LGTM App](https://github.com/marketplace/lgtm).\n\nI'm not an employee of GitHub nor of Semmle, I'm simply a user of [LGTM.com](https://lgtm.com) and an open-source security researcher.\n\n## Source\n\nYes, this contribution was automatically generated, however, the code to generate this PR was lovingly hand crafted to bring this security fix to your repository.\n\nThe source code that generated and submitted this PR can be found here:\n[JLLeitschuh/bulk-security-pr-generator](https://github.com/JLLeitschuh/bulk-security-pr-generator)\n\n## Opting-Out\n\nIf you'd like to opt-out of future automated security vulnerability fixes like this, please consider adding a file called\n`.github/GH-ROBOTS.txt` to your repository with the line:\n\n```\nUser-agent: JLLeitschuh/bulk-security-pr-generator\nDisallow: *\n```\n\nThis bot will respect the [ROBOTS.txt](https://moz.com/learn/seo/robotstxt) format for future contributions.\n\nAlternatively, if this project is no longer actively maintained, consider [archiving](https://help.github.com/en/github/creating-cloning-and-archiving-repositories/about-archiving-repositories) the repository.\n\n## CLA Requirements\n\n_This section is only relevant if your project requires contributors to sign a Contributor License Agreement (CLA) for external contributions._\n\nIt is unlikely that I'll be able to directly sign CLAs. However, all contributed commits are already automatically signed-off.\n\n> The meaning of a signoff depends on the project, but it typically certifies that committer has the rights to submit this work under the same license and agrees to a Developer Certificate of Origin \n> (see [https://developercertificate.org/](https://developercertificate.org/) for more information).\n>\n> \\- [Git Commit Signoff documentation](https://developercertificate.org/)\n\nIf signing your organization's CLA is a strict-requirement for merging this contribution, please feel free to close this PR.\n\n## Tracking\n\nAll PR's generated as part of this fix are tracked here: \nhttps://github.com/JLLeitschuh/bulk-security-pr-generator/issues/2",
          "issue_comments": []
        },
        {
          "issue_url": "https://github.com/floodlight/floodlight/issues/836",
          "issue_title": "Error while processing OFFlowRemoved in LearningSwitch module",
          "issue_number": 836,
          "issue_text": "**Bug description*\r\nWe get a runtime exception while using learning switch module with OVS supporting version openflow 1.5 produced due to OFlowRemoved message from switch.\r\nIt shows error while decoding the OFFlowRemoved Message\r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\n1. Go to src/main/resources/floodlightdefault.properties\r\n2. Add 'net.floodlightcontroller.learningswitch.LearningSwitch' to module list\r\n3. Go to root folder of project and recompile it\r\n4. Run floodlight\r\n5. Add a ovs-switch with 2 lxc-containers attached to it to floodlight\r\n6. Ping from a container to other\r\n7. Wait for sometime so that flow expires and we get the exception\r\n\r\n**Expected behavior**\r\nWe expected learning switch module gets notified for flow removed message from switch.\r\n\r\n\r\n\r\n**Screenshots**\r\n![Floodlight](https://user-images.githubusercontent.com/26287448/70243760-38498980-1799-11ea-8756-b95b5a30ed8b.png)\r\n\r\n**Desktop:**\r\n - Floodlight Version master\r\n - OS: Arch Linux\r\n - OVS-version: 2.12.0\r\n\r\n",
          "issue_comments": []
        },
        {
          "issue_url": "https://github.com/floodlight/floodlight/issues/835",
          "issue_title": "controller placement ",
          "issue_number": 835,
          "issue_text": "What modules do I need to write to solve the problem of controller placement?\r\nWhat to do between multiple controllers to send and receive a control packet?",
          "issue_comments": []
        },
        {
          "issue_url": "https://github.com/floodlight/floodlight/issues/834",
          "issue_title": " when I compile floodlight, I get the following error:",
          "issue_number": 834,
          "issue_text": "I use the following steps to install floodlight:\r\n\r\nsudo apt-get install default-jdk\r\njava -version\r\nsudo apt-get install build-essential ant maven python-dev eclipse\r\ngit clone git://github.com/floodlight/floodlight.git\r\ncd floodlight\r\nant\r\n\r\n\r\nBut when I compile floodlight, I get the following error:\r\n\r\n`Buildfile: /home/zddd/yanzhen/floodlight/build.xml\r\n  [taskdef] Could not load definitions from resource tasks.properties. It could not be found.\r\n\r\ninit:\r\n    [mkdir] Created dir: /home/zddd/yanzhen/floodlight/target/bin\r\n    [mkdir] Created dir: /home/zddd/yanzhen/floodlight/target/bin-test\r\n    [mkdir] Created dir: /home/zddd/yanzhen/floodlight/target/lib\r\n    [mkdir] Created dir: /home/zddd/yanzhen/floodlight/target/test\r\n\r\ncompile:\r\n    [javac] Compiling 593 source files to /home/zddd/yanzhen/floodlight/target/bin\r\n    [javac] javac: invalid target release: 1.8\r\n    [javac] Usage: javac <options> <source files>\r\n    [javac] use -help for a list of possible options\r\n\r\nBUILD FAILED\r\n/home/zddd/yanzhen/floodlight/build.xml:145: Compile failed; see the compiler error output for details.\r\n\r\nTotal time: 0 seconds`\r\n\r\nWhat's wrong with that?\r\n",
          "issue_comments": [
            {
              "comment_username": "mxyi",
              "comment_create_time": "2019-11-16T16:22:18Z",
              "comment_edit_time": "2019-11-16T16:22:18Z",
              "comment_text": "install jdk1.8"
            },
            {
              "comment_username": "wilton-tang",
              "comment_create_time": "2020-08-19T09:22:25Z",
              "comment_edit_time": "2020-08-19T10:06:01Z",
              "comment_text": "> \r\n> \r\n> install jdk1.8\r\n\r\nplease, there is only jdk1.8 effective?  i get failed by openjdk1.8.  thanks, best regards. my os is centos."
            }
          ]
        },
        {
          "issue_url": "https://github.com/floodlight/floodlight/issues/832",
          "issue_title": "Ant eclipse problems",
          "issue_number": 832,
          "issue_text": "On Ubutnu 18.04 with Oracle java 8, I can compile floodlight and run it, but if I try to create the Eclipse project files, I get the errors below.\r\n\r\nThanks in advance\r\n\r\n$ ant eclipse\r\nBuildfile: /home/claudio/Ricerca/docker/floodlight/build.xml\r\n  [taskdef] Could not load definitions from resource tasks.properties. It could not be found.\r\n\r\ninit:\r\n\r\neclipse:\r\n\r\nBUILD SUCCESSFUL\r\nTotal time: 0 seconds\r\n",
          "issue_comments": [
            {
              "comment_username": "qingwang-bsn",
              "comment_create_time": "2019-10-17T23:52:39Z",
              "comment_edit_time": "2019-10-17T23:52:39Z",
              "comment_text": "@ClaZu You should be able to safely ignore this \"taskdef\" warning, as long as you see \"BUILD SUCCESSFUL\" at the end of the Floodlight compile. "
            }
          ]
        },
        {
          "issue_url": "https://github.com/floodlight/floodlight/issues/831",
          "issue_title": "Report a bug that cause hosts network unreachability",
          "issue_number": 831,
          "issue_text": "This bug is different with #830\r\n\r\n**Describe the bug**\r\nIssue\r\n\r\nNetwork unreachability occurs between hosts which belong to benign switches if malicious switch send attack payload to SDN controller.\r\n\r\nSummary\r\n\r\nMalicious switch can cause network unreachability between hosts that belong to benign switches.\r\n\r\nAffected Software / Version\r\n\r\nFloodlight v1.2 latest release (27be59b)\r\n\r\nTopology configuration\r\n\r\nWe considered topology like topology picture in screenshot section. s0 is malicious switch, which is not related to flow path between hosts. s1, s2, s3 are benign switch and h1, h2, h3, h4 are benign hosts. The important thing here is s0 is not on the flow path between hosts. In other words, hosts can communicate through flow path s2-s1-s3 or s3-s1-s2 without s0.\r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\n1. Turn on Floodlight on control plane docker container\r\n2. Turn on Mininet on data plane docker container\r\n3. Connect mininet to Floodlight\r\n4. Send payload to Floodlight from switch s0\r\n5. Wait for 40 seconds\r\n6. Command \"pingall\" from mininet\r\n7. Then, ping between hosts are dropped (66% packets are dropped)\r\n\r\nPayload\r\n\r\nAttacking payload is attached to only email sent to floodlight@groups.io for responsible disclosure.\r\n\r\n**Expected behavior**\r\nA clear and concise description of what you expected to happen.\r\n1) Commanding \"pingall\" on mininet will be dropped (66% packets are dropped)\r\n\r\n**Screenshots**\r\nIf applicable, add screenshots to help explain your problem.\r\n![topo](https://user-images.githubusercontent.com/25295690/61431144-791a4080-a967-11e9-923a-5c48c2fe9776.png)\r\n<img width=\"365\" alt=\"figure1_\" src=\"https://user-images.githubusercontent.com/25295690/61431501-a3b8c900-a968-11e9-98c0-c80fb7a9d6b3.png\">\r\n\r\n\r\n**Desktop (please complete the following information):**\r\n- Floodlight v1.2 latest release (27be59b)\r\nIt is tested on Virtual Box 6.0.6 r130049 and VM setting is follows:\r\n- assign 2 core of intel i7-8700K\r\n- assign 9GB RAM\r\n- assign 64GB of SSD\r\n- Ubuntu 18.04\r\n\r\n**Additional context**\r\nSeverity\r\n\r\nMalicious switch, which is not related to flow path between hosts, can cause network unreachability between hosts in benign flow path: it breaks availability which is the one of the basic attribute of security\r\n",
          "issue_comments": []
        },
        {
          "issue_url": "https://github.com/floodlight/floodlight/issues/830",
          "issue_title": "Report a bug that cause hosts network unreachability and PACKET_OUT flooding",
          "issue_number": 830,
          "issue_text": "**Describe the bug**\r\n\r\nIssue\r\n\r\nNetwork unreachability occurs between hosts which belong to benign switches if malicious switch send attack payload to SDN controller.\r\nAnd also, concurrently, massive amounts of PACKET_OUT flooding is observed from Wireshark.\r\n\r\n\r\nSummary\r\n\r\nMalicious switch can cause network unreachability between hosts that belong to benign switches. And also, PACKET_OUT flooding occurs by this attack, which leads to control plane CPU usage exhaustion.\r\n\r\n\r\nAffected Software / Version\r\n\r\nFloodlight master branch (18ef888 latest commit)\r\n\r\n\r\nTopology configuration\r\n\r\nWe considered topology like topo.png in screenshot section. s0 is malicious switch, which is not related to flow path between hosts. s1, s2, s3 are benign switch and h1, h2, h3, h4 are benign hosts. The important thing here is s0 is not on the flow path between hosts. In other words, hosts can communicate through flow path s2-s1-s3 or s3-s1-s2 without s0.\r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\n1. Turn on Floodlight on control plane docker container\r\n2. Turn on Mininet on data plane docker container\r\n3. Connect mininet to Floodlight\r\n4. Send payload to Floodlight from switch s0\r\n5. Wait for 15 seconds\r\n6. Command \"pingall\" from mininet\r\n7. Then, ping between hosts are dropped\r\n8. Also, PACKET_OUT flooding can be observed from Wireshark\r\n\r\nPayload\r\n\r\nAttacking payload is attached to only email sent to floodlight@groups.io for responsible disclosure.\r\n\r\n**Expected behavior**\r\n1) Commanding \"pingall\" on mininet will be dropped\r\n2) Massive amounts of PACKET_OUT flooding can be observed from Wireshark\r\n\r\n**Screenshots**\r\nIf applicable, add screenshots to help explain your problem.\r\n![topo](https://user-images.githubusercontent.com/25295690/61428651-0fe1ff80-a95e-11e9-8508-29cbb19f5ff9.png)\r\n<img width=\"365\" alt=\"figure2\" src=\"https://user-images.githubusercontent.com/25295690/61428594-cdb8be00-a95d-11e9-8567-e9655199bc7f.png\">\r\n<img width=\"484\" alt=\"figure1\" src=\"https://user-images.githubusercontent.com/25295690/61428595-cdb8be00-a95d-11e9-8c54-ec78e77cbbab.png\">\r\n\r\n\r\n**Desktop (please complete the following information):**\r\n- Floodlight master branch (18ef888 latest commit)\r\n- It is tested on Virtual Box 6.0.6 r130049 and VM setting is follows:\r\n  - assign 2 core of intel i7-8700K\r\n  - assign 9GB RAM\r\n  - assign 64GB of SSD\r\n  - Ubuntu 18.04\r\n  - openjdk8\r\n\r\n**Additional context**\r\nSeverity\r\n\r\nMalicious switch, which is not related to flow path between hosts, can cause network unreachability between hosts in benign flow path: it breaks availability which is the one of the basic attribute of security\r\nAlso, PACKET_OUT flooding caused by malicious switch make attacker able to remotely cause control plane CPU usage exhaustion.\r\n",
          "issue_comments": []
        },
        {
          "issue_url": "https://github.com/floodlight/floodlight/issues/829",
          "issue_title": "topology freezing",
          "issue_number": 829,
          "issue_text": "Summary: We discovered a weakness in the module responsible for computing the topology instance which can be used to \"freeze\" the current topology instance. This attack can be launched to prevent the controller from updating part of the network topology view. \r\n\r\nEssentially, this attack is based on the following observation: When two links are created that originate from the same switch/port but end in two different network locations, the link discovery service (LDS) accepts both links but it treats the \"multi-link\" port as a broadcast domain port. This results in removing the \"origin\" port and its links from the topology graph construction. Yet, we made the crucial observation that these links are still being used by the controller to compute the shortest path (using an outdated topology instance), causing systematic runtime exceptions. \r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
          "issue_comments": []
        },
        {
          "issue_url": "https://github.com/floodlight/floodlight/issues/828",
          "issue_title": "LINK-TYPE weakness in the Link Discovery Service ",
          "issue_number": 828,
          "issue_text": "Summary: We discovered a weakness in the way the Link Discovery Service (LDS) handles the LINK-TYPE field inside LLDP packets. Suppose that switch 1 (S1) and switch 2 (S2) are connected with each other and the controller does not know yet about the existence of this link. Initially, the controller sends an LLDP packet with the LINK-TYPE field set to '0x01' to S1 which in turn sends it to S2. Once the controller infers a unidirectional link from S1 to S2, it immediately checks if the reverse link (from S2 to S1) exists by sending an LLDP packet with the LINK-TYPE field set to '0x02' to S2 (which then it forwards to S1). We found that, by repeatedly sending LLDP packets with the LINK-TYPE field reset to '0x01' (using host 1 connected to S1), we can force the controller to check if the reverse link exists indefinitely. The core idea behind this attack is to extend the duration of an LLDP round as much as possible to exhaust the controller resources, potentially leading to crashes. This attack also results in more LLDP packets being sent by the controller, which can negatively impact the network bandwidth.  \r\n\r\nPossible extension of the attack: We also found that LDS re-computes the topology instance every time the latency of a link changes. Then we conducted a series of experiments where we instructed host 1 to send LLDP packets (with the LINK-TYPE field set to '0x01') with a slightly modified time-stamp to force the controller into recomputing the topology instance. Here it is important to note that computing the topology instance is a very demanding task that can cause the controller to crash.\r\n\r\nSpecifically, this vulnerability originates here:\r\nhttps://github.com/floodlight/floodlight/blob/18ef8882180d2f002c3c57c17d270ade885ef479/src/main/java/net/floodlightcontroller/linkdiscovery/internal/LinkDiscoveryManager.java#L802\r\n\r\nPossible solution: The fundamental reason why this attack is possible is because LLDP packets lack integrity protection that guarantees that they have not been tampered with, and originate from the sender switch. To mitigate this attack, mechanisms should be incorportared that keep track of the number of times a certain LLDP packet is sent by the controller.",
          "issue_comments": []
        },
        {
          "issue_url": "https://github.com/floodlight/floodlight/issues/827",
          "issue_title": "Support building Eclipse project on Ant Windows",
          "issue_number": 827,
          "issue_text": "Support building Eclipse project on Ant Windows\\r\\n\\r\\n## Description\\r\\nRunning 'ant eclipse' fails on Windows OS as it lacks support for running shell/bash scripts. So, I added a \"setup-eclipse.bat\" (Windows Batch) file to do the same in Windows. This requires modification in \"build.xml\" to check for appropriate OS executable.\\r\\n\\r\\n## Motivation and Context\\r\\nBuilding eclipse project in Windows OS.\\r\\n\\r\\n## Types of changes\\r\\n<!--- What types of changes does your code introduce? Put an `x` in all the boxes that apply: -->\\r\\n- [ ] Bug fix (non-breaking change which fixes an issue)\\r\\n- [x] New feature (non-breaking change which adds functionality)\\r\\n- [ ] Breaking change (fix or feature that would cause existing functionality to change)\\r\\n\\r\\n## Checklist\\r\\n<!--- Go over all the following points, and put an `x` in all the boxes that apply. -->\\r\\n<!--- If you're unsure about any of these, don't hesitate to ask. We're here to help! -->\\r\\n- [x] My code follows the code style of this project.\\r\\n- [ ] My change requires a change to the documentation.\\r\\n- [ ] I have updated the documentation accordingly.\\r\\n- [ ] I have read the **CONTRIBUTING** document.\\r\\n- [ ] I have added tests to cover my changes.\\r\\n- [x] All new and existing tests passed.\\r\\n",
          "issue_comments": []
        },
        {
          "issue_url": "https://github.com/floodlight/floodlight/issues/826",
          "issue_title": "Floodlight Build Error",
          "issue_number": 826,
          "issue_text": "With Java 8, Java 11, Java 12, openjfx, the error still remains.\r\n\r\nstudent@controller:~/floodlight$ ant\r\nBuildfile: /users/student/floodlight/build.xml\r\n  [taskdef] Could not load definitions from resource tasks.properties. It could not be found.\r\n\r\ninit:\r\n\r\ncompile:\r\n    [javac] Compiling 593 source files to /users/student/floodlight/target/bin\r\n    [javac] warning: [options] bootstrap class path not set in conjunction with -source 8\r\n    [javac] /users/student/floodlight/src/main/java/net/floodlightcontroller/loadbalancer/LoadBalancer.java:57: error: package javafx.util does not exist\r\n    [javac] import javafx.util.Pair;\r\n    [javac]                   ^\r\n    [javac] /users/student/floodlight/src/main/java/net/floodlightcontroller/statistics/IStatisticsService.java:10: error: package javafx.util does not exist\r\n    [javac] import javafx.util.Pair;\r\n    [javac]                   ^\r\n    [javac] /users/student/floodlight/src/main/java/net/floodlightcontroller/loadbalancer/LoadBalancer.java:138: error: cannot find symbol\r\n    [javac]     protected HashMap<Pair<Match,DatapathId>,String> flowToVipId;\r\n    [javac]                       ^\r\n    [javac]   symbol:   class Pair\r\n    [javac]   location: class LoadBalancer\r\n    [javac] /users/student/floodlight/src/main/java/net/floodlightcontroller/statistics/IStatisticsService.java:25: error: cannot find symbol\r\n    [javac]     Map<Pair<Match,DatapathId>, FlowRuleStats> getFlowStats();\r\n    [javac]         ^\r\n    [javac]   symbol:   class Pair\r\n    [javac]   location: interface IStatisticsService\r\n    [javac] /users/student/floodlight/src/main/java/net/floodlightcontroller/statistics/StatisticsCollector.java:6: error: package javafx.util does not exist\r\n    [javac] import javafx.util.Pair;\r\n    [javac]                   ^\r\n    [javac] /users/student/floodlight/src/main/java/net/floodlightcontroller/statistics/StatisticsCollector.java:64: error: cannot find symbol\r\n    [javac]     private static final HashMap<Pair<Match,DatapathId>, FlowRuleStats> flowStats = new HashMap<Pair<Match,DatapathId>,FlowRuleStats>();\r\n    [javac]                                  ^\r\n    [javac]   symbol:   class Pair\r\n    [javac]   location: class StatisticsCollector\r\n    [javac] /users/student/floodlight/src/main/java/net/floodlightcontroller/statistics/StatisticsCollector.java:376: error: cannot find symbol\r\n    [javac]     public Map<Pair<Match, DatapathId>, FlowRuleStats> getFlowStats(){\r\n    [javac]                ^\r\n    [javac]   symbol:   class Pair\r\n    [javac]   location: class StatisticsCollector\r\n    [javac] /users/student/floodlight/src/main/java/org/sdnplatform/sync/internal/rpc/AbstractRPCChannelHandler.java:12: error: package javax.xml.bind does not exist\r\n    [javac] import javax.xml.bind.DatatypeConverter;\r\n    [javac]                      ^\r\n    [javac] /users/student/floodlight/src/main/java/org/sdnplatform/sync/internal/store/JavaDBStorageEngine.java:19: error: package javax.xml.bind does not exist\r\n    [javac] import javax.xml.bind.DatatypeConverter;\r\n    [javac]                      ^\r\n    [javac] /users/student/floodlight/src/main/java/net/floodlightcontroller/loadbalancer/LoadBalancer.java:725: error: cannot find symbol\r\n    [javac]                             Pair<Match, DatapathId> pair = new Pair<Match,DatapathId>(mb.build(),sw);\r\n    [javac]                             ^\r\n    [javac]   symbol:   class Pair\r\n    [javac]   location: class LoadBalancer\r\n    [javac] /users/student/floodlight/src/main/java/net/floodlightcontroller/loadbalancer/LoadBalancer.java:725: error: cannot find symbol\r\n    [javac]                             Pair<Match, DatapathId> pair = new Pair<Match,DatapathId>(mb.build(),sw);\r\n    [javac]                                                                ^\r\n    [javac]   symbol:   class Pair\r\n    [javac]   location: class LoadBalancer\r\n    [javac] /users/student/floodlight/src/main/java/net/floodlightcontroller/loadbalancer/LoadBalancer.java:837: error: cannot find symbol\r\n    [javac]                                             for(Pair<Match,DatapathId> pair: flowToVipId.keySet()){ // from the flows set from the load balancer\r\n    [javac]                                                 ^\r\n    [javac]   symbol:   class Pair\r\n    [javac]   location: class LoadBalancer.SetPoolStats\r\n    [javac] /users/student/floodlight/src/main/java/net/floodlightcontroller/loadbalancer/LoadBalancer.java:866: error: cannot find symbol\r\n    [javac]             HashMap<Pair<IDevice,String>,String> deviceToMemberId = new HashMap<Pair<IDevice,String>, String>();\r\n    [javac]                     ^\r\n    [javac]   symbol:   class Pair\r\n    [javac]   location: class LoadBalancer\r\n    [javac] /users/student/floodlight/src/main/java/net/floodlightcontroller/loadbalancer/LoadBalancer.java:866: error: cannot find symbol\r\n    [javac]             HashMap<Pair<IDevice,String>,String> deviceToMemberId = new HashMap<Pair<IDevice,String>, String>();\r\n    [javac]                                                                                 ^\r\n    [javac]   symbol:   class Pair\r\n    [javac]   location: class LoadBalancer\r\n    [javac] /users/student/floodlight/src/main/java/net/floodlightcontroller/loadbalancer/LoadBalancer.java:876: error: cannot find symbol\r\n    [javac]                                                     Pair<IDevice,String> pair = new Pair<IDevice,String>(d,pool.id);\r\n    [javac]                                                     ^\r\n    [javac]   symbol:   class Pair\r\n    [javac]   location: class LoadBalancer\r\n    [javac] /users/student/floodlight/src/main/java/net/floodlightcontroller/loadbalancer/LoadBalancer.java:876: error: cannot find symbol\r\n    [javac]                                                     Pair<IDevice,String> pair = new Pair<IDevice,String>(d,pool.id);\r\n    [javac]                                                                                     ^\r\n    [javac]   symbol:   class Pair\r\n    [javac]   location: class LoadBalancer\r\n    [javac] /users/student/floodlight/src/main/java/net/floodlightcontroller/loadbalancer/LoadBalancer.java:886: error: cannot find symbol\r\n    [javac]                     for(Pair<IDevice, String> membersDevice: deviceToMemberId.keySet()){\r\n    [javac]                         ^\r\n    [javac]   symbol:   class Pair\r\n    [javac]   location: class LoadBalancer\r\n    [javac] /users/student/floodlight/src/main/java/net/floodlightcontroller/loadbalancer/LoadBalancer.java:1340: error: cannot find symbol\r\n    [javac]             flowToVipId = new HashMap<Pair<Match,DatapathId>,String>();\r\n    [javac]                                       ^\r\n    [javac]   symbol:   class Pair\r\n    [javac]   location: class LoadBalancer\r\n    [javac] /users/student/floodlight/src/main/java/net/floodlightcontroller/statistics/StatisticsCollector.java:64: error: cannot find symbol\r\n    [javac]     private static final HashMap<Pair<Match,DatapathId>, FlowRuleStats> flowStats = new HashMap<Pair<Match,DatapathId>,FlowRuleStats>();\r\n    [javac]                                                                                                 ^\r\n    [javac]   symbol:   class Pair\r\n    [javac]   location: class StatisticsCollector\r\n    [javac] /users/student/floodlight/src/main/java/net/floodlightcontroller/statistics/StatisticsCollector.java:201: error: cannot find symbol\r\n    [javac]                                                     Pair<Match, DatapathId> pair = new Pair<Match,DatapathId>(pse.getMatch(),e.getKey());\r\n    [javac]                                                     ^\r\n    [javac]   symbol:   class Pair\r\n    [javac]   location: class StatisticsCollector.FlowStatsCollector\r\n    [javac] /users/student/floodlight/src/main/java/net/floodlightcontroller/statistics/StatisticsCollector.java:201: error: cannot find symbol\r\n    [javac]                                                     Pair<Match, DatapathId> pair = new Pair<Match,DatapathId>(pse.getMatch(),e.getKey());\r\n    [javac]                                                                                        ^\r\n    [javac]   symbol:   class Pair\r\n    [javac]   location: class StatisticsCollector.FlowStatsCollector\r\n    [javac] /users/student/floodlight/src/main/java/net/floodlightcontroller/statistics/StatisticsCollector.java:383: error: cannot find symbol\r\n    [javac]             for(Pair<Match,DatapathId> pair: flowStats.keySet()){\r\n    [javac]                 ^\r\n    [javac]   symbol:   class Pair\r\n    [javac]   location: class StatisticsCollector\r\n    [javac] /users/student/floodlight/src/main/java/org/sdnplatform/sync/internal/rpc/AbstractRPCChannelHandler.java:606: error: cannot find symbol\r\n    [javac]         currentChallenge = DatatypeConverter.printBase64Binary(challengeBytes);\r\n    [javac]                            ^\r\n    [javac]   symbol:   variable DatatypeConverter\r\n    [javac]   location: class AbstractRPCChannelHandler\r\n    [javac] /users/student/floodlight/src/main/java/org/sdnplatform/sync/internal/rpc/AbstractRPCChannelHandler.java:615: error: cannot find symbol\r\n    [javac]         byte[] expectedBytes = DatatypeConverter.parseBase64Binary(expected);\r\n    [javac]                                ^\r\n    [javac]   symbol:   variable DatatypeConverter\r\n    [javac]   location: class AbstractRPCChannelHandler\r\n    [javac] /users/student/floodlight/src/main/java/org/sdnplatform/sync/internal/rpc/AbstractRPCChannelHandler.java:616: error: cannot find symbol\r\n    [javac]         byte[] reponseBytes = DatatypeConverter.parseBase64Binary(response);\r\n    [javac]                               ^\r\n    [javac]   symbol:   variable DatatypeConverter\r\n    [javac]   location: class AbstractRPCChannelHandler\r\n    [javac] /users/student/floodlight/src/main/java/org/sdnplatform/sync/internal/rpc/AbstractRPCChannelHandler.java:640: error: cannot find symbol\r\n    [javac]                     mac.doFinal(DatatypeConverter.parseBase64Binary(challenge));\r\n    [javac]                                 ^\r\n    [javac]   symbol:   variable DatatypeConverter\r\n    [javac]   location: class AbstractRPCChannelHandler\r\n    [javac] /users/student/floodlight/src/main/java/org/sdnplatform/sync/internal/rpc/AbstractRPCChannelHandler.java:641: error: cannot find symbol\r\n    [javac]             return DatatypeConverter.printBase64Binary(output);\r\n    [javac]                    ^\r\n    [javac]   symbol:   variable DatatypeConverter\r\n    [javac]   location: class AbstractRPCChannelHandler\r\n    [javac] /users/student/floodlight/src/main/java/org/sdnplatform/sync/internal/store/JavaDBStorageEngine.java:392: error: cannot find symbol\r\n    [javac]         return DatatypeConverter.printBase64Binary(key.get());\r\n    [javac]                ^\r\n    [javac]   symbol:   variable DatatypeConverter\r\n    [javac]   location: class JavaDBStorageEngine\r\n    [javac] /users/student/floodlight/src/main/java/org/sdnplatform/sync/internal/store/JavaDBStorageEngine.java:397: error: cannot find symbol\r\n    [javac]         return new ByteArray(DatatypeConverter.parseBase64Binary(keyStr));\r\n    [javac]                              ^\r\n    [javac]   symbol:   variable DatatypeConverter\r\n    [javac]   location: class JavaDBStorageEngine\r\n    [javac] Note: Some input files use or override a deprecated API.\r\n    [javac] Note: Recompile with -Xlint:deprecation for details.\r\n    [javac] Note: Some input files use unchecked or unsafe operations.\r\n    [javac] Note: Recompile with -Xlint:unchecked for details.\r\n    [javac] 29 errors\r\n    [javac] 1 warning\r\n\r\nBUILD FAILED\r\n/users/student/floodlight/build.xml:145: Compile failed; see the compiler error output for details.\r\n",
          "issue_comments": [
            {
              "comment_username": "matthewstrasiotto",
              "comment_create_time": "2019-09-24T04:12:50Z",
              "comment_edit_time": "2019-09-24T04:12:50Z",
              "comment_text": "I also am having this build error, just following the instructions given in the install guide.\r\n\r\nMaybe the guide isn't up-to-date with the master branch? \r\nFor me, I'm unsure if its a system config problem - ive tried the default \r\n```\r\napt install openjfx\r\n```\r\n\r\nand I've used update-alternatives to set my jdk version from openjdk-11 to openjdk-8, given that the guide prescribes openjdk 8\r\n\r\nFrom what I can see, javafx doesn't seem to distribute backwards compatible versions of openjfx, at least not from their website. \r\n\r\nMaybe on ubuntu's ppa they do, if you specify the version.\r\n\r\nI assume that that might mean that the version of open jfx that you get with `sudo apt install openjfx` might only be supported by `openjdk-11` (and greater?) "
            },
            {
              "comment_username": "matthewstrasiotto",
              "comment_create_time": "2019-09-24T04:23:19Z",
              "comment_edit_time": "2019-09-24T04:23:19Z",
              "comment_text": "Running the following:\r\n\r\n```\r\nsudo apt list -a openjfx\r\n```\r\n\r\ni get this output\r\n\r\n```\r\nListing... Done\r\nopenjfx/bionic-updates,bionic-security,now 11.0.2+1-1~18.04.2 amd64 [installed]\r\nopenjfx/bionic 8u161-b12-1ubuntu2 amd64\r\n```\r\n\r\nI notice a promising version 8 specified on the second entry\r\nI'm able to install this with\r\n\r\n```\r\nsudo apt-get install openjfx=8u161-b12-1ubuntu2\r\n```\r\n\r\nGiving the output:\r\n\r\n```\r\nReading package lists... Done\r\nBuilding dependency tree       \r\nReading state information... Done\r\nThe following package was automatically installed and is no longer required:\r\n  openjfx-source\r\nUse 'sudo apt autoremove' to remove it.\r\nThe following packages will be DOWNGRADED:\r\n  openjfx\r\n0 to upgrade, 0 to newly install, 1 to downgrade, 0 to remove and 14 not to upgrade.\r\nNeed to get 33.6 kB of archives.\r\nAfter this operation, 47.1 kB of additional disk space will be used.\r\nDo you want to continue? [Y/n] y\r\nGet:1 http://au.archive.ubuntu.com/ubuntu bionic/universe amd64 openjfx amd64 8u161-b12-1ubuntu2 [33.6 kB]\r\nFetched 33.6 kB in 0s (236 kB/s) \r\ndpkg: warning: downgrading openjfx from 11.0.2+1-1~18.04.2 to 8u161-b12-1ubuntu2\r\n(Reading database ... 223880 files and directories currently installed.)\r\nPreparing to unpack .../openjfx_8u161-b12-1ubuntu2_amd64.deb ...\r\nUnpacking openjfx (8u161-b12-1ubuntu2) over (11.0.2+1-1~18.04.2) ...\r\nSetting up openjfx (8u161-b12-1ubuntu2) ...\r\nProcessing triggers for man-db (2.8.3-2ubuntu0.1) ...\r\n```\r\n\r\nI'll be running update-alternatives to set my openjdk version back to 8, and let you know what happens"
            },
            {
              "comment_username": "matthewstrasiotto",
              "comment_create_time": "2019-09-24T04:50:10Z",
              "comment_edit_time": "2019-09-24T04:50:10Z",
              "comment_text": "Okay so master isn't building (Isn't building according to the CI as well) lol\r\n\r\nJust checkout 1.2 and build that lmao"
            },
            {
              "comment_username": "apw005",
              "comment_create_time": "2019-09-24T05:10:03Z",
              "comment_edit_time": "2019-09-24T05:10:03Z",
              "comment_text": "I have to double check my version, I saved the working commands in a file. The install guide is wrong and needs to be updated to send users to the folder with a working build."
            },
            {
              "comment_username": "matthewstrasiotto",
              "comment_create_time": "2019-09-24T10:23:32Z",
              "comment_edit_time": "2019-09-24T10:23:32Z",
              "comment_text": "Yep, the docs are super super out of date. \r\nJust discovered it builds with Maven ahahaha. \r\nSo the typical workflow is probably\r\n\r\n```\r\nmvn compile\r\nmvn build\r\n```\r\n\r\npretty weird to have such a slackly documented codebase though\r\n"
            },
            {
              "comment_username": "apw005",
              "comment_create_time": "2019-09-24T19:44:43Z",
              "comment_edit_time": "2019-09-24T19:44:43Z",
              "comment_text": "cd /local/ && sudo apt-get install ant && sudo git clone git://github.com/floodlight/floodlight.git && cd floodlight && sudo git submodule init && sudo git submodule update \r\n\r\nsudo apt-get install build-essential ant maven python-dev eclipse\r\n\r\n# sudo ant\r\nsudo apt install locate\r\nsudo  update-alternatives --config java\r\n\r\n\r\nIs what I used. But only worked with the directory from github as mentioned in this comment. I could not get mvn compile and mvn build to work with the floodlight directory that is listed in the tutorial docs."
            },
            {
              "comment_username": "apw005",
              "comment_create_time": "2019-09-24T19:50:35Z",
              "comment_edit_time": "2019-09-24T19:50:35Z",
              "comment_text": "Thank you after many commands, a successful build.\r\nInstall Java 8\r\nsudo apt-get update && sudo apt-get install software-properties-common && sudo add-apt-repository ppa:linuxuprising/java && sudo apt-get update && sudo apt install openjdk-8-jdk-headless \r\n\r\ncd /local/ && sudo git clone https://github.com/rizard/floodlight\r\n\r\nsudo chown -hR student:ch--IoT-Security floodlight/\r\n\r\ncd floodlight/\r\n\r\nsudo update-alternatives --config java \r\n#select option for Java 8\r\nmvn package -DskipTests\r\n\r\nThank you all. \r\n\r\n\r\nFrom <https://groups.io/g/floodlight/topic/32228606#337>"
            },
            {
              "comment_username": "parsik6",
              "comment_create_time": "2019-09-27T11:15:43Z",
              "comment_edit_time": "2019-09-27T11:15:43Z",
              "comment_text": "I had the same issue, you must set your java (jdk) to version 8 (You must use java 8).\r\n(I faced with this error when i was building it with java11 in intellij, after that when is used java 8, it worked).\r\n"
            }
          ]
        },
        {
          "issue_url": "https://github.com/floodlight/floodlight/issues/825",
          "issue_title": "IP address parsing bug in webUI  when using IPv6",
          "issue_number": 825,
          "issue_text": "Hi! I'm recently working on a SDN project running under IPv6, but I encountered with a problem that the webUI cannot load any data thorgh REST Api and alerts showing \"SyntaxError: Failed to execute 'open' on 'XMLHttpRequest': Invalid URL \" keep poping out when the host name is an ipv6 address.\\r\\n\\r\\nAfter checking the source code, I found that the code used to obtain the host name and port  in /floodlight/src/main/resources/web/pages/login.html is the cause of the bug.\\r\\n\\r\\nThe original code:\\r\\n`        var url = window.location.host.split(':');\\r\\n        var ip = url[0];\\r\\n        var port = url[1];\\r\\n        $.cookie('cip', ip, { expires: 7 });\\r\\n        $.cookie('cport', port, { expires: 7 });`\\r\\nSince the address of ipv6 contains \":\", the url cannot be parsed correctly, leading to that cookies carry invalid ip and port infomation, which makes REST api failed.  Therefore, I modified the code \\r\\nas below:\\r\\n\\r\\n`       var ip = window.location.hostname;\\r\\n        var port = window.location.port;\\r\\n        $.cookie('cip', ip, { expires: 7 });\\r\\n        $.cookie('cport', port, { expires: 7 });\\r\\n`\\r\\nFinally, everything goes well after that changing : )\\r\\n\\r\\n",
          "issue_comments": []
        },
        {
          "issue_url": "https://github.com/floodlight/floodlight/issues/824",
          "issue_title": "Please help with sending a data packet from floodlight to a host connected to the OF switch",
          "issue_number": 824,
          "issue_text": "**Describe the bug**\r\nI try to send a data from floodlight controller to the OF switch the reach to a host connected to the OF switch. Therefore, I follow your instruction about \"How to create a packet out messages\" and put the code in the main program of the controller. However, the program cannot be run properly. \r\n\r\n**Source code**\r\nBelow is the code of the Main.java program.\r\n\r\npublic static void main(String[] args) throws FloodlightModuleException {\r\n\t\ttry {\r\n\t\t\tScanner scanIn = new Scanner(System.in);\r\n                        String data = \"\";\r\n                        System.out.println(\"Please enter the data input = \");\r\n                        data = scanIn.nextLine();\r\n                        System.err.println(\"Data is \" + data);\r\n                        \r\n                        IOFSwitch mySwitch = null;\r\n                        Ethernet l2 = new Ethernet();\r\n                        l2.setSourceMACAddress(MacAddress.of(\"00:0c:29:15:70:81\")); //MAC of the controller\r\n                        l2.setDestinationMACAddress(MacAddress.BROADCAST);\r\n                        l2.setEtherType(EthType.IPv4);\r\n                        \r\n                        IPv4 l3 = new IPv4();\r\n                        l3.setSourceAddress(IPv4Address.of(\"127.0.0.1\")); //IP of the controller\r\n                        l3.setDestinationAddress(IPv4Address.of(\"10.0.0.255\")); //broadcast to all hosts connected to the OVS\r\n                        l3.setTtl((byte) 64);\r\n                        l3.setProtocol(IpProtocol.UDP);\r\n                        \r\n                        UDP l4 = new UDP();\r\n                        l4.setSourcePort(TransportPort.of(65003));\r\n                        l4.setDestinationPort(TransportPort.of(67));\r\n                        \r\n                        Data l7 = new Data();\r\n                        l7.setData(data.getBytes()); //l7.setData(new byte[1000]);\r\n                        \r\n                        l2.setPayload(l3);\r\n                        l3.setPayload(l4);\r\n                        l4.setPayload(l7);\r\n                        \r\n                        byte[] serializedData = l2.serialize();\r\n                        OFPacketOut po = mySwitch.getOFFactory().buildPacketOut()\r\n                                .setData(serializedData)\r\n                                .setActions((List<OFAction>) Collections.singleton((OFAction) \r\n                       mySwitch.getOFFactory().actions().output(OFPort.FLOOD, 0xffFFffFF)))\r\n                                .setInPort(OFPort.CONTROLLER)\r\n                                .build();\r\n                        mySwitch.write(po);\r\n\r\n                        // Setup logger\r\n\t\t\tSystem.setProperty(\"org.restlet.engine.loggerFacadeClass\", \r\n\t\t\t\t\t\"org.restlet.ext.slf4j.Slf4jLoggerFacade\");\r\n\r\n\t\t\tCmdLineSettings settings = new CmdLineSettings();\r\n\t\t\tCmdLineParser parser = new CmdLineParser(settings);\r\n\t\t\ttry {\r\n\t\t\t\tparser.parseArgument(args);\r\n\t\t\t} catch (CmdLineException e) {\r\n\t\t\t\tparser.printUsage(System.out);\r\n\t\t\t\tSystem.exit(1);\r\n\t\t\t}\r\n\r\n\t\t\t// Load modules\r\n\t\t\tFloodlightModuleLoader fml = new FloodlightModuleLoader();\r\n\t\t\ttry {\r\n\t\t\t\tIFloodlightModuleContext moduleContext = \r\n                                      fml.loadModulesFromConfig(settings.getModuleFile());\r\n\t\t\t\tIRestApiService restApi = \r\n                                       moduleContext.getServiceImpl(IRestApiService.class);\r\n\t\t\t\trestApi.run(); \r\n\t\t\t} catch (FloodlightModuleConfigFileNotFoundException e) {\r\n\t\t\t\t// we really want to log the message, not the stack trace\r\n\t\t\t\tlogger.error(\"Could not read config file: {}\", e.getMessage());\r\n\t\t\t\tSystem.exit(1);\r\n\t\t\t}\r\n\t\t\ttry {\r\n                fml.runModules(); // run the controller module and all modules\r\n            } catch (FloodlightModuleException e) {\r\n                logger.error(\"Failed to run controller modules\", e);\r\n                System.exit(1);\r\n            }\r\n\t\t} catch (Exception e) {\r\n\t\t\tlogger.error(\"Exception in main\", e);\r\n\t\t\tSystem.exit(1);\r\n\t\t}\r\n\t}\r\n}\r\n\r\n**Expected behavior**\r\nI would like to send the data to all hosts connected to the ovs. Please help me to take a look at the code if the source MAC and IP (of the controller) and destination MAC and IP (all hosts) are specified properly. Thanks a lot.\r\n\r\n**Desktop (please complete the following information):**\r\n - Floodlight Version: v1.2 \r\n - OS: Ubuntu 16.04.5 LTS (xenial)\r\n\r\n**Additional context**\r\nCurrently, I can not run the code in NetBeans or Eclipse, so I just put the code to the Ubuntu VM that running floodlight and mininet. It can be built successfully, but cannot be run (please see the image below). When the floodlight run, the network in mininet has not started yet.\r\n\r\n![image](https://user-images.githubusercontent.com/18657081/57807606-d4cc2000-77a4-11e9-9ba9-6fbb2949593d.png)\r\n",
          "issue_comments": [
            {
              "comment_username": "kopida",
              "comment_create_time": "2019-05-22T07:12:30Z",
              "comment_edit_time": "2019-05-22T07:56:08Z",
              "comment_text": "Hi dear,\r\n\r\nI study the error. The exception is caused by the code extracted from the link: https://floodlight.atlassian.net/wiki/spaces/floodlightcontroller/pages/9142281/How+to+Create+a+Packet+Out+Message\r\n\r\nThe code causes error:\r\n\r\nOFPacketOut po = mySwitch.getOFFactory().buildPacketOut() /* mySwitch is some IOFSwitch object */\r\n    .setData(serializedData)\r\n    .setActions(Collections.singletonList((OFAction) \r\n               mySwitch.getOFFactory().actions().output(OFPort.FLOOD, 0xffFFffFF)))\r\n    .setInPort(OFPort.CONTROLLER)\r\n    .build();\r\n  \r\nmySwitch.write(po);\r\n\r\nThe following three reasons possibly cause the above issue:\r\n      i) the IOFSwitch is an interface and we cannot directly create an object for the IOFSwitch\r\n      ii) the method write(po) for the mySwitch object has not been defined\r\n      iii) all the methods for the po (OFPacketOut) have not been defined yet\r\n\r\nCan you please advice me how to define the above methods so I can send a data from the controller to a host connected to the ovs?\r\n\r\nI really appreciate your support!\r\nChau"
            }
          ]
        },
        {
          "issue_url": "https://github.com/floodlight/floodlight/issues/820",
          "issue_title": "n.f.c.i.OFChannelHandler:New I/O worker #11-14 Error",
          "issue_number": 820,
          "issue_text": "Hi,\r\nI have run \"floodlight\" also a topology in mininet but following errors have been shown and ping not exists.\r\n`ubuntu@sdnhubvm:~[02:21]$ sudo mn --controller=remote,ip=127.0.0.1,port=6653 --switch ovsk,protocols=OpenFlow13\r\n*** Creating network\r\n*** Adding controller\r\n*** Adding hosts:\r\nh1 h2 \r\n*** Adding switches:\r\ns1 \r\n*** Adding links:\r\n(h1, s1) (h2, s1) \r\n*** Configuring hosts\r\nh1 h2 \r\n*** Starting controller\r\nc0 \r\n*** Starting 1 switches\r\ns1 ...\r\n*** Starting CLI:\r\nmininet> pingall\r\n*** Ping: testing ping reachability\r\nh1 -> X \r\nh2 -> X \r\n*** Results: 100% dropped (0/2 received)\r\n`\r\n\r\n\r\n\r\n02:53:16.409 INFO [n.f.c.i.OFChannelHandler:New I/O worker #11] [[00:00:00:00:00:00:00:01(0x0) from 127.0.0.1:45087]] Disconnected connection\r\n02:53:17.154 INFO [n.f.c.i.OFChannelHandler:New I/O worker #12] New switch connection from /127.0.0.1:45088\r\n02:53:17.238 INFO [n.f.c.i.OFSwitchHandshakeHandler:New I/O worker #12] Switch OFSwitchBase DPID[00:00:00:00:00:00:00:01] bound to class class net.floodlightcontroller.core.OFSwitch, description SwitchDescription [manufacturerDescription=Nicira, Inc., hardwareDescription=Open vSwitch, softwareDescription=2.3.90, serialNumber=None, datapathDescription=None]\r\n02:53:17.323 ERROR [n.f.c.i.OFChannelHandler:New I/O worker #12] Disconnecting switch [00:00:00:00:00:00:00:01(0x0) from 127.0.0.1:45088] due to message parse failure\r\norg.projectfloodlight.openflow.exceptions.OFParseError: Wrong length: Expected to be >= 4, was: 0\r\n\tat org.projectfloodlight.openflow.protocol.ver13.OFTableFeaturePropInstructionsVer13$Reader.readFrom(OFTableFeaturePropInstructionsVer13.java:182) ~[floodlight.jar:na]\r\n\tat org.projectfloodlight.openflow.protocol.ver13.OFTableFeaturePropVer13$Reader.readFrom(OFTableFeaturePropVer13.java:66) ~[floodlight.jar:na]\r\n\tat org.projectfloodlight.openflow.protocol.ver13.OFTableFeaturePropVer13$Reader.readFrom(OFTableFeaturePropVer13.java:37) ~[floodlight.jar:na]\r\n\tat org.projectfloodlight.openflow.util.ChannelUtils.readList(ChannelUtils.java:65) ~[floodlight.jar:na]\r\n\tat org.projectfloodlight.openflow.protocol.ver13.OFTableFeaturesVer13$Reader.readFrom(OFTableFeaturesVer13.java:435) ~[floodlight.jar:na]\r\n\tat org.projectfloodlight.openflow.protocol.ver13.OFTableFeaturesVer13$Reader.readFrom(OFTableFeaturesVer13.java:413) ~[floodlight.jar:na]\r\n\tat org.projectfloodlight.openflow.util.ChannelUtils.readList(ChannelUtils.java:65) ~[floodlight.jar:na]\r\n\tat org.projectfloodlight.openflow.protocol.ver13.OFTableFeaturesStatsReplyVer13$Reader.readFrom(OFTableFeaturesStatsReplyVer13.java:301) ~[floodlight.jar:na]\r\n\tat org.projectfloodlight.openflow.protocol.ver13.OFStatsReplyVer13$Reader.readFrom(OFStatsReplyVer13.java:101) ~[floodlight.jar:na]\r\n\tat org.projectfloodlight.openflow.protocol.ver13.OFMessageVer13$Reader.readFrom(OFMessageVer13.java:52) ~[floodlight.jar:na]\r\n\tat org.projectfloodlight.openflow.protocol.ver13.OFMessageVer13$Reader.readFrom(OFMessageVer13.java:37) ~[floodlight.jar:na]\r\n\tat net.floodlightcontroller.core.internal.OFMessageDecoder.decode(OFMessageDecoder.java:66) ~[floodlight.jar:na]\r\n\tat org.jboss.netty.handler.codec.frame.FrameDecoder.callDecode(FrameDecoder.java:425) ~[floodlight.jar:na]\r\n\tat org.jboss.netty.handler.codec.frame.FrameDecoder.messageReceived(FrameDecoder.java:310) ~[floodlight.jar:na]\r\n\tat org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70) ~[floodlight.jar:na]\r\n\tat org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564) [floodlight.jar:na]\r\n\tat org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559) [floodlight.jar:na]\r\n\tat org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:268) [floodlight.jar:na]\r\n\tat org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:255) [floodlight.jar:na]\r\n\tat org.jboss.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88) [floodlight.jar:na]\r\n\tat org.jboss.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108) [floodlight.jar:na]\r\n\tat org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:337) [floodlight.jar:na]\r\n\tat org.jboss.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89) [floodlight.jar:na]\r\n\tat org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178) [floodlight.jar:na]\r\n\tat org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108) [floodlight.jar:na]\r\n\tat org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42) [floodlight.jar:na]\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [na:1.8.0_60]\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [na:1.8.0_60]\r\n\tat java.lang.Thread.run(Thread.java:745) [na:1.8.0_60]\r\n02:53:17.349 INFO [n.f.c.i.OFChannelHandler:New I/O worker #12] [[00:00:00:00:00:00:00:01(0x0) from 127.0.0.1:45088]] Disconnected connection\r\n``\r\n",
          "issue_comments": []
        },
        {
          "issue_url": "https://github.com/floodlight/floodlight/issues/819",
          "issue_title": "n.f.c.i.OFChannelHandler:New I/O worker #11-#14 Error",
          "issue_number": 819,
          "issue_text": "Hi,\r\nI have run floodlight also a topology in mininet but following errors have been shown and ping not exists.\r\n```\r\nubuntu@sdnhubvm:~[02:21]$ sudo mn --controller=remote,ip=127.0.0.1,port=6653 --switch ovsk,protocols=OpenFlow13\r\n*** Creating network\r\n*** Adding controller\r\n*** Adding hosts:\r\nh1 h2 \r\n*** Adding switches:\r\ns1 \r\n*** Adding links:\r\n(h1, s1) (h2, s1) \r\n*** Configuring hosts\r\nh1 h2 \r\n*** Starting controller\r\nc0 \r\n*** Starting 1 switches\r\ns1 ...\r\n*** Starting CLI:\r\nmininet> pingall\r\n*** Ping: testing ping reachability\r\nh1 -> X \r\nh2 -> X \r\n*** Results: 100% dropped (0/2 received)\r\n```\r\n`ERROR [n.f.c.i.OFChannelHandler:New I/O worker #14] Disconnecting switch [00:00:00:00:00:00:00:01(0x0) from 127.0.0.1:44919] due to message parse failure\r\norg.projectfloodlight.openflow.exceptions.OFParseError: Wrong length: Expected to be >= 4, was: 0\r\n\tat org.projectfloodlight.openflow.protocol.ver13.OFTableFeaturePropInstructionsVer13$Reader.readFrom(OFTableFeaturePropInstructionsVer13.java:182) ~[floodlight.jar:na]\r\n\tat org.projectfloodlight.openflow.protocol.ver13.OFTableFeaturePropVer13$Reader.readFrom(OFTableFeaturePropVer13.java:66) ~[floodlight.jar:na]\r\n\tat org.projectfloodlight.openflow.protocol.ver13.OFTableFeaturePropVer13$Reader.readFrom(OFTableFeaturePropVer13.java:37) ~[floodlight.jar:na]\r\n\tat org.projectfloodlight.openflow.util.ChannelUtils.readList(ChannelUtils.java:65) ~[floodlight.jar:na]\r\n\tat org.projectfloodlight.openflow.protocol.ver13.OFTableFeaturesVer13$Reader.readFrom(OFTableFeaturesVer13.java:435) ~[floodlight.jar:na]\r\n\tat org.projectfloodlight.openflow.protocol.ver13.OFTableFeaturesVer13$Reader.readFrom(OFTableFeaturesVer13.java:413) ~[floodlight.jar:na]\r\n\tat org.projectfloodlight.openflow.util.ChannelUtils.readList(ChannelUtils.java:65) ~[floodlight.jar:na]\r\n\tat org.projectfloodlight.openflow.protocol.ver13.OFTableFeaturesStatsReplyVer13$Reader.readFrom(OFTableFeaturesStatsReplyVer13.java:301) ~[floodlight.jar:na]\r\n\tat org.projectfloodlight.openflow.protocol.ver13.OFStatsReplyVer13$Reader.readFrom(OFStatsReplyVer13.java:101) ~[floodlight.jar:na]\r\n\tat org.projectfloodlight.openflow.protocol.ver13.OFMessageVer13$Reader.readFrom(OFMessageVer13.java:52) ~[floodlight.jar:na]\r\n\tat org.projectfloodlight.openflow.protocol.ver13.OFMessageVer13$Reader.readFrom(OFMessageVer13.java:37) ~[floodlight.jar:na]\r\n\tat net.floodlightcontroller.core.internal.OFMessageDecoder.decode(OFMessageDecoder.java:66) ~[floodlight.jar:na]\r\n\tat org.jboss.netty.handler.codec.frame.FrameDecoder.callDecode(FrameDecoder.java:425) ~[floodlight.jar:na]\r\n\tat org.jboss.netty.handler.codec.frame.FrameDecoder.messageReceived(FrameDecoder.java:310) ~[floodlight.jar:na]\r\n\tat org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70) ~[floodlight.jar:na]\r\n\tat org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564) [floodlight.jar:na]\r\n\tat org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559) [floodlight.jar:na]\r\n\tat org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:268) [floodlight.jar:na]\r\n\tat org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:255) [floodlight.jar:na]\r\n\tat org.jboss.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88) [floodlight.jar:na]\r\n\tat org.jboss.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108) [floodlight.jar:na]\r\n\tat org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:337) [floodlight.jar:na]\r\n\tat org.jboss.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89) [floodlight.jar:na]\r\n\tat org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178) [floodlight.jar:na]\r\n\tat org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108) [floodlight.jar:na]\r\n\tat org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42) [floodlight.jar:na]\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [na:1.8.0_60]\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [na:1.8.0_60]\r\n\tat java.lang.Thread.run(Thread.java:745) [na:1.8.0_60]\r\n02:50:25.331 INFO [n.f.c.i.OFChannelHandler:New I/O worker #14] [[00:00:00:00:00:00:00:01(0x0) from 127.0.0.1:44919]] Disconnected connection\r\n`\r\n",
          "issue_comments": [
            {
              "comment_username": "geddings",
              "comment_create_time": "2019-05-11T17:15:37Z",
              "comment_edit_time": "2019-05-11T17:15:37Z",
              "comment_text": "Hi! Can we get some more info about your setup? What version of Floodlight are you running?"
            }
          ]
        },
        {
          "issue_url": "https://github.com/floodlight/floodlight/issues/818",
          "issue_title": "Hosts are not shown in floodlight web UI.",
          "issue_number": 818,
          "issue_text": "**Describe the bug**\r\nHosts are not shown in floodlight web UI.\r\n\r\n**To Reproduce**\r\nPrecondition: install vmware workstation in my windows 10. (Then create TWO vms, one is floodlight, one is mininet ovf)\r\n1. Install ubuntu 18.04 in workstation and install floodlight in this VM.\r\ngit clone git://github.com/floodlight/floodlight.git \r\n2. Delpoly mininet ubuntu ovf VM to workstation to create topology.\r\nsudo mn --topo single,3 --mac --switch ovsk --controller remote,ip=x.x.x.x,port=6633\r\n3. Access floodlight web UI\r\n\r\n**Expected behavior**\r\nOne switch and three hosts should be created and can be shown in floodlight.\r\nBut in fact, i can only see the swich, there is no hosts.\r\n\r\n**Screenshots**\r\n![switchonlyshown](https://user-images.githubusercontent.com/29721589/54742118-75fe8600-4bfb-11e9-922b-447c49e38153.PNG)\r\n\r\n\r\n**Desktop (please complete the following information):**\r\n - Floodlight Version: the latest version. (git clone git://github.com/floodlight/floodlight.git )\r\n - mininet version: mininet-2.2.2-170321-ubuntu-14.04.4-server-amd64\r\n\r\n\r\n**Additional context**\r\nmininet@mininet-vm:~$ sudo mn --topo single,3 --mac --switch ovsk --controller remote,ip=192.168.184.128,port=6653\r\n*** Creating network\r\n*** Adding controller\r\n*** Adding hosts:\r\nh1 h2 h3 \r\n*** Adding switches:\r\ns1 \r\n*** Adding links:\r\n(h1, s1) (h2, s1) (h3, s1) \r\n*** Configuring hosts\r\nh1 h2 h3 \r\n*** Starting controller\r\nc0 \r\n*** Starting 1 switches\r\ns1 ...\r\n*** Starting CLI:\r\nmininet> \r\n",
          "issue_comments": [
            {
              "comment_username": "qingwang-bsn",
              "comment_create_time": "2019-03-22T03:26:06Z",
              "comment_edit_time": "2019-03-22T03:26:06Z",
              "comment_text": "@yandong01 Can we confirm if the switch are actually connected to Floodlight? You're using openflow port 6633 in mininet, while Floodlight changes to the port 6653 a long time ago. \r\n"
            }
          ]
        },
        {
          "issue_url": "https://github.com/floodlight/floodlight/issues/816",
          "issue_title": "Floodlight forwarding module not working while floodlight load VirtualNetworkFilter module",
          "issue_number": 816,
          "issue_text": "**Describe the bug**\r\nFloodlight forwarding module not working while floodlight load VirtualNetworkFilter module\r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\n1. i run floodlight controller, `java -jar target/floodlight.jar -cf src/main/resources/neutron.properties`\r\n2. i start simple mininet topology `sudo mn --controller=remote`\r\n3. then i execute `pingall` in mininet cli\r\n4. the hosts cannot ping each other, unlike if i start with `java -jar target/floodlight.jar`, it will works\r\n\r\n**Expected behavior**\r\nI want the forwarding module work, the hosts can ping each other\r\n\r\n**Screenshots**\r\nScreenshot of neutron.properties configuration\r\n![sc_neutron_properties](https://user-images.githubusercontent.com/8055093/52397993-cc57af80-2afa-11e9-9449-5a47acc02d5c.png)\r\n\r\n**Desktop (please complete the following information):**\r\n - Floodlight Version: master\r\n - OS: Ubuntu 16.04 lts",
          "issue_comments": [
            {
              "comment_username": "geddings",
              "comment_create_time": "2019-05-11T17:19:32Z",
              "comment_edit_time": "2019-05-11T17:19:32Z",
              "comment_text": "Hi! Apologies for the delayed response. Is there a reason that you decided to use the `neutron.properties` file in particular? How were you able to confirm that the forwarding module was not working as expected?"
            }
          ]
        },
        {
          "issue_url": "https://github.com/floodlight/floodlight/issues/807",
          "issue_title": "MultiIterator#remove() in net.floodlightcontroller.util always throws a NoSuchElementException",
          "issue_number": 807,
          "issue_text": "**Describe the bug**\r\nMultiIterator#remove() in net.floodlightcontroller.util first runs remove(), then always throws a NoSuchElementException.\r\n\r\n**To Reproduce**\r\nI didn't try to reproduce.\r\n\r\n**Expected behavior**\r\nIt should not throw a NoSuchElementException if the inner iterators support removal.\r\nAlso, I'd expect the class JavaDoc to mention that only the inner iterators need to support removal to make remove() work, because it's unclear when to remove() on the outer iterator.\r\n\r\n**Desktop (please complete the following information):**\r\nmaster (https://github.com/floodlight/floodlight/commit/fef25b2cf539e6d838af982fc325ab845c25419a).\r\n",
          "issue_comments": []
        },
        {
          "issue_url": "https://github.com/floodlight/floodlight/issues/803",
          "issue_title": "Packet Streamer Service broken",
          "issue_number": 803,
          "issue_text": "I've been running through the steps [here](https://floodlight.atlassian.net/wiki/spaces/floodlightcontroller/pages/1343612/Packet+Streamer+Dev) to try to get packet streaming working. I've noticed a couple of issues.\r\n\r\nFirst, only Thrift generation for Java is part of [the build](https://github.com/floodlight/floodlight/blob/master/build.xml#L161). The [Python client example](https://github.com/floodlight/floodlight/blob/master/example/packetStreamerClientExample.py) referenced in the tutorial doesn't work without modifying the build to include the python generation.\r\n\r\nThe second issue I found (which I've been unable to work around) is that the streaming session REST API is broken. When the [example script hit the endpoint](https://github.com/floodlight/floodlight/blob/master/example/packetStreamerClientExample.py#L84), I was seeing a NullPointerException in PacketTraceResource.java. This was due to IOFMessageFilterManagerService not getting loaded. Turns out there are actually no remaining implementations to that interface in the code. The old implementation, OFMessageFilterManager.java, was [removed in 2014](https://github.com/floodlight/floodlight/commit/ace9b4), and I wasn't able to find anything that replaced it.\r\n\r\nI'm using the Floodlight VM and am up-to-date with master (only a handful of commits behind at most).\r\n",
          "issue_comments": [
            {
              "comment_username": "joaoepj",
              "comment_create_time": "2019-02-11T12:49:26Z",
              "comment_edit_time": "2019-02-11T12:49:26Z",
              "comment_text": "I have also reached this issue of broken packet trace REST API. As I am interested in helping to fix the issue, I am wondering why the OFMessageFilterManager.java was removed? (Maybe due to refactoring.) Is this service viable in the current state of the code? If yes, what are the dependencies of such a service? And, can someone provide an implementation plan?"
            }
          ]
        },
        {
          "issue_url": "https://github.com/floodlight/floodlight/issues/802",
          "issue_title": "Floodlight Forwarding module replies OFPacket-Out with 00:00:00:00:00:00 ARP",
          "issue_number": 802,
          "issue_text": "I'm implementing Floodlight (latest version) on a Linux Virtual Machine with 2 Alcatel-Lucent physical switches (OS-6450). I'm using Openflow 1.3.1.\\r\\n\\r\\nUsing the default Floodlight documentation, I see that Forwarding module is enabled by default and that this is the way to go when you want to get a functional L2 network.\\r\\n\\r\\nSo, my switches can talk with the controller and receive static flows. But, when I use a ping between two hosts on two Openflow ports, the controller receives OFPacket-IN packet with the ARP request and it responds with a OPPacket-Out, but with a destination MAC of 00:00:00:00:00:00. With Forwarding enabled by default, shouldn't be the packet Flooded? Using Wireshark on the the destination host, no arp packets are received.\\r\\n\\r\\nI've changed net.floodlightcontroller.forwarding.Forwarding.flood-arp to YES and the result is the same.\\r\\n\\r\\nCan someone please give me a hint?\\r\\n\\r\\nHere are my Floodlight settings:\\r\\n\\r\\nfloodlight.modules=\\\\\\r\\nnet.floodlightcontroller.jython.JythonDebugInterface,\\\\\\r\\nnet.floodlightcontroller.storage.memory.MemoryStorageSource,\\\\\\r\\nnet.floodlightcontroller.core.internal.FloodlightProvider,\\\\\\r\\nnet.floodlightcontroller.threadpool.ThreadPool,\\\\\\r\\nnet.floodlightcontroller.debugcounter.DebugCounterServiceImpl,\\\\\\r\\nnet.floodlightcontroller.perfmon.PktInProcessingTime,\\\\\\r\\nnet.floodlightcontroller.staticentry.StaticEntryPusher,\\\\\\r\\nnet.floodlightcontroller.restserver.RestApiServer,\\\\\\r\\nnet.floodlightcontroller.topology.TopologyManager,\\\\\\r\\nnet.floodlightcontroller.routing.RoutingManager,\\\\\\r\\nnet.floodlightcontroller.forwarding.Forwarding,\\\\\\r\\nnet.floodlightcontroller.linkdiscovery.internal.LinkDiscoveryManager,\\\\\\r\\nnet.floodlightcontroller.ui.web.StaticWebRoutable,\\\\\\r\\nnet.floodlightcontroller.loadbalancer.LoadBalancer,\\\\\\r\\nnet.floodlightcontroller.firewall.Firewall,\\\\\\r\\nnet.floodlightcontroller.dhcpserver.DHCPServer,\\\\\\r\\nnet.floodlightcontroller.simpleft.FT,\\\\\\r\\nnet.floodlightcontroller.devicemanager.internal.DeviceManagerImpl,\\\\\\r\\nnet.floodlightcontroller.accesscontrollist.ACL,\\\\\\r\\nnet.floodlightcontroller.statistics.StatisticsCollector,\\\\\\r\\nnet.floodlightcontroller.hasupport.HAController\\r\\norg.sdnplatform.sync.internal.SyncManager.authScheme=CHALLENGE_RESPONSE\\r\\norg.sdnplatform.sync.internal.SyncManager.keyStorePath=/etc/floodlight/myKey.jceks\\r\\norg.sdnplatform.sync.internal.SyncManager.dbPath=/var/lib/floodlight/\\r\\norg.sdnplatform.sync.internal.SyncManager.keyStorePassword=syncPass\\r\\norg.sdnplatform.sync.internal.SyncManager.port=6642\\r\\norg.sdnplatform.sync.internal.SyncManager.thisNodeId=1\\r\\norg.sdnplatform.sync.internal.SyncManager.persistenceEnabled=FALSE\\r\\norg.sdnplatform.sync.internal.SyncManager.nodes=[\\\\\\r\\n{\"nodeId\": 1, \"domainId\": 1, \"hostname\": \"192.168.56.1\", \"port\": 6642},\\\\\\r\\n{\"nodeId\": 2, \"domainId\": 1, \"hostname\": \"192.168.56.1\", \"port\": 6643},\\\\\\r\\n{\"nodeId\": 3, \"domainId\": 1, \"hostname\": \"192.168.56.1\", \"port\": 6644},\\\\\\r\\n{\"nodeId\": 4, \"domainId\": 1, \"hostname\": \"192.168.56.1\", \"port\": 6645}\\\\\\r\\n]\\r\\nnet.floodlightcontroller.forwarding.Forwarding.match=in-port, vlan, mac, ip, transport, flag\\r\\nnet.floodlightcontroller.forwarding.Forwarding.detailed-match=src-mac, dst-mac, src-ip, dst-ip, src-transport, dst-transport\\r\\nnet.floodlightcontroller.forwarding.Forwarding.flood-arp=YES\\r\\nnet.floodlightcontroller.forwarding.Forwarding.idle-timeout=5\\r\\nnet.floodlightcontroller.forwarding.Forwarding.set-send-flow-rem-flag=FALSE\\r\\nnet.floodlightcontroller.forwarding.Forwarding.remove-flows-on-link-or-port-down=TRUE\\r\\nnet.floodlightcontroller.core.internal.FloodlightProvider.openFlowPort=6653\\r\\nnet.floodlightcontroller.core.internal.FloodlightProvider.role=ACTIVE\\r\\nnet.floodlightcontroller.core.internal.FloodlightProvider.controllerId=1\\r\\nnet.floodlightcontroller.linkdiscovery.internal.LinkDiscoveryManager.latency-history-size=10\\r\\nnet.floodlightcontroller.linkdiscovery.internal.LinkDiscoveryManager.latency-update-threshold=0.5\\r\\nnet.floodlightcontroller.core.internal.FloodlightProvider.shutdownOnTransitionToStandby=true\\r\\nnet.floodlightcontroller.core.internal.OFSwitchManager.openFlowPort=6653\\r\\nnet.floodlightcontroller.core.internal.OFSwitchManager.openFlowAddresses=0.0.0.0\\r\\nnet.floodlightcontroller.core.internal.OFSwitchManager.workerThreads=16\\r\\nnet.floodlightcontroller.core.internal.OFSwitchManager.bossThreads=1\\r\\nnet.floodlightcontroller.core.internal.OFSwitchManager.connectionBacklog=1000\\r\\nnet.floodlightcontroller.core.internal.OFSwitchManager.connectionTimeoutMs=60000\\r\\nnet.floodlightcontroller.core.internal.OFSwitchManager.defaultMaxTablesToReceiveTableMissFlow=1\\r\\nnet.floodlightcontroller.core.internal.OFSwitchManager.maxTablesToReceiveTableMissFlowPerDpid={\"00:00:00:00:00:00:00:01\":\"1\",\"2\":\"1\"}\\r\\nnet.floodlightcontroller.core.internal.OFSwitchManager.clearTablesOnInitialHandshakeAsMaster=YES\\r\\nnet.floodlightcontroller.core.internal.OFSwitchManager.clearTablesOnEachTransitionToMaster=YES\\r\\nnet.floodlightcontroller.core.internal.OFSwitchManager.keyStorePath=/path/to.jecks\\r\\nnet.floodlightcontroller.core.internal.OFSwitchManager.keyStorePassword=PassFL\\r\\nnet.floodlightcontroller.core.internal.OFSwitchManager.useSsl=NO\\r\\nnet.floodlightcontroller.core.internal.OFSwitchManager.supportedOpenFlowVersions=1.3\\r\\nnet.floodlightcontroller.core.internal.OFSwitchManager.switchesInitialState={\"00:00:00:00:00:00:00:01\":\"ROLE_MASTER\",\"00:00:00:00:00:00:00:02\":\"ROLE_MASTER\", \"00:00:00:00:00:00:00:03\":\"ROLE_MASTER\", \"00:00:00:00:00:00:00:04\":\"ROLE_MASTER\",\"00:00:00:00:00:00:00:05\":\"ROLE_MASTER\",\"00:00:00:00:00:00:00:06\":\"ROLE_MASTER\",\"00:00:00:00:00:00:00:07\":\"ROLE_MASTER\",\"00:00:00:00:00:00:00:08\":\"ROLE_MASTER\"}\\r\\nnet.floodlightcontroller.restserver.RestApiServer.keyStorePath=/path/to.jceks\\r\\nnet.floodlightcontroller.restserver.RestApiServer.keyStorePassword=Password\\r\\nnet.floodlightcontroller.restserver.RestApiServer.httpsNeedClientAuthentication=NO\\r\\nnet.floodlightcontroller.restserver.RestApiServer.useHttps=NO\\r\\nnet.floodlightcontroller.restserver.RestApiServer.useHttp=YES\\r\\nnet.floodlightcontroller.restserver.RestApiServer.httpsPort=8081\\r\\nnet.floodlightcontroller.restserver.RestApiServer.httpPort=8080\\r\\nnet.floodlightcontroller.restserver.RestApiServer.accessControlAllowAllOrigins=TRUE\\r\\nnet.floodlightcontroller.statistics.StatisticsCollector.enable=FALSE\\r\\nnet.floodlightcontroller.statistics.StatisticsCollector.collectionIntervalPortStatsSeconds=10\\r\\nnet.floodlightcontroller.topology.TopologyManager.pathMetric=latency\\r\\nnet.floodlightcontroller.topology.TopologyManager.maxPathsToCompute=3\\r\\nnet.floodlightcontroller.hasupport.HAController.nodeid=1\\r\\nnet.floodlightcontroller.hasupport.HAController.serverPort=127.0.0.1:4242\\r\\n",
          "issue_comments": [
            {
              "comment_username": "decosvaldo",
              "comment_create_time": "2018-09-17T14:24:56Z",
              "comment_edit_time": "2018-09-17T14:24:56Z",
              "comment_text": "Floodlight is having problem to talk with Alcatel when trying to flood. I'm also receiving this information:\r\n\r\nCould not locate broadcast ports for switch \r\nNo broadcast ports found. Using FLOOD output action\r\nAnd flood packets are marked as invalid"
            }
          ]
        },
        {
          "issue_url": "https://github.com/floodlight/floodlight/issues/801",
          "issue_title": "Switch connects to two floodlight controller problem",
          "issue_number": 801,
          "issue_text": "**Switch Connection problem with two controller**\\r\\nWhen I connect one switch (e.g. s1) to two controller (e.g. c0 and c1) with below mininet script one of controllers connects correctly and doing fine but another controller gives below error: \\r\\n\\r\\n> 2018-09-12 18:43:31.912 ERROR [n.f.c.i.OFSwitchHandshakeHandler] OFBadRequestErrorMsgVer13(xid=311, code=IS_SLAVE, data=[unparsed: 04 0d 00 7b 00 00 01 37 ff ff ff ff ff ff ff fd 00 10 00 00 00 00 00 00 00 00 00 10 00 00 00 01 00 00 00 00 00 00 00 00 ff ff ff ff ff ff 56 26 0e f2 78 b7 89 42 20 00 06 04 00 02 00 00 02 07]) from switch OFSwitch DPID[00:00:00:00:00:00:00:01] in state net.floodlightcontroller.core.internal.OFSwitchHandshakeHandler$MasterState@28c71a82\\r\\n> 2018-09-12 18:43:31.914 INFO  [n.f.l.LearningSwitch] received an error OFBadRequestErrorMsgVer13(xid=311, code=IS_SLAVE, data=[unparsed: 04 0d 00 7b 00 00 01 37 ff ff ff ff ff ff ff fd 00 10 00 00 00 00 00 00 00 00 00 10 00 00 00 01 00 00 00 00 00 00 00 00 ff ff ff ff ff ff 56 26 0e f2 78 b7 89 42 20 00 06 04 00 02 00 00 02 07]) from switch OFSwitch DPID[00:00:00:00:00:00:00:01]\\r\\n> 2018-09-12 18:43:31.917 ERROR [n.f.c.i.OFSwitchHandshakeHandler] OFBadRequestErrorMsgVer13(xid=312, code=IS_SLAVE, data=[unparsed: 04 0d 00 7b 00 00 01 38 ff ff ff ff ff ff ff fd 00 10 00 00 00 00 00 00 00 00 00 10 00 00 00 02 00 00 00 00 00 00 00 00 ff ff ff ff ff ff d6 14 80 a5 91 2e 89 42 20 00 06 04 00 02 00 00 02 07]) from switch OFSwitch DPID[00:00:00:00:00:00:00:01] in state net.floodlightcontroller.core.internal.OFSwitchHandshakeHandler$MasterState@28c71a82\\r\\n> 2018-09-12 18:43:31.919 INFO  [n.f.l.LearningSwitch] received an error OFBadRequestErrorMsgVer13(xid=312, code=IS_SLAVE, data=[unparsed: 04 0d 00 7b 00 00 01 38 ff ff ff ff ff ff ff fd 00 10 00 00 00 00 00 00 00 00 00 10 00 00 00 02 00 00 00 00 00 00 00 00 ff ff ff ff ff ff d6 14 80 a5 91 2e 89 42 20 00 06 04 00 02 00 00 02 07]) from switch OFSwitch DPID[00:00:00:00:00:00:00:01]\\r\\n\\r\\n**To Reproduce**\\r\\nUse below python script to connect one switch to two floodlight controller. (Consider changing IP addresses.) \\r\\n\\r\\n\\r\\n```\\r\\n#!/usr/bin/python\\r\\n\\r\\nfrom mininet.net import Mininet\\r\\nfrom mininet.node import Controller, OVSSwitch, RemoteController\\r\\nfrom mininet.cli import CLI\\r\\nfrom mininet.log import setLogLevel, info\\r\\n\\r\\ndef multiControllerNet():\\r\\n    \"Create a network from semi-scratch with multiple controllers.\"\\r\\n\\r\\n    net = Mininet( controller=Controller, switch=OVSSwitch )\\r\\n\\r\\n    info( \"*** Creating (reference) controllers\\\\n\" )\\r\\n    c0 = RemoteController( 'c0', ip='172.18.0.10', port=6653 )\\r\\n    c1 = RemoteController( 'c1', ip='172.18.0.11', port=6653 )\\r\\n\\r\\n    info( \"*** Creating switches\\\\n\" )\\r\\n    s1 = net.addSwitch( 's1' )\\r\\n\\r\\n    info( \"*** Creating hosts\\\\n\" )\\r\\n    hosts1 = [ net.addHost( 'h%d' % n ) for n in ( 1, 2 ) ]\\r\\n\\r\\n    info( \"*** Creating links\\\\n\" )\\r\\n    for h in hosts1:\\r\\n        net.addLink( s1, h )\\r\\n\\r\\n    info( \"*** Starting network\\\\n\" )\\r\\n    net.build()\\r\\n    c0.start()\\r\\n    c1.start()\\r\\n    s1.start( [ c0, c1] )\\r\\n\\r\\n    info( \"*** Running CLI\\\\n\" )\\r\\n    CLI( net )\\r\\n\\r\\n    info( \"*** Stopping network\\\\n\" )\\r\\n    net.stop()\\r\\n\\r\\nif __name__ == '__main__':\\r\\n    setLogLevel( 'info' )  # for CLI output\\r\\n    multiControllerNet()\\r\\n```\\r\\n\\r\\n**Expected behavior**\\r\\nI expect switch connects to both controllers without any errors.\\r\\n\\r\\n\\r\\n**I used**\\r\\n - Floodlight Version : master, last version\\r\\n - OS: used openjdk:8-jre-slim as a base docker\\r\\n\\r\\n",
          "issue_comments": [
            {
              "comment_username": "qingwang-bsn",
              "comment_create_time": "2018-10-02T15:00:54Z",
              "comment_edit_time": "2018-10-02T15:00:54Z",
              "comment_text": "Looks like there is something send from that controller when it is set to slave role. Could you use wireshark to verify? "
            },
            {
              "comment_username": "qingwang-bsn",
              "comment_create_time": "2018-10-08T04:15:48Z",
              "comment_edit_time": "2018-10-08T04:15:48Z",
              "comment_text": "If you see the error message printing for every 15 secs, then it might be the LLDP packets that Floodlight sent periodically to discover the link. In Floodlight log, you can also check if those error message is immediately printed after the LLDP packet information printing in the log. If this is the case, then those errors should be safe to ignore at this point. Although we should fix it to stop Floodlight in slave mode sending LLDP. "
            },
            {
              "comment_username": "quihp",
              "comment_create_time": "2019-05-11T07:55:07Z",
              "comment_edit_time": "2019-05-11T07:55:07Z",
              "comment_text": "I got the error as well. I could'nt pingall after run my topology. Only 3 switches (s4,s5,s6) which are managed by contrller 2 can ping each other. The rest of switches (s1,s2,s3) which are manged by controller 1 cannot ping each other.  \\r\\nI hope every hosts in each switch can ping each other. \\r\\nMay you help me or explain which errors that I had made? \\r\\nThanks in advanced.\\r\\nMy topology looks like this.\\r\\n![image](https://user-images.githubusercontent.com/38786683/57566792-814f8000-73fa-11e9-8828-fabc8b9a4c9a.png)\\r\\nI set switchesInitialState of s1,s2,s3 as ROLE MASTER  and s4,s5,s6 as ROLE SLAVE in Floodlight Controller 1 and vice versa, s4,s5,s6 as ROLE MASTER; s1,s2,s3 as ROLE SLAVE in Floodlight Controller 2. \\r\\nI modified the floodlightdefault.propertises file in every controller like this \\r\\nnet.floodlightcontroller.core.internal.OFSwitchManager.switchesInitialState={\"00:00:00:00:00:00:00:01\":\"ROLE_MASTER\",\"00:00:00:00:00:00:00:02\":\"ROLE_MASTER\", \"00:00:00:00:00:00:00:03\":\"ROLE_MASTER\"}\\r\\nnet.floodlightcontroller.core.internal.OFSwitchManager.switchesInitialState={ \"00:00:00:00:00:00:00:04\":\"ROLE_SLAVE\",\"00:00:00:00:00:00:00:05\":\"ROLE_SLAVE\",\"00:00:00:00:00:00:00:06\":\"ROLE_SLAVE\"}\\r\\n\\r\\nPython script: \\r\\n#!/usr/bin/python\\r\\n\\r\\nfrom mininet.net import Mininet\\r\\nfrom mininet.node import Controller, RemoteController, OVSController\\r\\nfrom mininet.node import CPULimitedHost, Host, Node\\r\\nfrom mininet.node import OVSKernelSwitch, UserSwitch\\r\\nfrom mininet.node import IVSSwitch\\r\\nfrom mininet.cli import CLI\\r\\nfrom mininet.log import setLogLevel, info\\r\\nfrom mininet.link import TCLink, Intf\\r\\nfrom subprocess import call\\r\\n\\r\\ndef myNetwork():\\r\\n\\r\\n    net = Mininet( topo=None,\\r\\n                   build=False,\\r\\n                   ipBase='10.0.0.0/8')\\r\\n\\r\\n    info( '*** Adding controller\\\\n' )\\r\\n    c2=net.addController(name='c1',\\r\\n                      controller=RemoteController,\\r\\n                      ip='10.102.11.249',\\r\\n                      protocol='tcp',\\r\\n                      port=6653)\\r\\n\\r\\n    c1=net.addController(name='c2',\\r\\n                      controller=RemoteController,\\r\\n                      ip='10.102.11.247',\\r\\n                      protocol='tcp',\\r\\n                      port=6653)\\r\\n\\r\\n    info( '*** Add switches\\\\n')\\r\\n    s2 = net.addSwitch('s2', dpid='00:00:00:00:00:00:00:02')\\r\\n    s4 = net.addSwitch('s4', dpid='00:00:00:00:00:00:00:04')\\r\\n    s3 = net.addSwitch('s3',  dpid='00:00:00:00:00:00:00:03')\\r\\n    s6 = net.addSwitch('s6',  dpid='00:00:00:00:00:00:00:06')\\r\\n    s5 = net.addSwitch('s5',  dpid='00:00:00:00:00:00:00:05')\\r\\n    s1 = net.addSwitch('s1',  dpid='00:00:00:00:00:00:00:01')\\r\\n\\r\\n    info( '*** Add hosts\\\\n')\\r\\n    h10 = net.addHost('h10', cls=Host, ip='10.0.0.10', defaultRoute=None)\\r\\n    h12 = net.addHost('h12', cls=Host, ip='10.0.0.12', defaultRoute=None)\\r\\n    h11 = net.addHost('h11', cls=Host, ip='10.0.0.11', defaultRoute=None)\\r\\n    h1 = net.addHost('h1', cls=Host, ip='10.0.0.1', defaultRoute=None)\\r\\n    h4 = net.addHost('h4', cls=Host, ip='10.0.0.4', defaultRoute=None)\\r\\n    h2 = net.addHost('h2', cls=Host, ip='10.0.0.2', defaultRoute=None)\\r\\n    h3 = net.addHost('h3', cls=Host, ip='10.0.0.3', defaultRoute=None)\\r\\n    h8 = net.addHost('h8', cls=Host, ip='10.0.0.8', defaultRoute=None)\\r\\n    h5 = net.addHost('h5', cls=Host, ip='10.0.0.5', defaultRoute=None)\\r\\n    h6 = net.addHost('h6', cls=Host, ip='10.0.0.6', defaultRoute=None)\\r\\n    h9 = net.addHost('h9', cls=Host, ip='10.0.0.9', defaultRoute=None)\\r\\n    h7 = net.addHost('h7', cls=Host, ip='10.0.0.7', defaultRoute=None)\\r\\n\\r\\n    info( '*** Add links\\\\n')\\r\\n    net.addLink(s4, h7)\\r\\n    net.addLink(s4, h8)\\r\\n    net.addLink(s5, h9)\\r\\n    net.addLink(s5, h10)\\r\\n    net.addLink(s6, h11)\\r\\n    net.addLink(s6, h12)\\r\\n    net.addLink(s1, s2)\\r\\n    net.addLink(s2, s3)\\r\\n    net.addLink(s3, s4)\\r\\n    net.addLink(s4, s5)\\r\\n    net.addLink(s5, s6)\\r\\n    net.addLink(s1, h1)\\r\\n    net.addLink(s1, h2)\\r\\n    net.addLink(s2, h3)\\r\\n    net.addLink(s2, h4)\\r\\n    net.addLink(s3, h5)\\r\\n    net.addLink(s3, h6)\\r\\n\\r\\n    info( '*** Starting network\\\\n')\\r\\n    net.build()\\r\\n    info( '*** Starting controllers\\\\n')\\r\\n    for controller in net.controllers:\\r\\n        controller.start()\\r\\n\\r\\n    info( '*** Starting switches\\\\n')\\r\\n    net.get('s2').start([c1,c2])\\r\\n    net.get('s4').start([c1,c2])\\r\\n    net.get('s3').start([c1,c2])\\r\\n    net.get('s6').start([c1,c2])\\r\\n    net.get('s5').start([c1,c2])\\r\\n    net.get('s1').start([c1,c2])\\r\\n\\r\\n    info( '*** Post configure switches and hosts\\\\n')\\r\\n\\r\\n    CLI(net)\\r\\n    net.stop()\\r\\n\\r\\nif __name__ == '__main__':\\r\\n    setLogLevel( 'info' )\\r\\n    myNetwork()\\r\\n"
            }
          ]
        },
        {
          "issue_url": "https://github.com/floodlight/floodlight/issues/799",
          "issue_title": "Add Generic Support for Multicasting",
          "issue_number": 799,
          "issue_text": "This PR includes a generic Multicasting Module and some bug fixes and optimization in TopologyInstance without breaking the code.\r\n\r\n## Description\r\nA Multicasting module is supposed to be able to form paths from any source to any set of destinations. This module does the same. Here, a destination primary is denoted by a Multicast Address and represents a set of devices. It could contain any device attached to any port of any switch. Hence, it's denoted by a **MacVlanPair** and a set of **attachmentPoints**. While attachmentPoints are enough to determine destinations, we also require to make sure when the attachmentPoints become no longer valid. For this, we have a provisioning in DeviceManager's service that can tell when a device is moved (change in attachmentPoints) or removed (either attachmentPoint switch goes down, or port goes down). Hence, we require a way to uniquely identify devices and as per documentation, a pair of MacAddress and VlanId (MacVlanPair) is perfect for that purpose. Additionally, it also checks if when a Vlan changes for a device.\r\n\r\nIt provides 2 interfaces: \r\n1. **IMulticastListener**: This provides the callback methods for when a Participant is/are added/removed.\r\n2. **IMulticastService**: This provides basic functionality to manage the Participants as well as register listeners for IMulticastListener.\r\n\r\nFor **deviceMoved** scenario, it has the ability to determine, if the moved attachmentPoint is valid or not (belongs to the same OpenFlow Island). It does so by using TopologyManager's service to map attachmentPoints based on their OpenFlow Island (Strong connected component of switches or Clusters). That way, it updates the table entry.\r\n\r\nFor **deviceRemoved** scenario, it simply removes every entry for each MacVlanPair associated with the device from the table.\r\n\r\nFor **deviceVlanChanged** changed scenario, it checks if a VlanId associated with the device is a participant and if it is no longer present, then prunes each matching entry accordingly from the table.\r\n\r\nIt create a **ParticipantTable** to register devices. Each participant comprises of a **groupAddress** _(Multicast IPAddress)_, a **set of devices** _(MacVlanPair)_ for that groupAddress and a **set of attachmentPoints** _(SwitchPort/NodePortTuple)_ for each group (**groupAddress** + **device**).\r\n\r\n### Module Dependencies:\r\n1. IDeviceService\r\n2. ITopologyService\r\n\r\n### Module Services:\r\n1. IMulticastService\r\n\r\n## At Topology:\r\n\r\n1. Implements **IMulticastListener** and uses **IMulticastService** to add itself as a listener to monitor changes in Participants and propagate them to current **TopologyInstance**.\r\n2. **TopologyInstance** uses **MulticastGroup** to form **MulticastPath** and caches them for use by other modules.\r\n3. A **MulticastGroup** holds participant device _(MacVlanPair)_ to attachmentPoints and Switches mappings per Archipelago. This gets updated in real-time for the same **TopologyInstance** and is used for:\r\n4. A **MulticastGroup** is identified by a **MulticastGroupId** which holds a token made of **Group\r\n4. A **MulticastPath** holds paths, that form the trunks and branches of a tree that is rooted at source switch and edges out at destination switches. It's made using **MulticastGroup** but is not affected in real-time and is used for exporting to other modules, such as Forwarding via **getMulticastPath()** method.\r\n5. **IRoutingService** for **TopologyManager** provides **getMulticastPath()** method, which is used by **Forwarding** module for making Multicast forwarding decisions.\r\n\r\n### Bug Fixes & Optimizations:\r\n1. Fixed a bug in **identifyArchipelagos()** where in the base case, **archipelagoFromCluster** wasn't getting updated.\r\n2. Optimized **getArchipelago(DatapathId)** method.\r\n3. Changed hashCode and equals methods in Archipelago to not include clusters as 2 archipelagos can't have the same or in fact any common cluster(s) at any state of the current **TopologyInstance** of **TopologyManager**. Even in the method **identifyArchipelagos()**, where it attempts to merge the 2 sets, each of them is bipartite already.\r\n4. Reverted **archipelagos** to use Set instead of List because when we need to change a key in HashSet, we are supposed to remove the key, change it and add it back. All that can happen in O(1). We don't need to use a List in that case.\r\n5. Add missing mock **IMulticastService** to **FloodlightModuleContext** in Topology & Link Discovery tests.\r\n6. Fixed a bug with **computeBroadcastPortsPerArchipelago()** in getting archipelagoId of a switch that is not connected to any other switch.\r\n\r\n## At Forwarding:\r\n\r\n1. Reduce **FLOWSET_BITS** from 28 to 24 to match usable bits from **OFPG_MAX** in Forwarding for matching 1-to-1, **flowSetId** of cookies in **OFFlowMods** with **groupId** in **OFGroupMods** during **FLOW_REMOVED**. _This needs to be done because Groups in GroupTable don't expire unlike Flows in FlowTable._ \r\n**Note:** We can't keep a map between them inside the controller because, in case the controller fails, HA can help remove the Groups even without a map. Also, we can't save Groups inside the switches permanently because flowSetId will keep changing for every packet received at the controller that needs to be forwarded and each flow is temporary, with limited idle_timeout.\r\n2. **doMulticast()** method is called upon receiving a Multicast packet, both for L2/L3 and IPv4/IPv6 and others.\r\n3. **pushMulticastFlow()** method uses **MulticastPath** to generate an inPort and outPorts for every switch in path, leading to attachmentPoints in O(n) time complexity. Then for each switch in path:\r\n\t1. A **PacketOut** message to only attachmentPoint ports that leads to destination devices is created.\r\n\t2. An **OFGroupMod** message with groupId set to flowSetId and bucket actions, each leading to an outPort of the switch is created.\r\n\t3. An **OFFlowMod** message switch input match, priority, timeout, SEND_FLOW_REM enabled and actionGroup pointing to groupId is created.\r\n\t4. All 3 messages are sent together to the switch.\r\n4. **IOFMessageListener** interface provides **FLOW_REMOVED** callback to help remove associated Groups from GroupTable using flowSetId/groupId from cookie.\r\n\r\n## Motivation and Context\r\n<!--- Why is this change required? What problem does it solve? -->\r\nThis solves the missing generic Multicasting feature. Sure, a flow could be manually pushed to implement the same but that would be static and won't reflect actual topology and changes. Moreover, that won't do On-The-Go multicasting or support any dynamic multicasting protocol like IGMP or DVMRP. This, module however can.\r\n<!--- If it fixes an open issue, please link to the issue here. -->\r\n\r\n### Use-cases\r\n1. Use **IGMP**/similar protocols or even external means like **REST API** for registering participants and deciding when to multicast and when not to. eg. \r\n\t1. It could be based on packet-in mechanism of forwarding module. In that case, we would tell the protocol module to imply **MULTICAST** decision to context, so that **doMulticast()** method of forwarding can do the rest by itself. (Fully automated using forwarding default: match, add and removal)\r\n\t2. In case we want to create our own match fields and provide it, we may first add the participant, then use **getMulticastPath()** method to retrieve the path, and finally install the flow using **pushMulticastFlow()**, with custom match and any other parameter (to be reviewed) including request to remove flow.\r\n2. De-register participants will remove path from topology but to force remove it from switch, before timeout, we may use **pushMulticastFlow()** with action set to **DELETE** for flowmod.\r\n3. In case we don't want a proactive approach, like in case of **DVMRP**, we would make sure that upon adding to participant table, the same is not propagated to topology (TODO). Instead we may learn the inPort and outPorts for different multicast groups per switch in real-time, create a new **MulticastPath** for that particular switch using learned information and provide it statically to **pushMulticastFlow()** method along with custom match. _For every update, it will replace the entry on the switch's table._\r\n4. In case we want to simply push custom multicast flows statically, we can always do that using pushMulticastFlow() method and also remove the same via **DELETE**.\r\n\r\n## How Has This Been Tested?\r\n<!--- Please describe in detail how you tested your changes. -->\r\n1. I created a simple IGMP Manager module that can listen to IGMPv3 group membership messages and accordingly, create and remove participants in the table. The branch is available here:\r\nhttps://github.com/souvikdas95/floodlight/tree/igmpmanager-master\r\n2. I created a verbose Multicast VideoStreaming application over Mininet that can create various topologies and parallely stream various media over various sources and destinations. Its details and test environment used, can be found here: https://github.com/souvikdas95/SDN_VideoStreaming\r\n<!--- Include details of your testing environment, and the tests you ran to -->\r\n<!--- see how your change affects other areas of the code, etc. -->\r\n\r\n## Types of changes\r\n<!--- What types of changes does your code introduce? Put an `x` in all the boxes that apply: -->\r\n- [x] Bug fix (non-breaking change which fixes an issue)\r\n- [x] New feature (non-breaking change which adds functionality)\r\n- [ ] Breaking change (fix or feature that would cause existing functionality to change)\r\n\r\n## Checklist\r\n<!--- Go over all the following points, and put an `x` in all the boxes that apply. -->\r\n<!--- If you're unsure about any of these, don't hesitate to ask. We're here to help! -->\r\n- [x] My code follows the code style of this project.\r\n- [x] My change requires a change to the documentation.\r\n- [ ] Remove hardcodes and finalize reviews.\r\n- [ ] I have updated the documentation accordingly.\r\n- [ ] I have read the **CONTRIBUTING** document.\r\n- [ ] I have added tests to cover my changes.\r\n- [ ] All new and existing tests passed.\r\n",
          "issue_comments": [
            {
              "comment_username": "qingwang-bsn",
              "comment_create_time": "2018-09-03T16:14:16Z",
              "comment_edit_time": "2018-09-03T16:14:16Z",
              "comment_text": "@souvikdas95 thanks for your contribution! Looks like your PR has the conflicting files, do you mind to resolve those? "
            },
            {
              "comment_username": "rizard",
              "comment_create_time": "2019-04-25T12:32:03Z",
              "comment_edit_time": "2019-04-25T12:32:03Z",
              "comment_text": "This is very cool @souvikdas95. Thank you for your contribution! Would you also consider opening another pull request with your example IGMP manager module and adding some documentation/tutorials to our wiki?https://floodlight.atlassian.net/wiki/spaces/floodlightcontroller/overview\r\n\r\nI feel many will find this feature useful, but sometimes a little documentation is required to show how to use it (or how it's automatically used) and to advertise it as a feature.\r\n"
            }
          ]
        },
        {
          "issue_url": "https://github.com/floodlight/floodlight/issues/798",
          "issue_title": "Adding a CONTRIBUTING file",
          "issue_number": 798,
          "issue_text": "**Is your feature request related to a problem? Please describe.**\r\nFloodlight needs a CONTRIBUTING file in order to help detail the contribution steps (opening an issue, opening a PR, code style etc.) for future contributors. This helps make would-be contributors feel a lot more comfortable and it keeps the entire process consistent. It also would act as an easy place to point to when trying to help explain the contribution process.\r\n\r\n**Describe the solution you'd like**\r\nA write up that's similar to the one outlined here: \r\nhttps://opensource.guide/starting-a-project/#writing-your-contributing-guidelines",
          "issue_comments": []
        }
      ]
    }
  },
  {
    "github_url": "https://github.com/jhy/jsoup",
    "github_info": {
      "name": "jhy/jsoup",
      "language": "Java"
    },
    "github_branches": {
      "branch_datas": [
        {
          "branch_version": "1285-patch-support",
          "branch_url": "https://github.com/jhy/jsoup/tree/1285-patch-support",
          "branch_download_url": "https://github.com/jhy/jsoup/archive/1285-patch-support.zip"
        },
        {
          "branch_version": "master",
          "branch_url": "https://github.com/jhy/jsoup/tree/master",
          "branch_download_url": "https://github.com/jhy/jsoup/archive/master.zip"
        }
      ]
    },
    "github_pull_requests": {
      "pull_datas": [
        {
          "pull_number": 1427,
          "pull_title": "Add @SafeHtml java validator",
          "pull_version": "jhy:master",
          "pull_version_url": "https://github.com/jhy/jsoup/tree/master"
        },
        {
          "pull_number": 1426,
          "pull_title": "扩展图片src和a标签href的安全校验",
          "pull_version": "jhy:master",
          "pull_version_url": "https://github.com/jhy/jsoup/tree/master"
        },
        {
          "pull_number": 1423,
          "pull_title": "Avoid the term whitelist",
          "pull_version": "jhy:master",
          "pull_version_url": "https://github.com/jhy/jsoup/tree/master"
        },
        {
          "pull_number": 1403,
          "pull_title": "Fix regression on Elements.forms()",
          "pull_version": "jhy:master",
          "pull_version_url": "https://github.com/jhy/jsoup/tree/master"
        },
        {
          "pull_number": 1402,
          "pull_title": "improve StringBuilder cache performance under high concurrency",
          "pull_version": "jhy:master",
          "pull_version_url": "https://github.com/jhy/jsoup/tree/master"
        },
        {
          "pull_number": 1396,
          "pull_title": "jsoup issue #1173 fix code commit",
          "pull_version": "jhy:master",
          "pull_version_url": "https://github.com/jhy/jsoup/tree/master"
        },
        {
          "pull_number": 1395,
          "pull_title": "jsoup issue #1289 fix code commit",
          "pull_version": "jhy:master",
          "pull_version_url": "https://github.com/jhy/jsoup/tree/master"
        },
        {
          "pull_number": 1394,
          "pull_title": "Fix NullPointerException on standalone elements",
          "pull_version": "jhy:master",
          "pull_version_url": "https://github.com/jhy/jsoup/tree/master"
        },
        {
          "pull_number": 1393,
          "pull_title": "jsoup issue #1035 fie code commit",
          "pull_version": "jhy:master",
          "pull_version_url": "https://github.com/jhy/jsoup/tree/master"
        },
        {
          "pull_number": 1392,
          "pull_title": "Jsoup issue#784 commit",
          "pull_version": "jhy:master",
          "pull_version_url": "https://github.com/jhy/jsoup/tree/master"
        },
        {
          "pull_number": 1391,
          "pull_title": "Fix incorrect template element xml serialization",
          "pull_version": "jhy:master",
          "pull_version_url": "https://github.com/jhy/jsoup/tree/master"
        },
        {
          "pull_number": 1390,
          "pull_title": "Fix the issue 1006",
          "pull_version": "jhy:master",
          "pull_version_url": "https://github.com/jhy/jsoup/tree/master"
        },
        {
          "pull_number": 1389,
          "pull_title": "fix issue 1294",
          "pull_version": "jhy:master",
          "pull_version_url": "https://github.com/jhy/jsoup/tree/master"
        },
        {
          "pull_number": 1387,
          "pull_title": "[Feature]Add  method Elements.Befor(Node node)",
          "pull_version": "jhy:master",
          "pull_version_url": "https://github.com/jhy/jsoup/tree/master"
        },
        {
          "pull_number": 1386,
          "pull_title": "add a new Exception for NPE after replaceWith in Node visitor",
          "pull_version": "jhy:master",
          "pull_version_url": "https://github.com/jhy/jsoup/tree/master"
        },
        {
          "pull_number": 1383,
          "pull_title": "Fix for issue #876",
          "pull_version": "jhy:master",
          "pull_version_url": "https://github.com/jhy/jsoup/tree/master"
        },
        {
          "pull_number": 1381,
          "pull_title": "Update dependencies",
          "pull_version": "jhy:master",
          "pull_version_url": "https://github.com/jhy/jsoup/tree/master"
        },
        {
          "pull_number": 1375,
          "pull_title": "I fix the bug in 986 and add corresponding test.",
          "pull_version": "jhy:master",
          "pull_version_url": "https://github.com/jhy/jsoup/tree/master"
        },
        {
          "pull_number": 1374,
          "pull_title": "Fix the bug in issue 845",
          "pull_version": "jhy:master",
          "pull_version_url": "https://github.com/jhy/jsoup/tree/master"
        },
        {
          "pull_number": 1372,
          "pull_title": "Fix Issue #1361 may bring higher performance",
          "pull_version": "jhy:master",
          "pull_version_url": "https://github.com/jhy/jsoup/tree/master"
        },
        {
          "pull_number": 1371,
          "pull_title": "basic fix for Issue 1117",
          "pull_version": "jhy:master",
          "pull_version_url": "https://github.com/jhy/jsoup/tree/master"
        },
        {
          "pull_number": 1370,
          "pull_title": "Issue 887 has been fixed.",
          "pull_version": "jhy:master",
          "pull_version_url": "https://github.com/jhy/jsoup/tree/master"
        },
        {
          "pull_number": 1368,
          "pull_title": "add new feature about isValid() about baseline",
          "pull_version": "jhy:master",
          "pull_version_url": "https://github.com/jhy/jsoup/tree/master"
        },
        {
          "pull_number": 1366,
          "pull_title": "Update Travis syntax",
          "pull_version": "jhy:master",
          "pull_version_url": "https://github.com/jhy/jsoup/tree/master"
        },
        {
          "pull_number": 1365,
          "pull_title": "Fix incorrect rtc element",
          "pull_version": "jhy:master",
          "pull_version_url": "https://github.com/jhy/jsoup/tree/master"
        },
        {
          "pull_number": 1362,
          "pull_title": "Fix the problem that jsoup will not treat an incoming <Alpha as text",
          "pull_version": "jhy:master",
          "pull_version_url": "https://github.com/jhy/jsoup/tree/master"
        },
        {
          "pull_number": 1359,
          "pull_title": "Add a parse method to encode illegal tag in html body",
          "pull_version": "jhy:master",
          "pull_version_url": "https://github.com/jhy/jsoup/tree/master"
        },
        {
          "pull_number": 1357,
          "pull_title": "Fix the problem that parser does not strip children nodes in the Document.title() method in xml file.",
          "pull_version": "jhy:master",
          "pull_version_url": "https://github.com/jhy/jsoup/tree/master"
        },
        {
          "pull_number": 1354,
          "pull_title": "Support Element.firstParent(query)",
          "pull_version": "jhy:master",
          "pull_version_url": "https://github.com/jhy/jsoup/tree/master"
        },
        {
          "pull_number": 1351,
          "pull_title": "Add select method overloads with Evaluator.",
          "pull_version": "jhy:master",
          "pull_version_url": "https://github.com/jhy/jsoup/tree/master"
        }
      ]
    },
    "github_issues": {
      "issue_datas": [
        {
          "issue_url": "https://github.com/jhy/jsoup/issues/1432",
          "issue_title": "performance of filters -> why so big difference?",
          "issue_number": 1432,
          "issue_text": "Hi.\r\n\r\nIt seems that performance of using filters very different in the following 2 cases:\r\n1) document.select(\"div[class*=test1]:not(:has(span[class*=test2]))\")\r\n2) document.select(\"div[class*=test1]\").stream().filter(v -> v.is(\":not(:has(span[class*=test2]))\"));\r\n\r\nFilter here is just an example - it can be any other.\r\nSelector returns 4 elements in my case ( it matters - so we do not expect to spend much time on filters processing ).\r\n\r\n**In the second case performance is several times better.\r\nSo the question is: how can that be possible at all?**\r\n\r\nThanks.",
          "issue_comments": []
        },
        {
          "issue_url": "https://github.com/jhy/jsoup/issues/1431",
          "issue_title": "childNodes + appendChild = ConcurrentModificationException?",
          "issue_number": 1431,
          "issue_text": "```java\r\n// ConcurrentModificationException\r\nfor (Node node : element.childNodes()) {\r\n    element.parent().appendChild(node);\r\n}\r\n\r\n// Seems right\r\nfor (Node node : new ArrayList<>(element.childNodes())) {\r\n    element.parent().appendChild(node);\r\n}\r\n```\r\n\r\nWhich is the right way to move child node to another parent in order?",
          "issue_comments": [
            {
              "comment_username": "gMan1990",
              "comment_create_time": "2020-09-26T11:56:16Z",
              "comment_edit_time": "2020-09-26T11:56:16Z",
              "comment_text": "```java\r\n// maybe like this\r\nfor (int i = 0; i < node.childNodeSize();) {\r\n    node.parent().appendChild(node.childNode(i));\r\n}\r\n```"
            }
          ]
        },
        {
          "issue_url": "https://github.com/jhy/jsoup/issues/1430",
          "issue_title": "`<p> == false`, tag p is block",
          "issue_number": 1430,
          "issue_text": "https://github.com/jhy/jsoup/blob/89580cc3d25d0d89ac1f46b349e5cd315883dc79/src/main/java/org/jsoup/nodes/Element.java#L179\r\n```java\r\n    /**\r\n     * Test if this element is a block-level element. (E.g. {@code <div> == true} or an inline element\r\n     * {@code <p> == false}).\r\n     * \r\n     * @return true if block, false if not (and thus inline)\r\n     */\r\n    public boolean isBlock() {\r\n        return tag.isBlock();\r\n    }\r\n```\r\nJavaDoc ERROR.",
          "issue_comments": []
        },
        {
          "issue_url": "https://github.com/jhy/jsoup/issues/1429",
          "issue_title": "Issue with replaceWith method",
          "issue_number": 1429,
          "issue_text": "When I am using replaceWith on an element, it changes the size of childNodes the element has.\r\nOn closely examining I found that element ito be replaced with has previous element as  \"br\" or \"br\" followed with textnode with just newline (\\n) are now after replacement only textnode with newline\"\\n\".  Is it replacing any prev \"br\" elements and newlines?\r\nIs this a known behvaiour or an issue? Can i change this behaviour or is there a workaround?",
          "issue_comments": []
        },
        {
          "issue_url": "https://github.com/jhy/jsoup/issues/1428",
          "issue_title": "Expose the method for registering new tags in XML and the possibility of saying \"formatAsBlock\" false or true",
          "issue_number": 1428,
          "issue_text": "In our system, we created some xml tags to facilitate our understanding, but some we want in the parser, to be considered as inline, because we need the number of characters of the text faithful to what was written, because without this, I was adding formatting spaces\\r\\n\\r\\nexemple:\\r\\n```\\r\\n<assessment-group id=\"2C93808974B5C5DA0174B73EB5712FBA\"> \\r\\n            <label> 2. </label> \\r\\n            <highlight show-answer=\"true\" id=\"2C93808974B5C5DA0174B73EB5712FBB\"> \\r\\n                <question> \\r\\n                    <p id=\"2C93808974B5C5DA0174B73EB5712FBC\">Underline the best word to complete each sentence below.</p> \\r\\n                </question> \\r\\n                <items> \\r\\n                    <item id=\"2C93808974B5C5DA0174B73EB5712FBD\">He has <token type=\"underline\" color=\"any\">received</token> many gifts, <strong>but</strong> his wife <strong>has</strong> received <strong>no /</strong> <token type=\"underline\" color=\"any\">none</token></item> \\r\\n                    <item id=\"2C93808974B5C5DA0174B73EB5712FBE\">The <strong>bird</strong> hasn't drunk <token type=\"circle\" color=\"any\"><strong>any</strong></token> <strong> / some</strong> water today.</item> \\r\\n                    <item id=\"2C93808974B5C5DA0174B73EB5712FBF\">Did you watch <strong>no / </strong> <token type=\"underline\" color=\"any\"> any </token> films last month?</item> \\r\\n                    <item id=\"2C93808974B5C5DA0174B73EB5712FC0\">I have <token type=\"underline\" color=\"any\"> some </token> <strong> / any</strong> news for you.</item> \\r\\n                    <item id=\"2C93808974B5C5DA0174B73EB5712FC1\">Steven is going to answer <token type=\"underline\" color=\"any\"> no </token> <strong>/ any</strong> e-mails tomorrow.</item> \\r\\n                    <item id=\"2C93808974B5C5DA0174B73EB5712FC2\">There is <token type=\"underline\" color=\"any\"> no </token> <strong>/ any</strong> reason to worry about Dan.</item> \\r\\n                    <item id=\"2C93808974B5C5DA0174B73EB5712FC3\">He has received many gifts, <strong>but</strong> his wife <strong>has</strong> received <strong>no /</strong> <token type=\"underline\" color=\"any\">none</token></item> \\r\\n                    <item id=\"2C93808974B5C5DA0174B73EB5712FC4\">Would you care for <strong>any / </strong> <token type=\"underline\" color=\"any\"> some </token> finger food?</item> \\r\\n                    <item id=\"2C93808974B5C5DA0174B73EB5712FC5\">Bring <token type=\"underline\" color=\"any\"> some </token> <strong> / any</strong> recyclable material next class.</item> \\r\\n                    <item id=\"2C93808974B5C5DA0174B73EB5712FC6\">If you have <token type=\"underline\" color=\"any\"> any </token> <strong> / no</strong> suggestions, contact me.</item> \\r\\n                    <item id=\"2C93808974B5C5DA0174B73EB5712FC7\">We seldom eat <strong>no / </strong> <token type=\"underline\" color=\"any\"> any </token> fattening food.</item> \\r\\n                </items> \\r\\n            </highlight> \\r\\n</assessment-group>\\r\\n```\\r\\n\\r\\nfor now, we have created a way to do it, but it is not a good practice\\r\\n\\r\\n\\tstatic {\\r\\n\\t\\ttry {\\r\\n\\t\\t\\tString[] inlineTags = {\"token\", \"item\"};\\r\\n\\t\\t\\tfor(String tagName : inlineTags) {\\r\\n\\t\\t\\t\\tTag tag = Tag.valueOf(tagName);\\r\\n\\t\\t\\t\\tField field = Tag.class.getDeclaredField(\"formatAsBlock\");\\r\\n\\t\\t\\t\\tfield.setAccessible(true);\\r\\n\\t\\t\\t\\tfield.set(tag, false);\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\tMethod method = Tag.class.getDeclaredMethod(\"register\", Tag.class);\\r\\n\\t\\t\\t\\tmethod.setAccessible(true);\\r\\n\\t\\t\\t\\tmethod.invoke(null, tag);\\t\\t\\t\\t\\r\\n\\t\\t\\t}\\r\\n\\t\\t} catch (Exception e) {\\r\\n\\t\\t\\te.printStackTrace();\\r\\n\\t\\t}\\r\\n\\t}",
          "issue_comments": []
        },
        {
          "issue_url": "https://github.com/jhy/jsoup/issues/1427",
          "issue_title": "Add @SafeHtml java validator",
          "issue_number": 1427,
          "issue_text": "This change does not add any new dependencies to jsoup.\r\nIf a bean validation implementation is present, then the new\r\n@org.jsoup.constraints.SafeHtml constraints can be used.\r\n\r\nThis implementation was copied from Hibernate Validator 6.1.5.FINAL\r\nSafeHtml was marked deprecated in Hibernate Validator following\r\nCVE-2019-10219 see https://in.relation.to/2019/11/20/hibernate-validator-610-6018-released/\r\n\r\nThe root cause of the CVE was a misunderstanding of jsoup by Hibernate\r\nValidator. By making the @SafeHtml constraint part of jsoup, that\r\nconcern is eliminated.\r\n\r\nWith this change, users have a logical source for @SafeHtml,\r\neliminating the need for each project to build and maintain @SafeHtml\r\nitself, making for more secure and maintainable software.\r\n\r\nCloses #1382",
          "issue_comments": []
        },
        {
          "issue_url": "https://github.com/jhy/jsoup/issues/1426",
          "issue_title": "扩展图片src和a标签href的安全校验",
          "issue_number": 1426,
          "issue_text": "使其能支持多种类型的链接",
          "issue_comments": []
        },
        {
          "issue_url": "https://github.com/jhy/jsoup/issues/1425",
          "issue_title": "group and children QueryParser",
          "issue_number": 1425,
          "issue_text": "https://github.com/jhy/jsoup/blob/3f0256953a2938875dba33e58004037989985f7d/src/main/java/org/jsoup/select/QueryParser.java#L83\r\n\r\n- error: `body.select(\">p>strong,>*>li>strong\")` at `,>*>li>strong`\r\n- right: `document.select(\"body>p>strong,body>*>li>strong\")`",
          "issue_comments": []
        },
        {
          "issue_url": "https://github.com/jhy/jsoup/issues/1424",
          "issue_title": "android Jsoup爬取动态网页失败",
          "issue_number": 1424,
          "issue_text": "Jsoup爬取的网页代码和PostMan加载的网页代码不一致",
          "issue_comments": []
        },
        {
          "issue_url": "https://github.com/jhy/jsoup/issues/1423",
          "issue_title": "Avoid the term whitelist",
          "issue_number": 1423,
          "issue_text": "This term was replaced with allowedList. See e.g. https://tools.ietf.org/id/draft-knodel-terminology-00.html\r\nfor a rationale.",
          "issue_comments": [
            {
              "comment_username": "drei01",
              "comment_create_time": "2020-08-20T07:56:13Z",
              "comment_edit_time": "2020-08-20T07:56:19Z",
              "comment_text": "This looks good to me, thanks for picking up on my original PR."
            }
          ]
        },
        {
          "issue_url": "https://github.com/jhy/jsoup/issues/1421",
          "issue_title": "HtmlTreeBuilderState issues",
          "issue_number": 1421,
          "issue_text": "In HtmlTreeBuilderState.java line 934.  When we have a single tag <input>. !startTag.attributes.get(\"type\").equalsIgnoreCase(\"hidden\")\r\nwill case the java.lang.NullPointerException.\r\nstartTag.attributes will be null",
          "issue_comments": [
            {
              "comment_username": "FSchumacher",
              "comment_create_time": "2020-08-07T15:33:18Z",
              "comment_edit_time": "2020-08-07T15:33:18Z",
              "comment_text": "At JMeter we stumbled over the same problem, where we (wrongly) try to parse Javascript as HTML. Our code expects a `HTMLParsingException` and doesn't cope well with the NPE.\r\n\r\nWe will probably change our logic to exclude Javascript (and JSON) code from parsing it with jsoup, but it would be nice, if parsing something foul would always throw a proper Exception.\r\n\r\nThe JMeter bug can be found at https://bz.apache.org/bugzilla/show_bug.cgi?id=64653"
            }
          ]
        },
        {
          "issue_url": "https://github.com/jhy/jsoup/issues/1420",
          "issue_title": "Special characters inside attribute are unescaped post 1.8.* version",
          "issue_number": 1420,
          "issue_text": "We are extensively using Jsoup version1.7.2 from sometime. Thanks for the library ,it has been wonderful.\r\n\r\nBut upgrading seems to be bit difficult with one particular issue we faced.\r\ntest case:\r\n\r\n```\r\n    public static void main(String[] args) {\r\n        final Document doc = Jsoup.parse(\"<input class=\\\"text\\\" value=\\\" &gt; data  &lt; \\\"/>\");\r\n        doc.outputSettings().escapeMode(org.jsoup.nodes.Entities.EscapeMode.xhtml);\r\n        System.out.println(\"Output: \" + doc.select(\"input.text\").get(0));\r\n    }\r\n```\r\noutput is\r\n\r\nJsoup 1.7.2\r\n`Output: <input class=\"text\" value=\" &gt; data  &lt; \" />`\r\n\r\n\r\nJsoup 1.8.2\r\n`Output: <input class=\"text\" value=\" > data  < \">\r\n`\r\nJsoup 1.11.3\r\n`Output: <input class=\"text\" value=\" > data  &lt; \">`\r\n\r\n\r\nThis is similar to the issue https://github.com/jhy/jsoup/issues/684.  but point to note here, this works fine in 1.7.2\r\nCould you please help me over here if there is a workaround or better way to parse this ",
          "issue_comments": [
            {
              "comment_username": "zjamshidi",
              "comment_create_time": "2020-07-29T14:39:08Z",
              "comment_edit_time": "2020-07-29T14:39:08Z",
              "comment_text": "I have the same problem with Jsoup version 1.11.2\r\nI'm using XML syntax to convert HTML to XHTML. I got the error:\r\n`Unescaped '<' not allowed in attributes values`"
            },
            {
              "comment_username": "rhdunn",
              "comment_create_time": "2020-08-03T13:22:35Z",
              "comment_edit_time": "2020-08-03T13:22:35Z",
              "comment_text": "You need to use the following setting:\r\n> doc.outputSettings().escapeMode(Entities.EscapeMode.xhtml)\r\n\r\nThat will cause `&lt;` to be preserved (while the other XML entities will have the same behaviour). This allows the resulting output to be parsed as XML/XHTML. __NOTE:__ This is not set when setting the output mode to XML."
            },
            {
              "comment_username": "RavishaNayak",
              "comment_create_time": "2020-08-18T09:23:28Z",
              "comment_edit_time": "2020-08-18T09:23:28Z",
              "comment_text": "Doesn't work for `&gt`. I get below output\\r\\n\\r\\n`Output: <input class=\"text\" value=\" > data  &lt; \">`\\r\\n\\r\\n\\r\\n"
            }
          ]
        },
        {
          "issue_url": "https://github.com/jhy/jsoup/issues/1419",
          "issue_title": "Replacing text() or html() content of script tag causes jsoup to escape the script contents.",
          "issue_number": 1419,
          "issue_text": "I've created a test in my fork of jsoup [here](https://github.com/pflagerd/jsoup/blob/master/src/test/java/org/jsoup/nodes/ScriptNodeTest.java).\r\n\r\nIt demonstrates the situation in which I encountered this issue.\r\n\r\nIncidentally, this is probably the same as issue #1049 and possibly related to #1057.\r\n",
          "issue_comments": []
        },
        {
          "issue_url": "https://github.com/jhy/jsoup/issues/1417",
          "issue_title": "Cleaner does not honor/copy output settings",
          "issue_number": 1417,
          "issue_text": "If you Clean a document which has output settings defined, then the result document does not have these settings set.",
          "issue_comments": [
            {
              "comment_username": "sebhofmann",
              "comment_create_time": "2020-07-23T13:01:23Z",
              "comment_edit_time": "2020-07-23T13:01:23Z",
              "comment_text": "Code to reproduce:\r\n```\r\n    public static void main(String[] args) {\r\n        final Document test = Jsoup.parse(\"<p>test<br></p>\");\r\n        test.outputSettings().syntax(Document.OutputSettings.Syntax.xml);\r\n        test.outputSettings().escapeMode(Entities.EscapeMode.xhtml);\r\n        final Whitelist whitelist = Whitelist.none().addTags(\"p\", \"br\");\r\n        final Document result = new Cleaner(whitelist).clean(test);\r\n        System.out.println(result.html());\r\n    }\r\n```\r\n\r\nExpected Result\r\n```\r\n<html>\r\n <head></head>\r\n <body>\r\n  <p>test<br /></p>\r\n </body>\r\n</html>\r\n```\r\n\r\nActual Result\r\n```\r\n<html>\r\n <head></head>\r\n <body>\r\n  <p>test<br></p>\r\n </body>\r\n</html>\r\n```"
            }
          ]
        },
        {
          "issue_url": "https://github.com/jhy/jsoup/issues/1416",
          "issue_title": "Cleaner copies the body to the new shell if body is in whitelist",
          "issue_number": 1416,
          "issue_text": "If you have a Document and use a Cleaner with a Whitelist which contains body, then the body outer html will be copied to the new body, resulting in `<body><body>...</body></body>.`\r\n",
          "issue_comments": [
            {
              "comment_username": "sebhofmann",
              "comment_create_time": "2020-07-23T12:57:54Z",
              "comment_edit_time": "2020-07-23T12:57:54Z",
              "comment_text": "Code to Reproduce:   \r\n```\r\npublic static void main(String[] args) {\r\n        final Document test = Jsoup.parse(\"<p>test</p>\");\r\n        final Whitelist whitelist = Whitelist.none().addTags(\"body\", \"p\");\r\n        final Document result = new Cleaner(whitelist).clean(test);\r\n        System.out.println(result.html());\r\n    }\r\n```\r\n\r\nExpected Result:\r\n```\r\n<html>\r\n <head></head>\r\n <body>\r\n   <p>test</p>\r\n </body>\r\n</html>\r\n```\r\n\r\nActual Result:\r\n```\r\n<html>\r\n <head></head>\r\n <body>\r\n  <body>\r\n   <p>test</p>\r\n  </body>\r\n </body>\r\n</html>\r\n```"
            }
          ]
        },
        {
          "issue_url": "https://github.com/jhy/jsoup/issues/1414",
          "issue_title": "where are the ssl cipher suites set? is it the default java ones?",
          "issue_number": 1414,
          "issue_text": "I am wondering where to view the code on how this portion of it was created as I would like to modify the cipher suites.[\"TLS_AES_128_GCM_SHA256\",\"TLS_AES_256_GCM_SHA384\",\"TLS_CHACHA20_POLY1305_SHA256\",\"TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256\",\"TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384\",\"TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305_SHA256\",\"TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256\",\"TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384\",\"TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305_SHA256\",\"TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA\",\"TLS_ECDHE_ECDSA_WITH_AES_256_CBC_SHA\",\"TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA\",\"TLS_ECDHE_RSA_WITH_AES_256_CBC_SHA\",\"TLS_RSA_WITH_AES_128_GCM_SHA256\",\"TLS_RSA_WITH_AES_256_GCM_SHA384\",\"TLS_RSA_WITH_AES_128_CBC_SHA\",\"TLS_RSA_WITH_AES_256_CBC_SHA\"]",
          "issue_comments": []
        },
        {
          "issue_url": "https://github.com/jhy/jsoup/issues/1411",
          "issue_title": "Element factory method",
          "issue_number": 1411,
          "issue_text": "This is a feature request for a more concise way of creating Elements objects directly from an HTML String.  Right now, especially in unit tests, we have lots of code that looks like this:\r\n\r\n`Element e = new Element(\"div\").html(\"<span>Some stuff</span><span>Second part</span>\");`\r\n\r\nIt would be very nice to have something more like this:\r\n\r\n`Element e = Element.of(\"<div><span>Some stuff</span><span>Second part</span></div>\")`\r\n\r\nOr alternatively have a parser method like\r\n\r\n`Element e = JSoup.parseElement(\"<div></div>\");`\r\n\r\nThis is not only more concise, it would allow easier dynamic creation of mock elements for testing from a data file or list of HTML strings.",
          "issue_comments": []
        },
        {
          "issue_url": "https://github.com/jhy/jsoup/issues/1410",
          "issue_title": "NullPointerException in shallowClone().toString() due to lack of parent",
          "issue_number": 1410,
          "issue_text": "I am using 1.13.1 org.jsoup.jsoup and have code  like this:\r\n```\r\nString nodeHTML = ((Element) node).shallowClone().toString();\r\n```\r\nand get a NPE because Element.isInlineable  at 1619 calls parent().isBlock() but a shallowClone has no parent.\r\n\r\nSuggest revising test to be && (parent()  == null || parent().isBlock()) rather than just parent().isBlock() since the latter causes NPE (unless no parents is not Inlineable...)",
          "issue_comments": [
            {
              "comment_username": "gtaborcrossword",
              "comment_create_time": "2020-07-13T16:34:03Z",
              "comment_edit_time": "2020-07-13T16:34:03Z",
              "comment_text": "Also `toString()` (internally `outerHtml()`) fails on this. So it seems to be a serious problem."
            }
          ]
        },
        {
          "issue_url": "https://github.com/jhy/jsoup/issues/1409",
          "issue_title": "Childs of <template> should be a document fragment",
          "issue_number": 1409,
          "issue_text": "The HTML5 living standard states regarding the template element (https://html.spec.whatwg.org/multipage/scripting.html#the-template-element):\r\n\r\n> The template contents of a template element are not children of the element itself.\r\n\r\nWhen parsing the following HTML:\r\n\r\n```\r\n<!DOCTYPE html>\r\n<html>\r\n<body>\r\n  <template id=a><p>Test</p></template>\r\n</body>\r\n</html>\r\n```\r\n\r\nI therefor would expect a DOM similar to this:\r\n\r\n```\r\ntemplate (DOMElement)\r\n   #DocumentFragement\r\n       p (DOMElement)\r\n```\r\n\r\nThis is the way Chromium presents it in dev tools\r\n\r\n![image](https://user-images.githubusercontent.com/940768/85845838-32eb7500-b7a5-11ea-8f55-f9de8f7532e8.png)\r\n\r\nAlternatively, having no children at all seems also OK. I'm not sure how to interpret the standard here.\r\n\r\nHowever, JSoup return this DOM tree:\r\n\r\n```\r\ntemplate (DOMElement)\r\n    p (DOMElement)\r\n```\r\n\r\nThe p element is placed as a child beneath the template tag, which I think violates the HTML5 specification.\r\n\r\n",
          "issue_comments": [
            {
              "comment_username": "gerdriesselmann",
              "comment_create_time": "2020-06-26T10:26:44Z",
              "comment_edit_time": "2020-06-26T10:26:44Z",
              "comment_text": "I wrote a little JavaScript to see how Browsers handle this, and bot Firefox and Chrome return an empty child list:\r\n\r\n```\r\n<!DOCTYPE html>\r\n<html>\r\n<body>\r\n  <template id=a><p>Test</p></template>\r\n  <script type=text/javascript>\r\n    let t = document.getElementById(\"a\");\r\n    console.log(t.tagName);\r\n    console.log(t.childNodes.length);\r\n  </script>\r\n</body>\r\n</html>\r\n```\r\n\r\nBoth browser output\r\n\r\n```\r\nTEMPLATE\r\n0\r\n```\r\n"
            }
          ]
        },
        {
          "issue_url": "https://github.com/jhy/jsoup/issues/1407",
          "issue_title": "Support for selector: enabled and disabled",
          "issue_number": 1407,
          "issue_text": "Implemented support for css selector using: disabled e: enabled",
          "issue_comments": []
        },
        {
          "issue_url": "https://github.com/jhy/jsoup/issues/1406",
          "issue_title": "Form data extraction",
          "issue_number": 1406,
          "issue_text": "Hello Jsoup,\r\n\r\nI am wondering if Jsoup could extract Form data even with a Get or a Post or Execute method ?\r\n\r\nI want to extract Form data received after a postUrl in an Android WebView.\r\n\r\nI can see form data on Chrome Devtools, in Network/Doc/Headers/Form Data , but i didn't found  a way to extract them, is Jsoup could do that ?\r\n\r\nThanks ",
          "issue_comments": []
        },
        {
          "issue_url": "https://github.com/jhy/jsoup/issues/1405",
          "issue_title": "Response question",
          "issue_number": 1405,
          "issue_text": "When I use version 1.13.1, the Response date is truncated and the character is truncated (♛o>_<o♛ (lady))",
          "issue_comments": []
        },
        {
          "issue_url": "https://github.com/jhy/jsoup/issues/1404",
          "issue_title": "Some HTML input causes NullPointerException in HtmlTreeBuilderState",
          "issue_number": 1404,
          "issue_text": "Running the following: `Jsoup.parseBodyFragment(\"<isindex>\");` results in:\\r\\n```\\r\\njava.lang.NullPointerException\\r\\n\\tat org.jsoup.parser.HtmlTreeBuilderState$7.inBodyStartTag(HtmlTreeBuilderState.java:454)\\r\\n\\tat org.jsoup.parser.HtmlTreeBuilderState$7.process(HtmlTreeBuilderState.java:282)\\r\\n\\tat org.jsoup.parser.HtmlTreeBuilder.process(HtmlTreeBuilder.java:136)\\r\\n\\tat org.jsoup.parser.TreeBuilder.runParser(TreeBuilder.java:66)\\r\\n\\tat org.jsoup.parser.HtmlTreeBuilder.parseFragment(HtmlTreeBuilder.java:126)\\r\\n\\tat org.jsoup.parser.Parser.parseFragment(Parser.java:122)\\r\\n\\tat org.jsoup.parser.Parser.parseBodyFragment(Parser.java:166)\\r\\n\\tat org.jsoup.Jsoup.parseBodyFragment(Jsoup.java:160)\\r\\n```\\r\\nThe problem appears to be caused by an attempt to call a method on `startTag.attributes`, which is `null`.\\r\\nA similar error occurs when running: `Jsoup.parseBodyFragment(\"<table><input></table>\");`.\\r\\n\\r\\nThe issue is discovered in Jsoup 1.13.1. I haven't tested it in the master branch, but looking at the code, it still exists.",
          "issue_comments": []
        },
        {
          "issue_url": "https://github.com/jhy/jsoup/issues/1403",
          "issue_title": "Fix regression on Elements.forms()",
          "issue_number": 1403,
          "issue_text": "the `Elements.forms()` method used to return FormElement-s that are directly in the Elements collection. This behaviour was broken since a657ae0240b875a703a1e4909d56ad59997d362d. See https://github.com/jhy/jsoup/issues/1384\r\n\r\nI wrote the fix accordingly to what's written in the javadocs of the methods involved:\r\n* `comments`, `textNodes` and `dataNodes` says they return direct child nodes. I specialized the private `nodesOfType` method to handle child node lookup only (and renamed it `childNodesOfType`)\r\n* `forms()` doc says it returns the forms elements present in the selected elements, not child nodes. I removed the use of `nodesOfType` and put back the previous code.",
          "issue_comments": []
        },
        {
          "issue_url": "https://github.com/jhy/jsoup/issues/1402",
          "issue_title": "improve StringBuilder cache performance under high concurrency",
          "issue_number": 1402,
          "issue_text": "Use ThreadLocal stack to store StringBuilder instance cache to avoid thread contention introduced by synchronized\r\n\r\nOur application which utilizes multithreads to parse html documents experienced performance degradation after updating from jsoup-1.10.x to recent version. Thread dump shows threads stuck at StringBuilder.borrowBuilder / releaseBuilder so the situation is very similar to #1352. It seems synchronized blocks in borrowBuilder / releaseBuilder is introduced in https://github.com/jhy/jsoup/commit/c8c05694da5489221c27e45a5e49d0f5fcb41863.\r\n\r\nI conducted a microbenchmark of the improvement, which shows ~10x reduction in multithreaded StringBuilder.borrowBuilder / releaseBuilder operations.\r\nhttps://github.com/morokosi/jsoup/pull/1",
          "issue_comments": []
        },
        {
          "issue_url": "https://github.com/jhy/jsoup/issues/1401",
          "issue_title": "Redundant whitespace appears on wrapping into a div",
          "issue_number": 1401,
          "issue_text": "Basically, I just want to wrap a text into **div** but the whitespace appears right after the opening tag. This is not critical but appending the whitespace is weird. See the screenshot:\r\n![image](https://user-images.githubusercontent.com/62619912/84534788-3dad0100-acf3-11ea-8fb7-3946b4069f18.png)\r\n",
          "issue_comments": []
        },
        {
          "issue_url": "https://github.com/jhy/jsoup/issues/1400",
          "issue_title": "java.lang.IllegalArgumentException: Request has already been read (with .parse())",
          "issue_number": 1400,
          "issue_text": "I don't call parse() method but after try to getting response body I got error\\r\\nSTACK_TRACE=```java.lang.IllegalArgumentException: Request has already been read (with .parse())\\r\\n\\tat org.jsoup.helper.Validate.isFalse(Validate.java:64)\\r\\n\\tat org.jsoup.helper.HttpConnection$Response.prepareByteData(HttpConnection.java:845)\\r\\n\\tat org.jsoup.helper.HttpConnection$Response.body(HttpConnection.java:858)```\\r\\n\\r\\nlib version: jsoup-1.13.1\\r\\n\\r\\n```\\r\\n        Connection conn = Jsoup.connect(url);\\r\\n       \\r\\n            conn.userAgent(myUserAgent());\\r\\n            conn.followRedirects(followRedirects);\\r\\n            conn.ignoreHttpErrors(true);\\r\\n            conn.cookies(getCookies());\\r\\n            conn.timeout(getTimeout());\\r\\n            conn.header(\"Accept-Encoding\", \"gzip,deflate,lzma,sdch\");\\r\\n            for (int i = 0; i < postData.length; i++) {\\r\\n                String[] item = postData[i].split(\"=\", 2);\\r\\n                conn.data(item[0], item[1]);\\r\\n            }\\r\\n            conn.method(Method.POST);\\r\\n            conn.maxBodySize(maxBodySize);\\r\\n            conn.header(\"X-CSRF-TOKEN\", getAjaxToken());\\r\\n            conn.post();\\r\\n\\r\\n            Connection.Response response = conn.response();\\r\\n            mStatusCode = response.statusCode();\\r\\n            if (mStatusCode == 302 && !followRedirects)\\r\\n                return \"\";\\r\\n\\r\\n            saveCookies(response.cookies(), url);\\r\\n            final String body = response.body(); // HERE IllegalArgumentException error\\r\\n```\\r\\n```java.lang.IllegalArgumentException: Request has already been read (with .parse())```\\r\\n\\r\\nThe response body text as I expect must return just a text plain string, not a html structured body. maybe problem here, but I'm not sure, but all other request with html body returns successful",
          "issue_comments": []
        },
        {
          "issue_url": "https://github.com/jhy/jsoup/issues/1399",
          "issue_title": "Missing Document",
          "issue_number": 1399,
          "issue_text": "Hi, I have set max Body Size (0) but the document is missing. How can I solve this problem?\r\nCode: \r\n`Document homePage = Jsoup.connect(HtmlUrl)\r\n                        .cookies(responsePostLogin.cookies())\r\n                        .userAgent(\"Mozilla/5.0 (Windows; U; WindowsNT 5.1; en-US; rv1.8.1.6) Gecko/20070725 Firefox/2.0.0.6\")\r\n                        .timeout(10 * 1000)\r\n                        .referrer(Refer)\r\n                        .followRedirects(true)\r\n                        .ignoreHttpErrors(true)\r\n                        .ignoreContentType(true)\r\n                        .maxBodySize(0).get();`",
          "issue_comments": [
            {
              "comment_username": "krystiangorecki",
              "comment_create_time": "2020-06-07T20:57:33Z",
              "comment_edit_time": "2020-06-07T20:58:34Z",
              "comment_text": "Try to ask this question here: https://stackoverflow.com/questions/tagged/jsoup\r\nAnd it will be difficult to answer without knowing the value of `HtmlUrl`."
            }
          ]
        },
        {
          "issue_url": "https://github.com/jhy/jsoup/issues/1396",
          "issue_title": "jsoup issue #1173 fix code commit",
          "issue_number": 1396,
          "issue_text": "Jsoup #1173 fix code commit",
          "issue_comments": []
        },
        {
          "issue_url": "https://github.com/jhy/jsoup/issues/1395",
          "issue_title": "jsoup issue #1289 fix code commit",
          "issue_number": 1395,
          "issue_text": "commit the issue #1289  fixed ",
          "issue_comments": []
        }
      ]
    }
  },
  {
    "github_url": "https://github.com/googlefonts/sfntly",
    "github_info": {
      "name": "googlefonts/sfntly",
      "language": "Java"
    },
    "github_branches": {
      "branch_datas": [
        {
          "branch_version": "cibu",
          "branch_url": "https://github.com/googlei18n/sfntly/tree/cibu",
          "branch_download_url": "https://github.com/googlei18n/sfntly/archive/cibu.zip"
        },
        {
          "branch_version": "master",
          "branch_url": "https://github.com/googlei18n/sfntly/tree/master",
          "branch_download_url": "https://github.com/googlei18n/sfntly/archive/master.zip"
        },
        {
          "branch_version": "stuartg",
          "branch_url": "https://github.com/googlei18n/sfntly/tree/stuartg",
          "branch_download_url": "https://github.com/googlei18n/sfntly/archive/stuartg.zip"
        }
      ]
    },
    "github_pull_requests": {
      "pull_datas": [
        {
          "pull_number": 111,
          "pull_title": "Use static_cast<T*>(NULL) instead of reinterpret_cast<T*>(NULL)",
          "pull_version": "googlefonts:master",
          "pull_version_url": "https://github.com/googlei18n/sfntly/tree/master"
        },
        {
          "pull_number": 97,
          "pull_title": "download",
          "pull_version": "googlefonts:master",
          "pull_version_url": "https://github.com/googlei18n/sfntly/tree/master"
        },
        {
          "pull_number": 70,
          "pull_title": "skfntly/c++: Simplify and devirtualize ByteArray",
          "pull_version": "googlefonts:master",
          "pull_version_url": "https://github.com/googlei18n/sfntly/tree/master"
        }
      ]
    },
    "github_issues": {
      "issue_datas": [
        {
          "issue_url": "https://github.com/googlei18n/sfntly/issues/112",
          "issue_title": "CMakeList  \"GCC_OR_CLANG\" doesn't work",
          "issue_number": 112,
          "issue_text": "I complie on Windows System.\\r\\nAnd Cmake the CMakeLists in \\\\sfntly\\\\cpp\\\\CMakeLists.txt.\\r\\nI use the VS2019 to open the sln and found the commandline has \"__wur=__attribute__\\\\(\\\\(warn_unused_result\\\\)\\\\) -Wall -Werror -fno-exceptions\".\\r\\nAt last, the reason is \"set(GCC_OR_CLANG ((CMAKE_CXX_COMPILER_ID MATCHES \"Clang\") OR CMAKE_COMPILER_IS_GNUCXX))\" doesn't work.\\r\\n",
          "issue_comments": []
        },
        {
          "issue_url": "https://github.com/googlei18n/sfntly/issues/111",
          "issue_title": "Use static_cast<T*>(NULL) instead of reinterpret_cast<T*>(NULL)",
          "issue_number": 111,
          "issue_text": "Fixes build failures reported on BSD systems, see https://bugzilla.mozilla.org/show_bug.cgi?id=1583192 and https://github.com/googlefonts/sfntly/issues/67.",
          "issue_comments": [
            {
              "comment_username": "leizleiz",
              "comment_create_time": "2020-01-30T07:32:17Z",
              "comment_edit_time": "2020-01-30T07:32:17Z",
              "comment_text": "Sorry this fell through the cracks. I left a couple comments, but otherwise this mostly looks good.\r\n\r\nPlease note Chromium is no longer using sfntly, and switched to HarfBuzz. So there's little interest in maintaining sfntly."
            }
          ]
        },
        {
          "issue_url": "https://github.com/googlei18n/sfntly/issues/101",
          "issue_title": "Wrong encoding in EOT's 255SHORT for large negative numbers",
          "issue_number": 101,
          "issue_text": "Code here:\\r\\nhttps://github.com/googlei18n/sfntly/blob/master/java/src/com/google/typography/font/tools/conversion/eot/GlyfEncoder.java#L185\\r\\n\\r\\nThe \"spec\" is here:\\r\\nhttps://www.w3.org/Submission/MTX/#id_255SHORT\\r\\n\\r\\nNote how in the spec, a wordCode-encoded short does NOT get a flipSignCode before it.  The short is encoded as is in two's-complement.\\r\\n\\r\\nEg. to encode the number -984, currently we encode: FA FD 03 D8\\r\\n \\r\\nFA – flipsignCode (250 decimal)\\r\\nFD – WordCode (253 decimal)\\r\\n03 D8 – Value (984 decimal)\\r\\n\\r\\nCorrect encoding is FD FC 28\\r\\nFD – WordCode (253 decimal)\\r\\nFC 28 – Value (-984 decimal)\\r\\n\\r\\n(Reported to me by Microsoft.)\\r\\n",
          "issue_comments": [
            {
              "comment_username": "behdad",
              "comment_create_time": "2018-09-25T17:29:00Z",
              "comment_edit_time": "2018-09-25T17:29:00Z",
              "comment_text": "The correct code should do something like this:\r\n```\r\n\r\nconst short lowestCode = 253;\r\n\r\nshort value;\r\n\r\n \r\n\r\nif(value > (3 * lowestCode) || value <= -lowestCode)\r\n\r\n{\r\n\r\n      os.write(253);\r\n\r\n      os.write((byte)(absValue >> 8));\r\n\r\n      os.write((byte)(absValue & 0xff));\r\n\r\n}\r\n\r\nelse\r\n\r\n{\r\n\r\n \r\n\r\n  if (value < 0) {\r\n\r\n      os.write(250); /* flip the sign */\r\n\r\n  value = (short)-value;\r\n\r\n  }\r\n\r\n  else if (value >= lowestCode) {\r\n\r\n    value = (short)(value - lowestCode);\r\n\r\n    if (value >= lowestCode) {\r\n\r\n      value = (short)(value - lowestCode);\r\n\r\n      os.write(254);\r\n\r\n    }\r\n\r\n    else {\r\n\r\n      os.write(255);\r\n\r\n    }\r\n\r\n  }\r\n\r\n  os.write((byte)value);\r\n\r\n}\r\n```"
            },
            {
              "comment_username": "nyshadhr9",
              "comment_create_time": "2018-09-26T18:14:58Z",
              "comment_edit_time": "2018-09-26T18:14:58Z",
              "comment_text": "There are some values which can be encoded in multiple ways. For Ex:\r\n500 can be encoded as both:\r\nFE (oneMoreByteCode2) 00 (decimal 0) \r\nor\r\nFF(oneMoreByteCode1) FA (decimal 250)\r\n\r\nIs there a preference for one or the other? Or are we just concerned about minimizing the number of bytes taken to encode, so either one works."
            },
            {
              "comment_username": "behdad",
              "comment_create_time": "2018-09-26T18:30:37Z",
              "comment_edit_time": "2018-09-26T18:30:37Z",
              "comment_text": "Yeah I see the overlaps.  No idea.  @taylorb-monotype can you advise, which way does Monotype encode / recommend encoding these?  Thanks."
            },
            {
              "comment_username": "taylorb-monotype",
              "comment_create_time": "2018-09-26T20:44:34Z",
              "comment_edit_time": "2018-09-26T20:44:34Z",
              "comment_text": "Our code to write 255Shorts is the same as what Behdad has indicated above.\r\nWith this code I don't think there is any ambiguity. The choice is made by the code above. \r\nFor example, you have the following encodings:\r\n\r\nvalue  encoding\r\n0        00\r\n1        01\r\n...\r\n249      f9\r\n250      ff 00 \r\n251      ff 01\r\n...\r\n499      ff f9 \r\n500      fe 00\r\n501      fe 01\r\n...\r\n749      fe f9\r\n750      fd 02 ee\r\n751      fd 02 ef\r\n...\r\n767      fd 02 ff\r\n768      fd 03 00\r\n769      fd 03 01\r\n...\r\netc.\r\n\r\n"
            },
            {
              "comment_username": "behdad",
              "comment_create_time": "2018-09-26T20:54:07Z",
              "comment_edit_time": "2018-09-26T20:54:07Z",
              "comment_text": "> Our code to write 255Shorts is the same as what Behdad has indicated above.\r\n\r\nThanks for confirming.\r\n\r\n> With this code I don't think there is any ambiguity.\r\n\r\nThere is ambiguity in the spec, but yeah sticking to the above scheme works.\r\n\r\n> The choice is made by the code above.\r\n> For example, you have the following encodings:\r\n> \r\n> value encoding\r\n> 0 00\r\n> 1 01\r\n> ...\r\n> 249 f9\r\n> 250 ff 00\r\n> 251 ff 01\r\n> ...\r\n> 499 ff f9\r\n> 500 fe 00\r\n> 501 fe 01\r\n> ...\r\n> 749 fe f9\r\n> 750 fd 02 ee\r\n> 751 fd 02 ef\r\n> ...\r\n> 767 fd 02 ff\r\n> 768 fd 03 00\r\n> 769 fd 03 01\r\n> ...\r\n> etc.\r\n\r\n"
            },
            {
              "comment_username": "nyshadhr9",
              "comment_create_time": "2018-09-26T21:04:02Z",
              "comment_edit_time": "2018-09-26T21:04:02Z",
              "comment_text": "Thank you for clarifying :)\r\n\r\nMy concern with the above approach was that, 750-755 take up 3 bytes whereas they could be encoded in 2 bytes as FE FA - FE FF. But I guess that can be sacrificed for a simpler and consistent solution. "
            }
          ]
        },
        {
          "issue_url": "https://github.com/googlei18n/sfntly/issues/99",
          "issue_title": "How to extract characters containing spaces under Java",
          "issue_number": 99,
          "issue_text": "java -jar sfnttool.jar  -s \"ab cd\" Arial.ttf  Arial_s.ttf，A character contains a space that causes failure，How to solve？Thank you. ",
          "issue_comments": []
        },
        {
          "issue_url": "https://github.com/googlei18n/sfntly/issues/98",
          "issue_title": "Possible unintended variable usage in file \"sfntly/cpp/src/sfntly/font.cc\" line 426",
          "issue_number": 98,
          "issue_text": "Hi,\\r\\n\\r\\nWhile experimenting with a CodeSonar plugin we develop, we noticed a potential bug in file \"sfntly/cpp/src/sfntly/font.cc\" line 426 function Font::Builder::InterRelateBuilders(TableBuilderMap* builder_map):\\r\\n\\r\\nTable::Builder* raw_hhea_builder = GetReadBuilder(builder_map, Tag::hhea);\\r\\n  HorizontalHeaderTableBuilderPtr horizontal_header_builder;\\r\\n  if (raw_head_builder != NULL) { //HERE\\r\\n    horizontal_header_builder =\\r\\n        down_cast<HorizontalHeaderTable::Builder*>(raw_hhea_builder);\\r\\n  }\\r\\n\\r\\nShouldn't you test for not being null the raw_hhea_builder variable? The one you test here is already tested in line 419.\\r\\n\\r\\nThanks,\\r\\nPetru Florin Mihancea\\r\\n",
          "issue_comments": [
            {
              "comment_username": "davelab6",
              "comment_create_time": "2018-07-02T13:20:35Z",
              "comment_edit_time": "2018-07-02T13:20:35Z",
              "comment_text": "This project is abandoned, please see https://github.com/rillig/sfntly\n"
            }
          ]
        },
        {
          "issue_url": "https://github.com/googlei18n/sfntly/issues/97",
          "issue_title": "download",
          "issue_number": 97,
          "issue_text": "download",
          "issue_comments": [
            {
              "comment_username": "googlebot",
              "comment_create_time": "2018-04-25T01:02:12Z",
              "comment_edit_time": "2018-04-25T01:02:12Z",
              "comment_text": "\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->"
            }
          ]
        },
        {
          "issue_url": "https://github.com/googlei18n/sfntly/issues/93",
          "issue_title": "macOS Helvetica Neue dfont subset fails",
          "issue_number": 93,
          "issue_text": "In macOS, I use this function, SfntlyWrapper::SubsetFont() to do subset for Helvetica Neue dfont. \r\nAlthough it returns OK (a valid size for subset font), it seems that the content buffer is not good. \r\nDoes anyone meet this issue before and how to resolve it? Do I update to the latest code for this issue? ",
          "issue_comments": []
        },
        {
          "issue_url": "https://github.com/googlei18n/sfntly/issues/92",
          "issue_title": "Index attempted to be read from list is out of bounds: 4e",
          "issue_number": 92,
          "issue_text": "Tried this font, and received the following:\r\nhttps://expirebox.com/download/2fdfa40680f73a2ecc271ed7243d9bb1.html\r\n\r\n![](https://i.imgur.com/TVp1lX7.png)",
          "issue_comments": []
        },
        {
          "issue_url": "https://github.com/googlei18n/sfntly/issues/90",
          "issue_title": "Can't compile, Error: invalid numeric argument '/Wextra'",
          "issue_number": 90,
          "issue_text": "I created a VS 2010 and VS 2015 .sln with CMAKE and tried to compile it. I always get the error:\r\n`invalid numeric argument '/Werror'.`\r\n\r\nI tried removing this block from cmake list:\r\n`if(GCC_OR_CLANG)\r\n  add_definitions(-D__wur=__attribute__\\(\\(warn_unused_result\\)\\) -Wall -Werror -fno-exceptions)\r\nendif(GCC_OR_CLANG)`\r\n\r\nThen I get a whole lot of other errors. What's the problem here?!\r\n",
          "issue_comments": [
            {
              "comment_username": "ZypherChan",
              "comment_create_time": "2020-08-22T06:16:46Z",
              "comment_edit_time": "2020-08-22T06:16:46Z",
              "comment_text": "I have the same problem.You can open the sln property setting and C/C++->Commandline->Other Option.Now you can delete '/Werror'.\\r\\nThe final reason is the CMakeLists \"set(GCC_OR_CLANG ((CMAKE_CXX_COMPILER_ID MATCHES \"Clang\") OR CMAKE_COMPILER_IS_GNUCXX))\" doesn't work.\\r\\nGood luck :)"
            }
          ]
        },
        {
          "issue_url": "https://github.com/googlei18n/sfntly/issues/88",
          "issue_title": "Feature: require Java ≥ 7 to reduce code redundancy",
          "issue_number": 88,
          "issue_text": "Currently, the required Java version for sfntly is not documented anywhere. By experimenting I found out that with the exception of the `SafeVarargs` annotation, sfntly currently targets Java 6.\r\n\r\nChanging the minimally supported Java version to 7 would bring these benefits:\r\n\r\n* Less code to write for `try-with-resources`\r\n* Less code to write for generic types\r\n* Less code to write for implementing `equals/hashCode`\r\n\r\nChanging the minimally supported Java version to 8 would bring these benefits:\r\n\r\n* Even less code to write for generic types\r\n\r\nThe required Java version should be documented somewhere.",
          "issue_comments": []
        },
        {
          "issue_url": "https://github.com/googlei18n/sfntly/issues/82",
          "issue_title": "edit glyf data",
          "issue_number": 82,
          "issue_text": "Can I use this to edit glyf data , if it can do it where can I find the sample code?\r\nThank you for your help!",
          "issue_comments": [
            {
              "comment_username": "rillig",
              "comment_create_time": "2017-06-06T21:17:49Z",
              "comment_edit_time": "2017-06-06T21:17:49Z",
              "comment_text": "No, not really.\r\n\r\nYou could start at the `GlyphTable`. With the help of the `LocaTable` you can access the individual glyphs. They are either `SimpleGlyph` or `CompositeGlyph`. The simple glyph has an inner class called `Contour`, but that is not implemented.\r\n\r\nThe assembly instructions are also not implemented. The only thing you can do with them is a hex dump."
            }
          ]
        },
        {
          "issue_url": "https://github.com/googlei18n/sfntly/issues/70",
          "issue_title": "skfntly/c++: Simplify and devirtualize ByteArray",
          "issue_number": 70,
          "issue_text": "Unify the GrowableMemoryByteArray and MemoryByteArray subclasses with\r\ntheir parent class ByteArray.",
          "issue_comments": [
            {
              "comment_username": "davelab6",
              "comment_create_time": "2017-10-11T16:13:36Z",
              "comment_edit_time": "2017-10-11T16:13:36Z",
              "comment_text": "@HalCanary I propose making a PR to @rillig 's fork for this :)"
            },
            {
              "comment_username": "behdad",
              "comment_create_time": "2017-10-19T18:21:38Z",
              "comment_edit_time": "2017-10-19T18:21:38Z",
              "comment_text": "Hal, can you take over maintaining this the C++ bits?"
            }
          ]
        },
        {
          "issue_url": "https://github.com/googlei18n/sfntly/issues/67",
          "issue_title": "cpp library fails to build on FreeBSD in C++11 mode",
          "issue_number": 67,
          "issue_text": "FreeBSD defines `NULL` via `nullptr` in C++11 mode since 9.1-RELEASE (or freebsd/freebsd@c8ed04c26b67) which conflicts with `reinterpret_cast<whatever>(NULL)` expression. Maybe expand `NULL` to `0`.\r\n\r\n```c++\r\n$ pkg info -x icu 'g.*test'\r\nicu-58.2,1\r\ngoogletest-1.8.0.11\r\n\r\n$ c++ -v\r\nFreeBSD clang version 4.0.0 (trunk 291476) (based on LLVM 4.0.0svn)\r\nTarget: x86_64-unknown-freebsd12.0\r\nThread model: posix\r\nInstalledDir: /usr/bin\r\nFound CUDA installation: /usr/local/cuda, version 7.5\r\n\r\n$ CXXFLAGS='-std=c++11' cmake /path/to/sfntly/cpp\r\n$ gmake\r\n[...]\r\ncpp/src/sample/subtly/utils.cc:82:22: error: reinterpret_cast from 'nullptr_t' to 'FILE *'\r\n      (aka '__sFILE *') is not allowed\r\n  if (output_file == reinterpret_cast<FILE*>(NULL))\r\n                     ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n1 error generated.\r\n\r\ncpp/src/sfntly/table/core/cmap_table.cc:442:21: error: reinterpret_cast from 'nullptr_t' to\r\n      'sfntly::ReadableFontData *' is not allowed\r\n    : CMap::Builder(reinterpret_cast<ReadableFontData*>(NULL),\r\n                    ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\ncpp/src/sfntly/table/core/cmap_table.cc:566:34: error: reinterpret_cast from 'nullptr_t' to\r\n      'sfntly::WritableFontData *' is not allowed\r\n                               : reinterpret_cast<WritableFontData*>(NULL),\r\n                                 ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\ncpp/src/sfntly/table/core/cmap_table.cc:577:34: error: reinterpret_cast from 'nullptr_t' to\r\n      'sfntly::ReadableFontData *' is not allowed\r\n                               : reinterpret_cast<ReadableFontData*>(NULL),\r\n                                 ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\ncpp/src/sfntly/table/core/cmap_table.cc:961:21: error: reinterpret_cast from 'nullptr_t' to\r\n      'sfntly::ReadableFontData *' is not allowed\r\n    : CMap::Builder(reinterpret_cast<ReadableFontData*>(NULL),\r\n                    ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\ncpp/src/sfntly/table/core/cmap_table.cc:969:21: error: reinterpret_cast from 'nullptr_t' to\r\n      'sfntly::ReadableFontData *' is not allowed\r\n    : CMap::Builder(reinterpret_cast<ReadableFontData*>(NULL),\r\n                    ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n5 errors generated.\r\n\r\ncpp/src/test/cmap_editing_test.cc:44:33: error: reinterpret_cast from 'nullptr_t' to\r\n      'CMapTable::Builder *' is not allowed\r\n  ASSERT_NE(cmap_table_builder, reinterpret_cast<CMapTable::Builder*>(NULL));\r\n  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n/usr/local/include/gtest/gtest.h:1960:54: note: expanded from macro 'ASSERT_NE'\r\n# define ASSERT_NE(val1, val2) GTEST_ASSERT_NE(val1, val2)\r\n                               ~~~~~~~~~~~~~~~~~~~~~~^~~~~\r\n/usr/local/include/gtest/gtest.h:1942:63: note: expanded from macro 'GTEST_ASSERT_NE'\r\n  ASSERT_PRED_FORMAT2(::testing::internal::CmpHelperNE, val1, val2)\r\n  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^~~~~\r\n/usr/local/include/gtest/gtest_pred_impl.h:166:40: note: expanded from macro 'ASSERT_PRED_FORMAT2'\r\n  GTEST_PRED_FORMAT2_(pred_format, v1, v2, GTEST_FATAL_FAILURE_)\r\n  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~\r\n/usr/local/include/gtest/gtest_pred_impl.h:147:43: note: expanded from macro 'GTEST_PRED_FORMAT2_'\r\n  GTEST_ASSERT_(pred_format(#v1, #v2, v1, v2), \\\r\n  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^~~~~~\r\n/usr/local/include/gtest/gtest_pred_impl.h:77:52: note: expanded from macro 'GTEST_ASSERT_'\r\n  if (const ::testing::AssertionResult gtest_ar = (expression)) \\\r\n                                                   ^~~~~~~~~~\r\ncpp/src/test/cmap_editing_test.cc:98:19: error: reinterpret_cast from 'nullptr_t' to\r\n      'CMapTable::CMap *' is not allowed\r\n  ASSERT_NE(cmap, reinterpret_cast<CMapTable::CMap*>(NULL));\r\n  ~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n/usr/local/include/gtest/gtest.h:1960:54: note: expanded from macro 'ASSERT_NE'\r\n# define ASSERT_NE(val1, val2) GTEST_ASSERT_NE(val1, val2)\r\n                               ~~~~~~~~~~~~~~~~~~~~~~^~~~~\r\n/usr/local/include/gtest/gtest.h:1942:63: note: expanded from macro 'GTEST_ASSERT_NE'\r\n  ASSERT_PRED_FORMAT2(::testing::internal::CmpHelperNE, val1, val2)\r\n  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^~~~~\r\n/usr/local/include/gtest/gtest_pred_impl.h:166:40: note: expanded from macro 'ASSERT_PRED_FORMAT2'\r\n  GTEST_PRED_FORMAT2_(pred_format, v1, v2, GTEST_FATAL_FAILURE_)\r\n  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~\r\n/usr/local/include/gtest/gtest_pred_impl.h:147:43: note: expanded from macro 'GTEST_PRED_FORMAT2_'\r\n  GTEST_ASSERT_(pred_format(#v1, #v2, v1, v2), \\\r\n  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^~~~~~\r\n/usr/local/include/gtest/gtest_pred_impl.h:77:52: note: expanded from macro 'GTEST_ASSERT_'\r\n  if (const ::testing::AssertionResult gtest_ar = (expression)) \\\r\n                                                   ^~~~~~~~~~\r\n2 errors generated.\r\n\r\ncpp/src/test/cmap_iterator_test.cc:135:13: error: reinterpret_cast from 'nullptr_t' to\r\n      'CMapTable::CMap::CharacterIterator *' is not allowed\r\n            reinterpret_cast<CMapTable::CMap::CharacterIterator*>(NULL));\r\n            ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n/usr/local/include/gtest/gtest.h:1927:63: note: expanded from macro 'EXPECT_NE'\r\n  EXPECT_PRED_FORMAT2(::testing::internal::CmpHelperNE, val1, val2)\r\n  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^~~~~\r\n/usr/local/include/gtest/gtest_pred_impl.h:162:40: note: expanded from macro 'EXPECT_PRED_FORMAT2'\r\n  GTEST_PRED_FORMAT2_(pred_format, v1, v2, GTEST_NONFATAL_FAILURE_)\r\n  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n/usr/local/include/gtest/gtest_pred_impl.h:147:43: note: expanded from macro 'GTEST_PRED_FORMAT2_'\r\n  GTEST_ASSERT_(pred_format(#v1, #v2, v1, v2), \\\r\n  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^~~~~~\r\n/usr/local/include/gtest/gtest_pred_impl.h:77:52: note: expanded from macro 'GTEST_ASSERT_'\r\n  if (const ::testing::AssertionResult gtest_ar = (expression)) \\\r\n                                                   ^~~~~~~~~~\r\n1 error generated.\r\n\r\ncpp/src/test/cmap_test.cc:118:25: error: reinterpret_cast from 'nullptr_t' to 'FontArray *'\r\n      (aka 'vector<Ptr<sfntly::Font> > *') is not allowed\r\n  ASSERT_NE(font_array, reinterpret_cast<FontArray*>(NULL));\r\n  ~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n/usr/local/include/gtest/gtest.h:1960:54: note: expanded from macro 'ASSERT_NE'\r\n# define ASSERT_NE(val1, val2) GTEST_ASSERT_NE(val1, val2)\r\n                               ~~~~~~~~~~~~~~~~~~~~~~^~~~~\r\n/usr/local/include/gtest/gtest.h:1942:63: note: expanded from macro 'GTEST_ASSERT_NE'\r\n  ASSERT_PRED_FORMAT2(::testing::internal::CmpHelperNE, val1, val2)\r\n  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^~~~~\r\n/usr/local/include/gtest/gtest_pred_impl.h:166:40: note: expanded from macro 'ASSERT_PRED_FORMAT2'\r\n  GTEST_PRED_FORMAT2_(pred_format, v1, v2, GTEST_FATAL_FAILURE_)\r\n  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~\r\n/usr/local/include/gtest/gtest_pred_impl.h:147:43: note: expanded from macro 'GTEST_PRED_FORMAT2_'\r\n  GTEST_ASSERT_(pred_format(#v1, #v2, v1, v2), \\\r\n  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^~~~~~\r\n/usr/local/include/gtest/gtest_pred_impl.h:77:52: note: expanded from macro 'GTEST_ASSERT_'\r\n  if (const ::testing::AssertionResult gtest_ar = (expression)) \\\r\n                                                   ^~~~~~~~~~\r\ncpp/src/test/cmap_test.cc:122:19: error: reinterpret_cast from 'nullptr_t' to\r\n      'sfntly::Font *' is not allowed\r\n  ASSERT_NE(font, reinterpret_cast<Font*>(NULL));\r\n  ~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n/usr/local/include/gtest/gtest.h:1960:54: note: expanded from macro 'ASSERT_NE'\r\n# define ASSERT_NE(val1, val2) GTEST_ASSERT_NE(val1, val2)\r\n                               ~~~~~~~~~~~~~~~~~~~~~~^~~~~\r\n/usr/local/include/gtest/gtest.h:1942:63: note: expanded from macro 'GTEST_ASSERT_NE'\r\n  ASSERT_PRED_FORMAT2(::testing::internal::CmpHelperNE, val1, val2)\r\n  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^~~~~\r\n/usr/local/include/gtest/gtest_pred_impl.h:166:40: note: expanded from macro 'ASSERT_PRED_FORMAT2'\r\n  GTEST_PRED_FORMAT2_(pred_format, v1, v2, GTEST_FATAL_FAILURE_)\r\n  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~\r\n/usr/local/include/gtest/gtest_pred_impl.h:147:43: note: expanded from macro 'GTEST_PRED_FORMAT2_'\r\n  GTEST_ASSERT_(pred_format(#v1, #v2, v1, v2), \\\r\n  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^~~~~~\r\n/usr/local/include/gtest/gtest_pred_impl.h:77:52: note: expanded from macro 'GTEST_ASSERT_'\r\n  if (const ::testing::AssertionResult gtest_ar = (expression)) \\\r\n                                                   ^~~~~~~~~~\r\ncpp/src/test/cmap_test.cc:127:23: error: reinterpret_cast from 'nullptr_t' to\r\n      'CMapTable::CMap *' is not allowed\r\n  ASSERT_NE((cmap1_), reinterpret_cast<CMapTable::CMap*>(NULL));\r\n  ~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n/usr/local/include/gtest/gtest.h:1960:54: note: expanded from macro 'ASSERT_NE'\r\n# define ASSERT_NE(val1, val2) GTEST_ASSERT_NE(val1, val2)\r\n                               ~~~~~~~~~~~~~~~~~~~~~~^~~~~\r\n/usr/local/include/gtest/gtest.h:1942:63: note: expanded from macro 'GTEST_ASSERT_NE'\r\n  ASSERT_PRED_FORMAT2(::testing::internal::CmpHelperNE, val1, val2)\r\n  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^~~~~\r\n/usr/local/include/gtest/gtest_pred_impl.h:166:40: note: expanded from macro 'ASSERT_PRED_FORMAT2'\r\n  GTEST_PRED_FORMAT2_(pred_format, v1, v2, GTEST_FATAL_FAILURE_)\r\n  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~\r\n/usr/local/include/gtest/gtest_pred_impl.h:147:43: note: expanded from macro 'GTEST_PRED_FORMAT2_'\r\n  GTEST_ASSERT_(pred_format(#v1, #v2, v1, v2), \\\r\n  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^~~~~~\r\n/usr/local/include/gtest/gtest_pred_impl.h:77:52: note: expanded from macro 'GTEST_ASSERT_'\r\n  if (const ::testing::AssertionResult gtest_ar = (expression)) \\\r\n                                                   ^~~~~~~~~~\r\ncpp/src/test/cmap_test.cc:130:23: error: reinterpret_cast from 'nullptr_t' to\r\n      'CMapTable::CMap *' is not allowed\r\n  ASSERT_NE((cmap2_), reinterpret_cast<CMapTable::CMap*>(NULL));\r\n  ~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n/usr/local/include/gtest/gtest.h:1960:54: note: expanded from macro 'ASSERT_NE'\r\n# define ASSERT_NE(val1, val2) GTEST_ASSERT_NE(val1, val2)\r\n                               ~~~~~~~~~~~~~~~~~~~~~~^~~~~\r\n/usr/local/include/gtest/gtest.h:1942:63: note: expanded from macro 'GTEST_ASSERT_NE'\r\n  ASSERT_PRED_FORMAT2(::testing::internal::CmpHelperNE, val1, val2)\r\n  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^~~~~\r\n/usr/local/include/gtest/gtest_pred_impl.h:166:40: note: expanded from macro 'ASSERT_PRED_FORMAT2'\r\n  GTEST_PRED_FORMAT2_(pred_format, v1, v2, GTEST_FATAL_FAILURE_)\r\n  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~\r\n/usr/local/include/gtest/gtest_pred_impl.h:147:43: note: expanded from macro 'GTEST_PRED_FORMAT2_'\r\n  GTEST_ASSERT_(pred_format(#v1, #v2, v1, v2), \\\r\n  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^~~~~~\r\n/usr/local/include/gtest/gtest_pred_impl.h:77:52: note: expanded from macro 'GTEST_ASSERT_'\r\n  if (const ::testing::AssertionResult gtest_ar = (expression)) \\\r\n                                                   ^~~~~~~~~~\r\n4 errors generated.\r\n\r\ncpp/src/test/test_font_utils.cc:73:25: error: reinterpret_cast from 'nullptr_t' to 'FILE *'\r\n      (aka '__sFILE *') is not allowed\r\n  EXPECT_NE(input_file, reinterpret_cast<FILE*>(NULL));\r\n  ~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n/usr/local/include/gtest/gtest.h:1927:63: note: expanded from macro 'EXPECT_NE'\r\n  EXPECT_PRED_FORMAT2(::testing::internal::CmpHelperNE, val1, val2)\r\n  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^~~~~\r\n/usr/local/include/gtest/gtest_pred_impl.h:162:40: note: expanded from macro 'EXPECT_PRED_FORMAT2'\r\n  GTEST_PRED_FORMAT2_(pred_format, v1, v2, GTEST_NONFATAL_FAILURE_)\r\n  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n/usr/local/include/gtest/gtest_pred_impl.h:147:43: note: expanded from macro 'GTEST_PRED_FORMAT2_'\r\n  GTEST_ASSERT_(pred_format(#v1, #v2, v1, v2), \\\r\n  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^~~~~~\r\n/usr/local/include/gtest/gtest_pred_impl.h:77:52: note: expanded from macro 'GTEST_ASSERT_'\r\n  if (const ::testing::AssertionResult gtest_ar = (expression)) \\\r\n                                                   ^~~~~~~~~~\r\ncpp/src/test/test_font_utils.cc:93:26: error: reinterpret_cast from 'nullptr_t' to 'FILE *'\r\n      (aka '__sFILE *') is not allowed\r\n  EXPECT_NE(output_file, reinterpret_cast<FILE*>(NULL));\r\n  ~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n/usr/local/include/gtest/gtest.h:1927:63: note: expanded from macro 'EXPECT_NE'\r\n  EXPECT_PRED_FORMAT2(::testing::internal::CmpHelperNE, val1, val2)\r\n  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^~~~~\r\n/usr/local/include/gtest/gtest_pred_impl.h:162:40: note: expanded from macro 'EXPECT_PRED_FORMAT2'\r\n  GTEST_PRED_FORMAT2_(pred_format, v1, v2, GTEST_NONFATAL_FAILURE_)\r\n  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n/usr/local/include/gtest/gtest_pred_impl.h:147:43: note: expanded from macro 'GTEST_PRED_FORMAT2_'\r\n  GTEST_ASSERT_(pred_format(#v1, #v2, v1, v2), \\\r\n  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^~~~~~\r\n/usr/local/include/gtest/gtest_pred_impl.h:77:52: note: expanded from macro 'GTEST_ASSERT_'\r\n  if (const ::testing::AssertionResult gtest_ar = (expression)) \\\r\n                                                   ^~~~~~~~~~\r\n2 errors generated.\r\n\r\ncpp/src/test/autogenerated/cmap_basic_test.cc:70:19: error: reinterpret_cast from\r\n      'nullptr_t' to 'sfntly::Font *' is not allowed\r\n  ASSERT_NE(font, reinterpret_cast<Font*>(NULL));\r\n  ~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n/usr/local/include/gtest/gtest.h:1960:54: note: expanded from macro 'ASSERT_NE'\r\n# define ASSERT_NE(val1, val2) GTEST_ASSERT_NE(val1, val2)\r\n                               ~~~~~~~~~~~~~~~~~~~~~~^~~~~\r\n/usr/local/include/gtest/gtest.h:1942:63: note: expanded from macro 'GTEST_ASSERT_NE'\r\n  ASSERT_PRED_FORMAT2(::testing::internal::CmpHelperNE, val1, val2)\r\n  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^~~~~\r\n/usr/local/include/gtest/gtest_pred_impl.h:166:40: note: expanded from macro 'ASSERT_PRED_FORMAT2'\r\n  GTEST_PRED_FORMAT2_(pred_format, v1, v2, GTEST_FATAL_FAILURE_)\r\n  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~\r\n/usr/local/include/gtest/gtest_pred_impl.h:147:43: note: expanded from macro 'GTEST_PRED_FORMAT2_'\r\n  GTEST_ASSERT_(pred_format(#v1, #v2, v1, v2), \\\r\n  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^~~~~~\r\n/usr/local/include/gtest/gtest_pred_impl.h:77:52: note: expanded from macro 'GTEST_ASSERT_'\r\n  if (const ::testing::AssertionResult gtest_ar = (expression)) \\\r\n                                                   ^~~~~~~~~~\r\ncpp/src/test/autogenerated/cmap_basic_test.cc:74:26: error: reinterpret_cast from\r\n      'nullptr_t' to 'sfntly::CMapTable *' is not allowed\r\n  ASSERT_NE(cmap_table_, reinterpret_cast<CMapTable*>(NULL));\r\n  ~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n/usr/local/include/gtest/gtest.h:1960:54: note: expanded from macro 'ASSERT_NE'\r\n# define ASSERT_NE(val1, val2) GTEST_ASSERT_NE(val1, val2)\r\n                               ~~~~~~~~~~~~~~~~~~~~~~^~~~~\r\n/usr/local/include/gtest/gtest.h:1942:63: note: expanded from macro 'GTEST_ASSERT_NE'\r\n  ASSERT_PRED_FORMAT2(::testing::internal::CmpHelperNE, val1, val2)\r\n  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^~~~~\r\n/usr/local/include/gtest/gtest_pred_impl.h:166:40: note: expanded from macro 'ASSERT_PRED_FORMAT2'\r\n  GTEST_PRED_FORMAT2_(pred_format, v1, v2, GTEST_FATAL_FAILURE_)\r\n  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~\r\n/usr/local/include/gtest/gtest_pred_impl.h:147:43: note: expanded from macro 'GTEST_PRED_FORMAT2_'\r\n  GTEST_ASSERT_(pred_format(#v1, #v2, v1, v2), \\\r\n  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^~~~~~\r\n/usr/local/include/gtest/gtest_pred_impl.h:77:52: note: expanded from macro 'GTEST_ASSERT_'\r\n  if (const ::testing::AssertionResult gtest_ar = (expression)) \\\r\n                                                   ^~~~~~~~~~\r\ncpp/src/test/autogenerated/cmap_basic_test.cc:88:29: error: reinterpret_cast from\r\n      'nullptr_t' to 'TiXmlAttribute *' is not allowed\r\n  ASSERT_NE(num_cmaps_attr, reinterpret_cast<TiXmlAttribute*>(NULL));\r\n  ~~~~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n/usr/local/include/gtest/gtest.h:1960:54: note: expanded from macro 'ASSERT_NE'\r\n# define ASSERT_NE(val1, val2) GTEST_ASSERT_NE(val1, val2)\r\n                               ~~~~~~~~~~~~~~~~~~~~~~^~~~~\r\n/usr/local/include/gtest/gtest.h:1942:63: note: expanded from macro 'GTEST_ASSERT_NE'\r\n  ASSERT_PRED_FORMAT2(::testing::internal::CmpHelperNE, val1, val2)\r\n  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^~~~~\r\n/usr/local/include/gtest/gtest_pred_impl.h:166:40: note: expanded from macro 'ASSERT_PRED_FORMAT2'\r\n  GTEST_PRED_FORMAT2_(pred_format, v1, v2, GTEST_FATAL_FAILURE_)\r\n  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~\r\n/usr/local/include/gtest/gtest_pred_impl.h:147:43: note: expanded from macro 'GTEST_PRED_FORMAT2_'\r\n  GTEST_ASSERT_(pred_format(#v1, #v2, v1, v2), \\\r\n  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^~~~~~\r\n/usr/local/include/gtest/gtest_pred_impl.h:77:52: note: expanded from macro 'GTEST_ASSERT_'\r\n  if (const ::testing::AssertionResult gtest_ar = (expression)) \\\r\n                                                   ^~~~~~~~~~\r\n3 errors generated.\r\n```\r\n",
          "issue_comments": [
            {
              "comment_username": "jfkthame",
              "comment_create_time": "2017-01-12T12:47:47Z",
              "comment_edit_time": "2017-01-12T12:47:47Z",
              "comment_text": "> Maybe expand NULL to 0.\r\n\r\nI think a more idiomatic C++ solution would be to replace the use of\r\n\r\n    reinterpret_cast<PointerType>(NULL)\r\n\r\nwith\r\n\r\n    static_cast<PointerType>(nullptr)\r\n\r\nthroughout. Does this work as expected for you?"
            },
            {
              "comment_username": "jbeich",
              "comment_create_time": "2017-01-12T13:09:28Z",
              "comment_edit_time": "2017-01-12T13:09:28Z",
              "comment_text": "libc++ (from LLVM) provides a synthetic `nullptr` even in pre-C++11 mode but libstdc++ (from GCC) before 6.0 doesn't expose it unless `-std=c++11` is explicitly passed. However, both work fine with `static_cast<PointerType>(NULL)`."
            }
          ]
        },
        {
          "issue_url": "https://github.com/googlei18n/sfntly/issues/66",
          "issue_title": "cpp unit tests crash on Serialization.Simple",
          "issue_number": 66,
          "issue_text": "```c++\\r\\n$ pkg info -x icu 'g.*test'\\r\\nicu-58.2,1\\r\\ngoogletest-1.8.0.11\\r\\n\\r\\n$ c++ -v\\r\\nFreeBSD clang version 4.0.0 (trunk 291476) (based on LLVM 4.0.0svn)\\r\\nTarget: x86_64-unknown-freebsd12.0\\r\\nThread model: posix\\r\\nInstalledDir: /usr/bin\\r\\nFound CUDA installation: /usr/local/cuda, version 7.5\\r\\n\\r\\n$ cmake /path/to/sfntly/cpp\\r\\n$ gmake\\r\\n$ lldb ./bin/unit_test\\r\\n(lldb) r\\r\\nProcess 50032 launching\\r\\nProcess 50032 launched: 'objdir/bin/unit_test' (x86_64)\\r\\nRunning main() from gtest_main.cc\\r\\n[==========] Running 177 tests from 20 test cases.\\r\\n[----------] Global test environment set-up.\\r\\n[----------] 1 test from TestUtils\\r\\n[ RUN      ] TestUtils.All\\r\\n[       OK ] TestUtils.All (0 ms)\\r\\n[----------] 1 test from TestUtils (0 ms total)\\r\\n\\r\\n[----------] 1 test from SmartPointer\\r\\n[ RUN      ] SmartPointer.All\\r\\n[       OK ] SmartPointer.All (0 ms)\\r\\n[----------] 1 test from SmartPointer (0 ms total)\\r\\n\\r\\n[----------] 2 tests from Serialization\\r\\n[ RUN      ] Serialization.Simple\\r\\nProcess 50032 stopped\\r\\n* thread #1, stop reason = signal SIGSEGV: invalid address (fault address: 0x0)\\r\\n    frame #0: unit_test`sfntly::TestSerialization(void) at serialization_test.cc:70\\r\\n   67     for (size_t i = 0; i < SAMPLE_TTF_KNOWN_TAGS; ++i) {\\r\\n   68         TablePtr original_table = original->GetTable(TTF_KNOWN_TAGS[i]);\\r\\n   69         TablePtr serialized_table = serialized->GetTable(TTF_KNOWN_TAGS[i]);\\r\\n-> 70       EXPECT_EQ(original_table->CalculatedChecksum(),\\r\\n   71                 serialized_table->CalculatedChecksum());\\r\\n   72       EXPECT_EQ(original_table->DataLength(), serialized_table->DataLength());\\r\\n   73\\r\\n\\r\\n(lldb) bt\\r\\n* thread #1, stop reason = signal SIGSEGV: invalid address (fault address: 0x0)\\r\\n  * frame #0: unit_test`sfntly::TestSerialization(void) at serialization_test.cc:70\\r\\n    frame #1: unit_test`Serialization_Simple_Test::TestBody(this=0x000000080282f9f0) at serialization_test.cc:146\\r\\n    frame #2: libgtest.so.0`void testing::internal::HandleSehExceptionsInMethodIfSupported<testing::Test, void>(object=0x000000080282f9f0, method=21 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00, location=\"the test body\")(void), char const*) at gtest.cc:2402\\r\\n    frame #3: libgtest.so.0`void testing::internal::HandleExceptionsInMethodIfSupported<testing::Test, void>(object=0x000000080282f9f0, method=21 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00, location=\"the test body\")(void), char const*) at gtest.cc:2438\\r\\n    frame #4: libgtest.so.0`testing::Test::Run(this=0x000000080282f9f0) at gtest.cc:2474\\r\\n    frame #5: libgtest.so.0`testing::TestInfo::Run(this=0x0000000802838380) at gtest.cc:2656\\r\\n    frame #6: libgtest.so.0`testing::TestCase::Run(this=0x0000000802838460) at gtest.cc:2774\\r\\n    frame #7: libgtest.so.0`testing::internal::UnitTestImpl::RunAllTests(this=0x0000000802828000) at gtest.cc:4649\\r\\n    frame #8: libgtest.so.0`bool testing::internal::HandleSehExceptionsInMethodIfSupported<testing::internal::UnitTestImpl, bool>(object=0x0000000802828000, method=e0 e4 e8 00 08 00 00 00 00 00 00 00 00 00 00 00, location=\"auxiliary test code (environments or event listeners)\")(void), char const*) at gtest.cc:2402\\r\\n    frame #9: libgtest.so.0`bool testing::internal::HandleExceptionsInMethodIfSupported<testing::internal::UnitTestImpl, bool>(object=0x0000000802828000, method=e0 e4 e8 00 08 00 00 00 00 00 00 00 00 00 00 00, location=\"auxiliary test code (environments or event listeners)\")(void), char const*) at gtest.cc:2438\\r\\n    frame #10: libgtest.so.0`testing::UnitTest::Run(this=0x0000000801102b08) at gtest.cc:4257\\r\\n    frame #11: libgtest_main.so.0`RUN_ALL_TESTS(void) at gtest.h:2233\\r\\n    frame #12: libgtest_main.so.0`main(argc=1, argv=0x00007fffffffe480) at gtest_main.cc:37\\r\\n    frame #13: unit_test`_start(ap=<unavailable>, cleanup=<unavailable>) at crt1.c:72\\r\\n\\r\\n(lldb) p original_table\\r\\n(sfntly::TablePtr) $0 = {\\r\\n  p_ = 0x0000000000000000\\r\\n}\\r\\n```\\r\\n",
          "issue_comments": []
        },
        {
          "issue_url": "https://github.com/googlei18n/sfntly/issues/65",
          "issue_title": "cpp unit tests fail to build",
          "issue_number": 65,
          "issue_text": "```c++\r\n$ pkg info -x icu 'g.*test'\r\nicu-58.2,1\r\ngoogletest-1.8.0.11\r\n\r\n$ c++ -v\r\nFreeBSD clang version 4.0.0 (trunk 291476) (based on LLVM 4.0.0svn)\r\nTarget: x86_64-unknown-freebsd12.0\r\nThread model: posix\r\nInstalledDir: /usr/bin\r\nFound CUDA installation: /usr/local/cuda, version 7.5\r\n\r\n$ cmake /path/to/sfntly/cpp\r\n$ gmake\r\n[...]\r\ncpp/src/test/smart_pointer_test.cc:35:30: error: 'ref_count_' is a private member of\r\n      'sfntly::RefCounted<Foo>'\r\n    EXPECT_EQ(size_t(1), p1->ref_count_);\r\n                             ^\r\ncpp/src/sfntly/port/refcount.h:159:18: note: declared private here\r\n  mutable size_t ref_count_;  // reference count of current object\r\n                 ^\r\ncpp/src/test/smart_pointer_test.cc:36:43: error: 'object_counter_' is a private member of\r\n      'sfntly::RefCounted<Foo>'\r\n    EXPECT_EQ(size_t(1), RefCounted<Foo>::object_counter_);\r\n                                          ^\r\ncpp/src/sfntly/port/refcount.h:161:17: note: declared private here\r\n  static size_t object_counter_;\r\n                ^\r\ncpp/src/test/smart_pointer_test.cc:40:30: error: 'ref_count_' is a private member of\r\n      'sfntly::RefCounted<Foo>'\r\n    EXPECT_EQ(size_t(2), p1->ref_count_);\r\n                             ^\r\ncpp/src/sfntly/port/refcount.h:159:18: note: declared private here\r\n  mutable size_t ref_count_;  // reference count of current object\r\n                 ^\r\ncpp/src/test/smart_pointer_test.cc:41:30: error: 'ref_count_' is a private member of\r\n      'sfntly::RefCounted<Foo>'\r\n    EXPECT_EQ(size_t(2), p2->ref_count_);\r\n                             ^\r\ncpp/src/sfntly/port/refcount.h:159:18: note: declared private here\r\n  mutable size_t ref_count_;  // reference count of current object\r\n                 ^\r\ncpp/src/test/smart_pointer_test.cc:42:43: error: 'object_counter_' is a private member of\r\n      'sfntly::RefCounted<Foo>'\r\n    EXPECT_EQ(size_t(1), RefCounted<Foo>::object_counter_);\r\n                                          ^\r\ncpp/src/sfntly/port/refcount.h:161:17: note: declared private here\r\n  static size_t object_counter_;\r\n                ^\r\ncpp/src/test/smart_pointer_test.cc:46:30: error: 'ref_count_' is a private member of\r\n      'sfntly::RefCounted<Foo>'\r\n    EXPECT_EQ(size_t(3), p1->ref_count_);\r\n                             ^\r\ncpp/src/sfntly/port/refcount.h:159:18: note: declared private here\r\n  mutable size_t ref_count_;  // reference count of current object\r\n                 ^\r\ncpp/src/test/smart_pointer_test.cc:47:30: error: 'ref_count_' is a private member of\r\n      'sfntly::RefCounted<Foo>'\r\n    EXPECT_EQ(size_t(3), p2->ref_count_);\r\n                             ^\r\ncpp/src/sfntly/port/refcount.h:159:18: note: declared private here\r\n  mutable size_t ref_count_;  // reference count of current object\r\n                 ^\r\ncpp/src/test/smart_pointer_test.cc:48:30: error: 'ref_count_' is a private member of\r\n      'sfntly::RefCounted<Foo>'\r\n    EXPECT_EQ(size_t(3), p3->ref_count_);\r\n                             ^\r\ncpp/src/sfntly/port/refcount.h:159:18: note: declared private here\r\n  mutable size_t ref_count_;  // reference count of current object\r\n                 ^\r\ncpp/src/test/smart_pointer_test.cc:49:43: error: 'object_counter_' is a private member of\r\n      'sfntly::RefCounted<Foo>'\r\n    EXPECT_EQ(size_t(1), RefCounted<Foo>::object_counter_);\r\n                                          ^\r\ncpp/src/sfntly/port/refcount.h:161:17: note: declared private here\r\n  static size_t object_counter_;\r\n                ^\r\ncpp/src/test/smart_pointer_test.cc:52:30: error: 'ref_count_' is a private member of\r\n      'sfntly::RefCounted<Foo>'\r\n    EXPECT_EQ(size_t(2), p1->ref_count_);\r\n                             ^\r\ncpp/src/sfntly/port/refcount.h:159:18: note: declared private here\r\n  mutable size_t ref_count_;  // reference count of current object\r\n                 ^\r\ncpp/src/test/smart_pointer_test.cc:53:30: error: 'ref_count_' is a private member of\r\n      'sfntly::RefCounted<Foo>'\r\n    EXPECT_EQ(size_t(1), p2->ref_count_);\r\n                             ^\r\ncpp/src/sfntly/port/refcount.h:159:18: note: declared private here\r\n  mutable size_t ref_count_;  // reference count of current object\r\n                 ^\r\ncpp/src/test/smart_pointer_test.cc:54:30: error: 'ref_count_' is a private member of\r\n      'sfntly::RefCounted<Foo>'\r\n    EXPECT_EQ(size_t(2), p3->ref_count_);\r\n                             ^\r\ncpp/src/sfntly/port/refcount.h:159:18: note: declared private here\r\n  mutable size_t ref_count_;  // reference count of current object\r\n                 ^\r\ncpp/src/test/smart_pointer_test.cc:55:43: error: 'object_counter_' is a private member of\r\n      'sfntly::RefCounted<Foo>'\r\n    EXPECT_EQ(size_t(2), RefCounted<Foo>::object_counter_);\r\n                                          ^\r\ncpp/src/sfntly/port/refcount.h:161:17: note: declared private here\r\n  static size_t object_counter_;\r\n                ^\r\ncpp/src/test/smart_pointer_test.cc:58:30: error: 'ref_count_' is a private member of\r\n      'sfntly::RefCounted<Foo>'\r\n    EXPECT_EQ(size_t(1), p1->ref_count_);\r\n                             ^\r\ncpp/src/sfntly/port/refcount.h:159:18: note: declared private here\r\n  mutable size_t ref_count_;  // reference count of current object\r\n                 ^\r\ncpp/src/test/smart_pointer_test.cc:60:43: error: 'object_counter_' is a private member of\r\n      'sfntly::RefCounted<Foo>'\r\n    EXPECT_EQ(size_t(2), RefCounted<Foo>::object_counter_);\r\n                                          ^\r\ncpp/src/sfntly/port/refcount.h:161:17: note: declared private here\r\n  static size_t object_counter_;\r\n                ^\r\ncpp/src/test/smart_pointer_test.cc:63:43: error: 'object_counter_' is a private member of\r\n      'sfntly::RefCounted<Foo>'\r\n    EXPECT_EQ(size_t(1), RefCounted<Foo>::object_counter_);\r\n                                          ^\r\ncpp/src/sfntly/port/refcount.h:161:17: note: declared private here\r\n  static size_t object_counter_;\r\n                ^\r\ncpp/src/test/smart_pointer_test.cc:66:30: error: 'ref_count_' is a private member of\r\n      'sfntly::RefCounted<Foo>'\r\n    EXPECT_EQ(size_t(1), p1->ref_count_);\r\n                             ^\r\ncpp/src/sfntly/port/refcount.h:159:18: note: declared private here\r\n  mutable size_t ref_count_;  // reference count of current object\r\n                 ^\r\ncpp/src/test/smart_pointer_test.cc:67:43: error: 'object_counter_' is a private member of\r\n      'sfntly::RefCounted<Foo>'\r\n    EXPECT_EQ(size_t(1), RefCounted<Foo>::object_counter_);\r\n                                          ^\r\ncpp/src/sfntly/port/refcount.h:161:17: note: declared private here\r\n  static size_t object_counter_;\r\n                ^\r\ncpp/src/test/smart_pointer_test.cc:70:30: error: 'ref_count_' is a private member of\r\n      'sfntly::RefCounted<Foo>'\r\n    EXPECT_EQ(size_t(1), p1->ref_count_);\r\n                             ^\r\ncpp/src/sfntly/port/refcount.h:159:18: note: declared private here\r\n  mutable size_t ref_count_;  // reference count of current object\r\n                 ^\r\ncpp/src/test/smart_pointer_test.cc:71:43: error: 'object_counter_' is a private member of\r\n      'sfntly::RefCounted<Foo>'\r\n    EXPECT_EQ(size_t(1), RefCounted<Foo>::object_counter_);\r\n                                          ^\r\ncpp/src/sfntly/port/refcount.h:161:17: note: declared private here\r\n  static size_t object_counter_;\r\n                ^\r\ncpp/src/test/smart_pointer_test.cc:73:41: error: 'object_counter_' is a private member of\r\n      'sfntly::RefCounted<Foo>'\r\n  EXPECT_EQ(size_t(0), RefCounted<Foo>::object_counter_);\r\n                                        ^\r\ncpp/src/sfntly/port/refcount.h:161:17: note: declared private here\r\n  static size_t object_counter_;\r\n                ^\r\n21 errors generated.\r\n```\r\n",
          "issue_comments": []
        },
        {
          "issue_url": "https://github.com/googlei18n/sfntly/issues/55",
          "issue_title": "Could not add some characters to the output subset font",
          "issue_number": 55,
          "issue_text": "I use command like the following:\n\nsfnTool   -s \"测试字体\" c:\\windows\\fonts\\华文彩云.ttf c:\\out.ttf\n\nbut the output font have no characters, just an empty one.\n\n[华文彩云.ttf](http://static.focusky.com/common/fonts/caiyun.ttf)\n",
          "issue_comments": [
            {
              "comment_username": "Jijun",
              "comment_create_time": "2019-02-10T03:03:54Z",
              "comment_edit_time": "2019-02-10T03:03:54Z",
              "comment_text": "it's a bug in subset com.google.typography.font.tools.sfnttool.GlyphCoverage.getBestCMap() , RenumberingCMapTableSubsetter.getCMapFormat()\\r\\nthe font 华文彩云.ttf have two cmap format4 subtable：\\r\\n```\\r\\ncmap_format_4 platformID=\"1\" platEncID=\"25\"\\r\\ncmap_format_4 platformID=\"3\" platEncID=\"1\"\\r\\n```\\r\\nthe code never get the bigger cmap4\\r\\n```\\r\\ncmap_format_4 platformID=\"3\" platEncID=\"1\"\\r\\n```\\r\\nbut use \\r\\n```\\r\\ncmap_format_4 platformID=\"1\" platEncID=\"25\"\\r\\n```\\r\\n "
            }
          ]
        },
        {
          "issue_url": "https://github.com/googlei18n/sfntly/issues/46",
          "issue_title": "Can't compile under openjdk-6 6b35-1.13.7-1ubuntu1",
          "issue_number": 46,
          "issue_text": "```\nreedy@ubuntu64-web-esxi:~/sfntly-read-only/java$ ant\nBuildfile: /home/reedy/sfntly-read-only/java/build.xml\n\ncompile:\n    [mkdir] Created dir: /home/reedy/sfntly-read-only/java/build/classes\n    [javac] Compiling 200 source files to /home/reedy/sfntly-read-only/java/build/classes\n    [javac] /home/reedy/sfntly-read-only/java/src/com/google/typography/font/sfntly/sample/sfview/OtTableTagger.java:128: cannot find symbol\n    [javac] symbol  : class SafeVarargs\n    [javac] location: class com.google.typography.font.sfntly.sample.sfview.OtTableTagger\n    [javac]   @SafeVarargs\n    [javac]    ^\n    [javac] Note: /home/reedy/sfntly-read-only/java/src/com/google/typography/font/sfntly/sample/sfview/OtTableTagger.java uses unchecked or unsafe operations.\n    [javac] Note: Recompile with -Xlint:unchecked for details.\n    [javac] 1 error\n\nBUILD FAILED\n/home/reedy/sfntly-read-only/java/common.xml:21: Compile failed; see the \ncompiler error output for details.\n\nTotal time: 14 seconds\nreedy@ubuntu64-web-esxi:~/sfntly-read-only/java$ javac -version\njavac 1.6.0_35\nreedy@ubuntu64-web-esxi:~/sfntly-read-only/java$ java -version\njava version \"1.6.0_35\"\nOpenJDK Runtime Environment (IcedTea6 1.13.7) (6b35-1.13.7-1ubuntu1)\nOpenJDK 64-Bit Server VM (build 23.25-b01, mixed mode)\nreedy@ubuntu64-web-esxi:~/sfntly-read-only/java$ dpkg -l | grep jdk-6\nii  openjdk-6-jdk:amd64                  6b35-1.13.7-1ubuntu1                \namd64        OpenJDK Development Kit (JDK)\nii  openjdk-6-jre:amd64                  6b35-1.13.7-1ubuntu1                \namd64        OpenJDK Java runtime, using Hotspot JIT\nii  openjdk-6-jre-headless:amd64         6b35-1.13.7-1ubuntu1                \namd64        OpenJDK Java runtime, using Hotspot JIT (headless)\nii  openjdk-6-jre-lib                    6b35-1.13.7-1ubuntu1                \nall          OpenJDK Java runtime (architecture independent libraries)\nreedy@ubuntu64-web-esxi:~/sfntly-read-only/java$\n\n\n\nSeemed to work fine on openjdk-6 6b33-1.13.5-1ubuntu1\n```\n\nOriginal issue reported on code.google.com by `tehre...@gmail.com` on 9 May 2015 at 10:22\n",
          "issue_comments": []
        },
        {
          "issue_url": "https://github.com/googlei18n/sfntly/issues/45",
          "issue_title": "Proposed build instructions not working on Mac OS X 10.10",
          "issue_number": 45,
          "issue_text": "```\nBoth the make and make lines will produce errors on OS X (tested on Yosemite). \nIf you run `cmake ..` from within build, you get: `CMake Error: The source \ndirectory \"/Applications/sfntly\" does not appear to contain CMakeLists.txt.`\n\nsfntly Revision 239 on OS X 10.10 (14A389)\n\nHere is what I did:\nsvn checkout http://sfntly.googlecode.com/svn/trunk/ /Applications/sfntly/\ncd /Applications/sfntly/\nmkdir build\ncd build\ncmake ..\n\nThen I get the error. If I continue with …\n\ncmake ../cpp\n\n… but then I get this output:\n\n-- The C compiler identification is AppleClang 6.0.0.6000054\n-- The CXX compiler identification is AppleClang 6.0.0.6000054\n-- Check for working C compiler: /usr/bin/cc\n-- Check for working C compiler: /usr/bin/cc -- works\n-- Detecting C compiler ABI info\n-- Detecting C compiler ABI info - done\n-- Check for working CXX compiler: /usr/bin/c++\n-- Check for working CXX compiler: /usr/bin/c++ -- works\n-- Detecting CXX compiler ABI info\n-- Detecting CXX compiler ABI info - done\n-- Configuring done\nCMake Error at CMakeLists.txt:84 (add_executable):\n  Cannot find source file:\n    ext/gtest/src/gtest-all.cc\n  Tried extensions .c .C .c++ .cc .cpp .cxx .m .M .mm .h .hh .h++ .hm .hpp\n  .hxx .in .txx\n-- Build files have been written to: /Applications/sfntly/build\n\nThen I get this error from `make`:\n\nmake: *** No targets specified and no makefile found.  Stop.\n```\n\nOriginal issue reported on code.google.com by `mekkab...@gmail.com` on 3 Nov 2014 at 3:38\n",
          "issue_comments": [
            {
              "comment_username": "GoogleCodeExporter",
              "comment_create_time": "2015-06-15T19:39:59Z",
              "comment_edit_time": "2015-06-15T19:39:59Z",
              "comment_text": "```\nSorry, I meant \"both the cmake and make lines\", but autocorrect removed the c.\n```\n\nOriginal comment by `mekkab...@gmail.com` on 3 Nov 2014 at 3:40\n"
            }
          ]
        },
        {
          "issue_url": "https://github.com/googlei18n/sfntly/issues/42",
          "issue_title": "Incorrect data type for maxp table version number",
          "issue_number": 42,
          "issue_text": "```\nAs per the specification, the table version number is Fixed and not UShort.\n\nhttp://www.microsoft.com/typography/otspec/maxp.htm\n```\n\nOriginal issue reported on code.google.com by `tinned...@yahoo.com` on 4 Dec 2013 at 12:21\n\nAttachments:\n- [MaximumProfileTable.java.patch](https://storage.googleapis.com/google-code-attachments/sfntly/issue-42/comment-0/MaximumProfileTable.java.patch)\n",
          "issue_comments": [
            {
              "comment_username": "GoogleCodeExporter",
              "comment_create_time": "2015-06-15T19:39:57Z",
              "comment_edit_time": "2015-06-15T19:39:57Z",
              "comment_text": "Original comment by `stua...@google.com` on 29 Jan 2014 at 12:58\n- Changed state: **Accepted**\n"
            }
          ]
        },
        {
          "issue_url": "https://github.com/googlei18n/sfntly/issues/41",
          "issue_title": "UnicodeRange enum missing entries",
          "issue_number": 41,
          "issue_text": "```\nUnicodeRange appears to be missing two entries:\n34  Combining Diacritical Marks For Symbols\n35  Letterlike Symbols\n\n\n```\n\nOriginal issue reported on code.google.com by `tinned...@yahoo.com` on 30 Nov 2013 at 2:49\n\nAttachments:\n- [OS2Table.java.patch](https://storage.googleapis.com/google-code-attachments/sfntly/issue-41/comment-0/OS2Table.java.patch)\n",
          "issue_comments": [
            {
              "comment_username": "GoogleCodeExporter",
              "comment_create_time": "2015-06-15T19:39:57Z",
              "comment_edit_time": "2015-06-15T19:39:57Z",
              "comment_text": "```\nThe incorrect patch was uploaded\n```\n\nOriginal comment by `tinned...@yahoo.com` on 30 Nov 2013 at 2:52\n\nAttachments:\n- [OS2Table.java.patch](https://storage.googleapis.com/google-code-attachments/sfntly/issue-41/comment-1/OS2Table.java.patch)\n"
            },
            {
              "comment_username": "GoogleCodeExporter",
              "comment_create_time": "2015-06-15T19:39:57Z",
              "comment_edit_time": "2015-06-15T19:39:57Z",
              "comment_text": "Original comment by `stua...@google.com` on 29 Jan 2014 at 12:55\n- Changed state: **Accepted**\n"
            }
          ]
        },
        {
          "issue_url": "https://github.com/googlei18n/sfntly/issues/39",
          "issue_title": "QNX build is broken",
          "issue_number": 39,
          "issue_text": "```\nWhen building on QNX, the build is broken.\n\nsfntly uses \"#include <cstddef>\" to get size_t, but that only puts size_t in \nthe std:: namespace with the strict library on QNX.\n```\n\nOriginal issue reported on code.google.com by `efidler....@gmail.com` on 29 Oct 2013 at 6:02\n",
          "issue_comments": [
            {
              "comment_username": "GoogleCodeExporter",
              "comment_create_time": "2015-06-15T19:39:56Z",
              "comment_edit_time": "2015-06-15T19:39:56Z",
              "comment_text": "```\nhttps://codereview.appspot.com/19330043\n```\n\nOriginal comment by `efidler....@gmail.com` on 29 Oct 2013 at 6:07\n"
            },
            {
              "comment_username": "GoogleCodeExporter",
              "comment_create_time": "2015-06-15T19:39:56Z",
              "comment_edit_time": "2015-06-15T19:39:56Z",
              "comment_text": "```\nThis was fixed in r230. I don't have EditBugs permission to close it.\n```\n\nOriginal comment by `efidler....@gmail.com` on 11 Nov 2013 at 5:24\n"
            }
          ]
        },
        {
          "issue_url": "https://github.com/googlei18n/sfntly/issues/37",
          "issue_title": "Renumbering subsetter should support more cmap formats",
          "issue_number": 37,
          "issue_text": "```\nThe renumbering subsetter only reads format 4 cmaps. This is a limitation in \ncomputeMapping in the RenumberingCMapTableSubsetter.\n\nCurrently there is only a builder for format 4 cmaps, so I'd expect sfntly to \nsupport all 16-bit cmaps and just write them out as format 4, yet it doesn't. \nIt should also honour the Subsetter's cmapId() list.\n\nFor 32-bit cmaps a format 12 builder is needed.\n```\n\nOriginal issue reported on code.google.com by `j...@jahewson.com` on 7 Oct 2013 at 9:17\n",
          "issue_comments": [
            {
              "comment_username": "GoogleCodeExporter",
              "comment_create_time": "2015-06-15T19:39:55Z",
              "comment_edit_time": "2015-06-15T19:39:55Z",
              "comment_text": "Original comment by `stua...@google.com` on 29 Jan 2014 at 12:56\n- Added labels: **Priority-Low**, **Type-Enhancement**\n- Removed labels: **Priority-Medium**, **Type-Defect**\n"
            }
          ]
        },
        {
          "issue_url": "https://github.com/googlei18n/sfntly/issues/36",
          "issue_title": "Missing utility function to compute OS-2 unicode range bits from unicode values",
          "issue_number": 36,
          "issue_text": "```\nWhat steps will reproduce the problem?\n1.\n2.\n3.\n\nWhat is the expected output? What do you see instead?\n\n\nWhat version of the product are you using? On what operating system?\n\n\nPlease provide any additional information below.\nThis is an enhancement suggestion (is there a \"new enhacement\" hidden somewhere \nthat I missed?).\nThe attached patch provides a class to compute the bits.\n\n\n```\n\nOriginal issue reported on code.google.com by `alexgel...@web.de` on 2 Sep 2013 at 4:13\n\nAttachments:\n- [OS2UnicodeBits.patch](https://storage.googleapis.com/google-code-attachments/sfntly/issue-36/comment-0/OS2UnicodeBits.patch)\n",
          "issue_comments": [
            {
              "comment_username": "GoogleCodeExporter",
              "comment_create_time": "2015-06-15T19:39:54Z",
              "comment_edit_time": "2015-06-15T19:39:54Z",
              "comment_text": "Original comment by `stua...@google.com` on 16 Oct 2013 at 10:49\n- Added labels: **Type-Enhancement**\n- Removed labels: **Type-Defect**\n"
            }
          ]
        },
        {
          "issue_url": "https://github.com/googlei18n/sfntly/issues/35",
          "issue_title": "Function Fontinfo.listGlyphDimensionBounds() can fail with an ArrayIndexOutOfBoundsException on some fonts",
          "issue_number": 35,
          "issue_text": "```\nWhat steps will reproduce the problem?\n1. Call the function for a font that has glyphs with no data \n(LocaTable.glyphLength()==0\n2.\n3.\n\nWhat is the expected output? What do you see instead?\n\n\nWhat version of the product are you using? On what operating system?\n\n\nPlease provide any additional information below.\nThe attached patch fixes the issue by ignoring such glyphs during computation.\n\n\n```\n\nOriginal issue reported on code.google.com by `alexgel...@web.de` on 2 Sep 2013 at 3:57\n\nAttachments:\n- [131020.patch](https://storage.googleapis.com/google-code-attachments/sfntly/issue-35/comment-0/131020.patch)\n",
          "issue_comments": []
        },
        {
          "issue_url": "https://github.com/googlei18n/sfntly/issues/34",
          "issue_title": "Function OS2Table.setUsWinDescent() sets usWinAscent instead",
          "issue_number": 34,
          "issue_text": "```\nWhat steps will reproduce the problem?\n1. Reading the source code of the method OS2Table.setUsWinDescent(). The error \nis obvious (probably a copy/paste error)\n2.\n3.\n\nWhat is the expected output? What do you see instead?\n\n\nWhat version of the product are you using? On what operating system?\n\n\nPlease provide any additional information below.\nThe attached patch fixes the problem.\n\n\n```\n\nOriginal issue reported on code.google.com by `alexgel...@web.de` on 2 Sep 2013 at 3:48\n\nAttachments:\n- [131019.patch](https://storage.googleapis.com/google-code-attachments/sfntly/issue-34/comment-0/131019.patch)\n",
          "issue_comments": [
            {
              "comment_username": "GoogleCodeExporter",
              "comment_create_time": "2015-06-15T19:39:54Z",
              "comment_edit_time": "2015-06-15T19:39:54Z",
              "comment_text": "```\nThis needs higher priority because not setting the usWinDescent value when \ncreating a font will lead to an unusable font.\n```\n\nOriginal comment by `rtaylor...@gmail.com` on 2 Dec 2014 at 6:15\n"
            }
          ]
        },
        {
          "issue_url": "https://github.com/googlei18n/sfntly/issues/33",
          "issue_title": "NameTable iteration breaks on some fonts",
          "issue_number": 33,
          "issue_text": "```\nSome fonts have name table entries where the name bytes field is empty (zero \nlength), and additionally has the index to the entry equal to the bounds of the \nname table.\n\nExtra bounds checking introduced with r155 causes sfntly to crash on such \nentries with an IndexOutOfBounds exception. I believe that in this case, such \nentries are valid (since the length is zero) and sfntly should not throw an \nexception.\n\n```\n\nOriginal issue reported on code.google.com by `tinned...@yahoo.com` on 10 Jul 2013 at 4:22\n\nAttachments:\n- [NameTableTest.java](https://storage.googleapis.com/google-code-attachments/sfntly/issue-33/comment-0/NameTableTest.java)\n",
          "issue_comments": []
        },
        {
          "issue_url": "https://github.com/googlei18n/sfntly/issues/30",
          "issue_title": "add maven support and deploy 2 central",
          "issue_number": 30,
          "issue_text": "```\\nHi,\\n\\nI'm a member of project https://code.google.com/p/xdocreport/.\\n\\nThis is a set of java libs that enable templating in docx/ odt and allows to \\nexport to PDF the merged content.\\n\\nWe deploy our jars to central maven repo : \\nhttp://search.maven.org/#search%7Cga%7C1%7Cxdocreport.\\n\\nWe want to make a \"Google App Engine\" extension of our libs and we need a \\ndependency on your lib (sfntly-java).\\n\\nUnfortunatly, sfntly-jaa cannot be found on central maven repo.\\n\\nIf you are interested I worked on a fork of the project  \\nhttps://github.com/pascalleclercq/sfntly-java \\nin order to \"mavenize\" It : I may commit my work on your repo.\\n\\nbest regards\\n```\\n\\nOriginal issue reported on code.google.com by `pascal.leclercq` on 18 Mar 2013 at 1:48\\n",
          "issue_comments": [
            {
              "comment_username": "GoogleCodeExporter",
              "comment_create_time": "2015-06-15T19:39:52Z",
              "comment_edit_time": "2015-06-15T19:39:52Z",
              "comment_text": "```\nUnfortunately this is probably going to somewhat of a low priority item.\n```\n\nOriginal comment by `stua...@google.com` on 30 Mar 2013 at 12:59\n- Added labels: **Priority-Low**, **Type-Enhancement**\n- Removed labels: **Priority-Medium**, **Type-Defect**\n"
            },
            {
              "comment_username": "GoogleCodeExporter",
              "comment_create_time": "2015-06-15T19:39:52Z",
              "comment_edit_time": "2015-06-15T19:39:52Z",
              "comment_text": "```\nThat is unfortunate because this is the most starred issue in the tracker. Most \nJava developers outside of the Google build system rely on maven central to \nmanage their external dependencies more easily.\n\nIt looks like pascal has done enough work to be able to send you a code review \nwithout too much effort. Is there anybody on your team who can review it?\n```\n\nOriginal comment by `dpar...@netflix.com` on 14 Mar 2014 at 1:08\n"
            },
            {
              "comment_username": "nikyoudale",
              "comment_create_time": "2015-09-29T07:42:59Z",
              "comment_edit_time": "2015-09-29T07:42:59Z",
              "comment_text": "+1\n"
            },
            {
              "comment_username": "thomashunziker",
              "comment_create_time": "2017-05-02T19:11:29Z",
              "comment_edit_time": "2017-05-02T19:11:29Z",
              "comment_text": "+1"
            }
          ]
        },
        {
          "issue_url": "https://github.com/googlei18n/sfntly/issues/29",
          "issue_title": "C++ Tables (font and cmap) in wrong order",
          "issue_number": 29,
          "issue_text": "```\nOTS santizer rejected fonts serialized by C++ library because wrong order of \nfont tables. And because of wrong ordered cmap tables.\n\nThis font could not be loaded in web browser (except IE) after I changed \ncomparator classes it started to work.\n\nJan Hruby\n```\n\nOriginal issue reported on code.google.com by `jhruby....@gmail.com` on 28 Jan 2013 at 6:44\n",
          "issue_comments": [
            {
              "comment_username": "GoogleCodeExporter",
              "comment_create_time": "2015-06-15T19:39:51Z",
              "comment_edit_time": "2015-06-15T19:39:51Z",
              "comment_text": "```\nI ran into this problem as well. This bug is especially problematic because it \ngenerates fonts that will be rejected by the OTS sanitizer \n(<https://code.google.com/p/ots/source/browse/trunk/src/ots.cc#425>), which is \nused by a bunch of browsers to determine whether or not to allow a font.\n\nI think the problem is that the comparator in header.cc \n<https://code.google.com/p/sfntly/source/browse/trunk/cpp/src/sfntly/table/heade\nr.cc?r=89#63> causes the headers to be sorted in reverse order:\n\nbool HeaderComparatorByTag::operator() (const HeaderPtr lhs,\n                                        const HeaderPtr rhs) {\n  return lhs->tag_ > rhs->tag_;\n}\n\nSwitching the \">\" to a \"<\" fixes the problem. Note that this is probably a \nmis-port of the \ncom.google.typography.font.sfntly.table.Header.COMPARATOR_BY_TAG \n<https://code.google.com/p/sfntly/source/browse/trunk/java/src/com/google/typogr\naphy/font/sfntly/table/Header.java?r=104#53>, which sorts like:\n\n  public static final Comparator<Header> COMPARATOR_BY_TAG = new Comparator<Header>() {\n    @Override\n    public int compare(Header h1, Header h2) {\n      return h1.tag - h2.tag;\n    } \n\n```\n\nOriginal comment by `mprud...@gmail.com` on 22 Dec 2013 at 3:06\n"
            }
          ]
        },
        {
          "issue_url": "https://github.com/googlei18n/sfntly/issues/25",
          "issue_title": "C++ inline methods in nested classes break with -fPIC and gcc",
          "issue_number": 25,
          "issue_text": "```\nThere are some inline methods in nested classes. These break when sfntly is \ncompiled with gcc using -fPIC (for creation of a shared library). The methods \nreturn incorrect results, which can lead to generation of corrupted \nfonts/segfaults/etc.\n\nHere is a patch that fixes this for the loca table (there may be other places \nthat exhibit this issue, but this is the only one that impacted me, so far).\n\nhttp://bazaar.launchpad.net/~kovid/calibre/trunk/revision/13579\n```\n\nOriginal issue reported on code.google.com by `adse...@calibre-ebook.com` on 30 Oct 2012 at 6:10\n",
          "issue_comments": [
            {
              "comment_username": "GoogleCodeExporter",
              "comment_create_time": "2015-06-15T19:39:49Z",
              "comment_edit_time": "2015-06-15T19:39:49Z",
              "comment_text": "```\nAnother instance: \nhttp://bazaar.launchpad.net/~kovid/calibre/trunk/revision/13583\n```\n\nOriginal comment by `adse...@calibre-ebook.com` on 30 Oct 2012 at 11:00\n"
            }
          ]
        },
        {
          "issue_url": "https://github.com/googlei18n/sfntly/issues/23",
          "issue_title": "Incorrect font checksum calculation",
          "issue_number": 23,
          "issue_text": "```\nWhat steps will reproduce the problem?\n1. Load any font for building\n2. Build it\n3. Observe the checksum adjustment value in the 'head' table\n\nWhat is the expected output? What do you see instead?\nA valid checksum adjustment is expected - an invalid one is seen instead, \nverifiable through using Microsoft's font validator.\n\nWhat version of the product are you using? On what operating system?\nr147, Java version on Windows XP\n\nPlease provide any additional information below.\nI think the issue is that when the font checksum is calculated in \nbuildTablesFromBuilders, it does not include checksumming the offset table. \nFrom http://support.microsoft.com/kb/102354, the whole file including the \noffset table (except for the checksum adjustment field) has to be included in \nthe checksum, which is not done in sfntly (the offset table is only produced on \nserialization)\n\n```\n\nOriginal issue reported on code.google.com by `randomra...@yahoo.com.au` on 26 Oct 2012 at 6:22\n",
          "issue_comments": [
            {
              "comment_username": "GoogleCodeExporter",
              "comment_create_time": "2015-06-15T19:39:48Z",
              "comment_edit_time": "2015-06-15T19:39:48Z",
              "comment_text": "```\n[deleted comment]\n```\n"
            },
            {
              "comment_username": "GoogleCodeExporter",
              "comment_create_time": "2015-06-15T19:39:48Z",
              "comment_edit_time": "2015-06-15T19:39:48Z",
              "comment_text": "```\nActually it seems that going straight from loading to building will mean that \nthe checksum adjustment is zero for some reason...\n\nWhen using the font validator, the value it lists next to error E1305 is the \nchecksum adjustment value\n```\n\nOriginal comment by `randomra...@yahoo.com.au` on 26 Oct 2012 at 7:18\n"
            },
            {
              "comment_username": "GoogleCodeExporter",
              "comment_create_time": "2015-06-15T19:39:48Z",
              "comment_edit_time": "2015-06-15T19:39:48Z",
              "comment_text": "Original comment by `stua...@google.com` on 30 Mar 2013 at 1:00\n"
            },
            {
              "comment_username": "GoogleCodeExporter",
              "comment_create_time": "2015-06-15T19:39:48Z",
              "comment_edit_time": "2015-06-15T19:39:48Z",
              "comment_text": "```\nOk, I see what the problem is here. I'm going to have to change the code to do \nmore of the offset table work in the Font.Builder when building the Font rather \nthan in the serialization. This will mean changing a few APIs that I think are \nprobably not used that much or perhaps not all (table ordering parameters \nduring serialization).\n```\n\nOriginal comment by `stua...@google.com` on 3 Apr 2013 at 11:04\n- Changed state: **Started**\n"
            }
          ]
        }
      ]
    }
  },
  {
    "github_url": "https://github.com/miltonio/milton2",
    "github_info": {
      "name": "miltonio/milton2",
      "language": "Java"
    },
    "github_branches": {
      "branch_datas": [
        {
          "branch_version": "master",
          "branch_url": "https://github.com/miltonio/milton2/tree/master",
          "branch_download_url": "https://github.com/miltonio/milton2/archive/master.zip"
        },
        {
          "branch_version": "milton2.0.3",
          "branch_url": "https://github.com/miltonio/milton2/tree/milton2.0.3",
          "branch_download_url": "https://github.com/miltonio/milton2/archive/milton2.0.3.zip"
        }
      ]
    },
    "github_pull_requests": {
      "pull_datas": [
        {
          "pull_number": 133,
          "pull_title": "Origin/master",
          "pull_version": "miltonio:master",
          "pull_version_url": "https://github.com/miltonio/milton2/tree/master"
        }
      ]
    },
    "github_issues": {
      "issue_datas": [
        {
          "issue_url": "https://github.com/miltonio/milton2/issues/140",
          "issue_title": "Failed to read license xml",
          "issue_number": 140,
          "issue_text": "Works with version `2.8.0.1` but with `2.8.0.2`, I get:\r\n```\r\nINFO  2020 Sep 19, 15:55:46 [main] io - Initializing Milton2 Webdav library Enterprise edition. . Local milton version: 2.8.0.2\r\nWARN  2020 Sep 19, 15:55:46 [main] io - Exception checking for milton commercial license: io.milton.webdav.exceptions.ServerException: Failed to read license xml. If you have a commercial license please check with the licensor at http://milton.io\r\n\r\n[Fatal Error] :1:1: Content is not allowed in prolog.\r\nio.milton.webdav.exceptions.ServerException: Failed to read license xml.\r\n\tat io.milton.webdav.license.LicenseValidator.checkLicense(Unknown Source)\r\n\tat io.milton.webdav.utils.LockUtils.getValidatedLicenseProperties(Unknown Source)\r\n\tat io.milton.webdav.utils.LockUtils.displayCopyrightNotice(Unknown Source)\r\n\tat io.milton.webdav.utils.LockUtils.<clinit>(Unknown Source)\r\n\tat io.milton.http.webdav2.LockHandler.<init>(LockHandler.java:43)\r\n\tat io.milton.http.webdav2.WebDavLevel2Protocol.<init>(WebDavLevel2Protocol.java:51)\r\n\tat io.milton.ent.config.HttpManagerBuilderEnt.buildProtocolHandlers(HttpManagerBuilderEnt.java:209)\r\n\tat io.milton.config.HttpManagerBuilder.initProtocols(HttpManagerBuilder.java:519)\r\n\tat io.milton.config.HttpManagerBuilder.init(HttpManagerBuilder.java:488)\r\n\tat io.milton.config.HttpManagerBuilder.init(HttpManagerBuilder.java:405)\r\n\tat io.milton.config.HttpManagerBuilder.buildHttpManager(HttpManagerBuilder.java:530)\r\n\tat io.milton.servlet.DefaultMiltonConfigurator.build(DefaultMiltonConfigurator.java:174)\r\n\tat io.milton.servlet.DefaultMiltonConfigurator.configure(DefaultMiltonConfigurator.java:156)\r\n\tat io.milton.servlet.MiltonFilter.init(MiltonFilter.java:69)\r\n\tat com.seanergie.webdav.servlet.WebDavFilter.init(WebDavFilter.java:25)\r\n\tat org.apache.catalina.core.ApplicationFilterConfig.initFilter(ApplicationFilterConfig.java:270)\r\n\tat org.apache.catalina.core.ApplicationFilterConfig.getFilter(ApplicationFilterConfig.java:251)\r\n\tat org.apache.catalina.core.ApplicationFilterConfig.<init>(ApplicationFilterConfig.java:102)\r\n\tat org.apache.catalina.core.StandardContext.filterStart(StandardContext.java:4528)\r\n\tat org.apache.catalina.core.StandardContext.startInternal(StandardContext.java:5165)\r\n\tat org.apache.catalina.util.LifecycleBase.start(LifecycleBase.java:183)\r\n\tat org.apache.catalina.core.ContainerBase$StartChild.call(ContainerBase.java:1384)\r\n\tat org.apache.catalina.core.ContainerBase$StartChild.call(ContainerBase.java:1374)\r\n\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\r\n\tat org.apache.tomcat.util.threads.InlineExecutorService.execute(InlineExecutorService.java:75)\r\n\tat java.base/java.util.concurrent.AbstractExecutorService.submit(AbstractExecutorService.java:140)\r\n\tat org.apache.catalina.core.ContainerBase.startInternal(ContainerBase.java:909)\r\n\tat org.apache.catalina.core.StandardHost.startInternal(StandardHost.java:843)\r\n\tat org.apache.catalina.util.LifecycleBase.start(LifecycleBase.java:183)\r\n\tat org.apache.catalina.core.ContainerBase$StartChild.call(ContainerBase.java:1384)\r\n\tat org.apache.catalina.core.ContainerBase$StartChild.call(ContainerBase.java:1374)\r\n\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\r\n\tat org.apache.tomcat.util.threads.InlineExecutorService.execute(InlineExecutorService.java:75)\r\n\tat java.base/java.util.concurrent.AbstractExecutorService.submit(AbstractExecutorService.java:140)\r\n\tat org.apache.catalina.core.ContainerBase.startInternal(ContainerBase.java:909)\r\n\tat org.apache.catalina.core.StandardEngine.startInternal(StandardEngine.java:262)\r\n\tat org.apache.catalina.util.LifecycleBase.start(LifecycleBase.java:183)\r\n\tat org.apache.catalina.core.StandardService.startInternal(StandardService.java:421)\r\n\tat org.apache.catalina.util.LifecycleBase.start(LifecycleBase.java:183)\r\n\tat org.apache.catalina.core.StandardServer.startInternal(StandardServer.java:930)\r\n\tat org.apache.catalina.util.LifecycleBase.start(LifecycleBase.java:183)\r\n\tat org.apache.catalina.startup.Tomcat.start(Tomcat.java:486)\r\n\tat com.seanergie.web.embedded.WebappLauncher.run(WebappLauncher.java:74)\r\n\tat com.equitativa.intranet.Main.main(Main.java:8)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:564)\r\n\tat com.intellij.rt.execution.application.AppMainV2.main(AppMainV2.java:128)\r\nCaused by: org.xml.sax.SAXParseException; lineNumber: 1; columnNumber: 1; Content is not allowed in prolog.\r\n\tat java.xml/com.sun.org.apache.xerces.internal.parsers.DOMParser.parse(DOMParser.java:261)\r\n\tat java.xml/com.sun.org.apache.xerces.internal.jaxp.DocumentBuilderImpl.parse(DocumentBuilderImpl.java:339)\r\n\tat java.xml/javax.xml.parsers.DocumentBuilder.parse(DocumentBuilder.java:122)\r\n\t... 49 more\r\n```\r\n\r\nThe `milton.license.properties` and `milton.license.sig` are located at the root of one of our jar files.\r\nI get the same error if I move the license files at the root of the webapp.",
          "issue_comments": [
            {
              "comment_username": "ophiuhus",
              "comment_create_time": "2020-09-21T16:56:14Z",
              "comment_edit_time": "2020-09-21T16:56:14Z",
              "comment_text": "Thank you for your question. You are right, the new license is required to activate v2.8.02+. To use the versions 2.8.0.2+, please log-in to the product download area here: https://milton.io/downloads/ with the email that was specified during the purchase and download the new license. Please note that you may need to reset your password."
            }
          ]
        },
        {
          "issue_url": "https://github.com/miltonio/milton2/issues/139",
          "issue_title": "NullPointerException in SpecificityUtils",
          "issue_number": 139,
          "issue_text": "```\r\nCaused by: java.lang.NullPointerException\r\n\tat io.milton.http.annotated.SpecificityUtils.sourceSpecifityIndex(SpecificityUtils.java:29) ~[milton-server-ce-2.7.2.4.jar:?]\r\n\tat io.milton.http.annotated.AbstractAnnotationHandler.getBestMethod(AbstractAnnotationHandler.java:100) ~[milton-server-ce-2.7.2.4.jar:?]\r\n\tat io.milton.http.annotated.AbstractAnnotationHandler.isCompatible(AbstractAnnotationHandler.java:183) ~[milton-server-ce-2.7.2.4.jar:?]\r\n\tat io.milton.http.annotated.AnnotationResourceFactory.instantiate(AnnotationResourceFactory.java:648) ~[milton-server-ce-2.7.2.4.jar:?]\r\n\tat io.milton.http.annotated.AnnotationResourceFactory.createAndAppend(AnnotationResourceFactory.java:792) ~[milton-server-ce-2.7.2.4.jar:?]\r\n\tat io.milton.http.annotated.ChildrenOfAnnotationHandler.execute(ChildrenOfAnnotationHandler.java:68) ~[milton-server-ce-2.7.2.4.jar:?]\r\n\tat io.milton.http.annotated.AnnoCollectionResource.initChildren(AnnoCollectionResource.java:176) ~[milton-server-ce-2.7.2.4.jar:?]\r\n\t... 42 more\r\n```\r\n\r\nAn NPE is occurring in https://github.com/miltonio/milton2/blob/master/milton-server-ce/src/main/java/io/milton/http/annotated/SpecificityUtils.java#L28.\r\n\r\nThe following happens:\r\n\r\n```java\r\n# Params\r\nmethodType = \"interface java.nio.file.Path\"\r\nactualType = \"class com.google.common.jimfs.JimfsPath\"\r\n\r\n# Code Flow\r\nc = actualType // c = \"class com.google.common.jimfs.JimfsPath\"\r\ni = 0\r\nwhile (!methodType.equals(c)) // true\r\nc = c.getSuperclass();  // c = \"class java.lang.Object\"\r\nwhile (!methodType.equals(c)) // true\r\nc = c.getSuperclass(); // c = null\r\nwhile(!methodType.equals(c)) // true\r\nc = c.getSuperclass(); // ==> NullPointerException, because c is null\r\n```\r\n\r\nMy controller looks like this:\r\n\r\n```java\r\npackage foo.bar;\r\n\r\nimport com.google.common.jimfs.Configuration;\r\nimport com.google.common.jimfs.Jimfs;\r\nimport io.milton.annotations.ChildrenOf;\r\nimport io.milton.annotations.ResourceController;\r\nimport io.milton.annotations.Root;\r\n\r\nimport java.io.IOException;\r\nimport java.nio.charset.StandardCharsets;\r\nimport java.nio.file.FileSystem;\r\nimport java.nio.file.Files;\r\nimport java.nio.file.Path;\r\nimport java.util.List;\r\nimport java.util.stream.Collectors;\r\n\r\n@ResourceController\r\npublic class MinimalVirtualFSDavController {\r\n\tprivate final FileSystem fs = Jimfs.newFileSystem(Configuration.unix());\r\n\r\n\tpublic MinimalVirtualFSDavController() {\r\n\t\ttry {\r\n\t\t\tPath dir = Files.createDirectory(fs.getPath(\"/foo\"));\r\n\t\t\tFiles.writeString(fs.getPath(dir.toString(), \"bar1\"), \"Hello\", StandardCharsets.UTF_8);\r\n\t\t\tFiles.writeString(fs.getPath(dir.toString(), \"bar2\"), \"Hello\", StandardCharsets.UTF_8);\r\n\t\t\tFiles.writeString(fs.getPath(dir.toString(), \"bar3\"), \"Hello\", StandardCharsets.UTF_8);\r\n\t\t\tFiles.writeString(fs.getPath(dir.toString(), \"bar4\"), \"Hello\", StandardCharsets.UTF_8);\r\n\t\t\tFiles.writeString(fs.getPath(dir.toString(), \"bar5\"), \"Hello\", StandardCharsets.UTF_8);\r\n\t\t} catch (IOException e) {\r\n\t\t\te.printStackTrace();\r\n\t\t}\r\n\t}\r\n\r\n\t@Root\r\n\tpublic Path root() {\r\n\t\treturn fs.getPath(\"/\");\r\n\t}\r\n\r\n\t@ChildrenOf\r\n\tpublic List<Path> getChildrenOfDavFolder(Path p) {\r\n\t\ttry {\r\n\t\t\tList<Path> collect = Files.list(p).collect(Collectors.toList());\r\n\t\t\treturn collect;\r\n\t\t} catch (IOException e) {\r\n\t\t\te.printStackTrace();\r\n\t\t}\r\n\t\treturn List.of();\r\n\t}\r\n}\r\n```\r\n\r\nHow could I implement this in a way that works with milton? Does milton even already support the `java.nio.file.FileSystem` API?",
          "issue_comments": []
        },
        {
          "issue_url": "https://github.com/miltonio/milton2/issues/138",
          "issue_title": "No git tags, change logs or release notes",
          "issue_number": 138,
          "issue_text": "\r\nIf I look at the maven repo then I can see new versions are available\r\n\r\nhttps://mvnrepository.com/artifact/io.milton/milton-server-ce?repo=bt-milton\r\n\r\nHowever,  Almost all of them have no corresponding tags and no release notes/change log.\r\nDo I miss the pointer to that information? Even official web page provide no information and\r\noffers  2.7.2.4 as a `Current release`\r\n\r\n",
          "issue_comments": []
        },
        {
          "issue_url": "https://github.com/miltonio/milton2/issues/133",
          "issue_title": "Origin/master",
          "issue_number": 133,
          "issue_text": "",
          "issue_comments": []
        },
        {
          "issue_url": "https://github.com/miltonio/milton2/issues/123",
          "issue_title": "Change Propfind directory directive programming  ",
          "issue_number": 123,
          "issue_text": "in Propfind method, when we need to pars XML tree propfind method act recursive  and in some reason for performance issue  I need to Directory pars all the tree, not recursive,\r\n",
          "issue_comments": [
            {
              "comment_username": "bradmac",
              "comment_create_time": "2019-11-03T19:52:49Z",
              "comment_edit_time": "2019-11-03T19:52:49Z",
              "comment_text": "Milton's API is very resource oriented, it might be tricky to adjust it to allow for non-recursive deep directory listing. Also, I'm not aware of any situation where actual Dav clients do a deep directory listing .. that would be unworkably slow in most situations.\r\n\r\nIf you need to optimise for this what i suggest is\r\n- create a custom filter which will pre-fetch data.\r\n- this will need to inspect the request to determine the method and URL, and fetch the data into a request attribute (or other cache holder)\r\n- your CollectionResource.child method can then be optimised to use the pre-fetched data if available\r\n\r\n"
            }
          ]
        },
        {
          "issue_url": "https://github.com/miltonio/milton2/issues/120",
          "issue_title": "ERROR: Failed to resolve: io.milton:aspirin:0.10.03.03",
          "issue_number": 120,
          "issue_text": "I received an error when add milton-server-ce in android gradle: \r\n\r\nimplementation group: 'io.milton', name: 'milton-server-ce', version: '2.7.1.7'\r\n\r\nERROR: Failed to resolve: io.milton:aspirin:0.10.03.03\r\n\r\nHow to resolve it? ",
          "issue_comments": [
            {
              "comment_username": "bradmac",
              "comment_create_time": "2019-09-16T20:40:52Z",
              "comment_edit_time": "2019-09-16T20:40:52Z",
              "comment_text": "Hey, that aspirin dependency should be fine, not sure why its not resolving for you\r\n\r\nYou can get it manually from the bintray repo:\r\nhttps://bintray.com/milton/Milton/milton#files/io%2Fmilton%2Faspirin%2F0.10.03.03"
            }
          ]
        },
        {
          "issue_url": "https://github.com/miltonio/milton2/issues/117",
          "issue_title": "CALDAV verification fails with iOS.",
          "issue_number": 117,
          "issue_text": "CALDAV stopped working on iPhone. Works well on other clients.",
          "issue_comments": [
            {
              "comment_username": "bradmac",
              "comment_create_time": "2019-03-20T20:27:55Z",
              "comment_edit_time": "2019-03-20T20:27:55Z",
              "comment_text": "please provide details.\r\n"
            },
            {
              "comment_username": "mjoaquim12",
              "comment_create_time": "2019-03-24T09:28:27Z",
              "comment_edit_time": "2019-03-24T09:28:27Z",
              "comment_text": "The two step authentication process fails for iOS CalDAV account. The first request is processed but no subsequent request with auth details is received.\r\n\r\niOS 12.1.4 error logs with production URL:\r\n```\r\nerror\t11:39:36.914815 +0400\taccountsd\t\"<private> is trying to verify account credentials for account: <private>\"\r\nerror\t11:39:36.916537 +0400\taccountsd\tCredStore - performQuery - Error copying matching creds.  Error=-25300, query=<private>\r\nerror\t11:39:37.464118 +0400\taccountsd\tCredStore - performQuery - Error copying matching creds.  Error=-25300, query=<private>\r\nerror\t11:39:38.089785 +0400\tPreferences\tvalidation failed with error Error Domain=CoreDAVErrorDomain Code=8\r\n```\r\niOS 12.1.4 error logs with development URL:\r\n```\r\nerror\t11:33:51.451997 +0400\taccountsd\t\"<private> is trying to verify account credentials for account: <private>\"\r\nerror\t11:33:51.453017 +0400\taccountsd\tCredStore - performQuery - Error copying matching creds.  Error=-25300, query=<private>\r\nerror\t11:33:51.544240 +0400\taccountsd\tCredStore - performQuery - Error copying matching creds.  Error=-25300, query=<private>\r\nerror\t11:33:51.544845 +0400\taccountsd\tCancelling authentication challenge for insecure connection using basic authentication for URL <private>\r\nerror\t11:33:51.547042 +0400\tPreferences\tvalidation failed with error Error Domain=DAAccountValidationDomain Code=102 UserInfo={NSUnderlyingError=0x283c0eeb0 {Error Domain=NSURLErrorDomain Code=-1012 UserInfo={NSErrorFailingURLStringKey=<private>, NSErrorFailingURLKey=<private>}}}\r\n```\r\nStackoverflow says:\r\nhttps://stackoverflow.com/questions/46099940/credstore-perform-query-error/46806008#46806008\r\n"
            },
            {
              "comment_username": "HasanGee",
              "comment_create_time": "2019-04-15T12:50:13Z",
              "comment_edit_time": "2019-04-15T12:50:13Z",
              "comment_text": "Hello, has this issue been resolved? We are experiencing the same issue."
            },
            {
              "comment_username": "bradmac",
              "comment_create_time": "2019-04-15T21:12:06Z",
              "comment_edit_time": "2019-04-15T21:12:06Z",
              "comment_text": "Hey guys, the ios logs above really dont tell me anything.\r\n\r\nI really need to see some logs from the milton side.\r\n\r\nAnd the first thing you should do to debug is to check the network logs. Ie what status codes is milton returning for each request? Any error codes at all indicate an incorrect implementation and will cause the operation as a whole to fail.\r\n\r\nEg check for 500's, 401s, 404's, etc\r\n\r\nIf you're seeing an error response code then try to find out why. A very common problem is that developers just ignore required semantics they dont understand or dont see a need for, but OS clients tend to be sensitive to edge cases.\r\n\r\nI dont think the SO link above is relevant."
            }
          ]
        },
        {
          "issue_url": "https://github.com/miltonio/milton2/issues/114",
          "issue_title": "Race condition in creating directories on PUT",
          "issue_number": 114,
          "issue_text": "`PutHandler` contains the following code:\\r\\n\\r\\n```\\r\\n                Resource r = parent.child(path.getName());\\r\\n\\r\\n                if (r == null) {\\r\\n                        log.info(\"Could not find child: \" + path.getName() + \" in parent: \" + parent.getName() + \" - \" + parent.getClass());\\r\\n                        if (parent instanceof MakeCollectionableResource) {\\r\\n                                MakeCollectionableResource mkcol = (MakeCollectionableResource) parent;\\r\\n                                if (!handlerHelper.checkAuthorisation(manager, mkcol, request)) {\\r\\n                                        throw new NotAuthorizedException(mkcol);\\r\\n                                }\\r\\n                                log.info(\"autocreating new folder: \" + path.getName());\\r\\n                                CollectionResource newCol = mkcol.createCollection(path.getName());\\r\\n                                manager.getEventManager().fireEvent(new NewFolderEvent(newCol));\\r\\n                                return newCol;\\r\\n                        } else {\\r\\n                                log.info(\"parent folder isnt a MakeCollectionableResource: \" + parent.getName() + \" - \" + parent.getClass());\\r\\n                                return null;\\r\\n                        }\\r\\n                } else if (r instanceof CollectionResource) {\\r\\n```\\r\\n\\r\\nThe problem is that, if there are many concurrent requests targeting the same non-existing path then it is possible for a collection that does not exist for `parent.child(path.getName())` to be created by the time `mkcol.createCollection(path.getName())` is called.\\r\\n\\r\\nRFC 4918 is a little vague on how MKCOL should react if the entity already exists: it says the request must fail, but does not indicate with which status code.  I believe 400 Bad Request (i.e., `BadRequestException`) is the expected response, despite not being listed in RFC 4918 § 9.3.1.  The `MakeCollectionableResource`  interface supports `BadRequestException` but the JavaDoc doesn't describe the expectation (i.e., when this exception is to be thrown).\\r\\n\\r\\nAssuming `BadRequestException` is the expected reaction, one way of fixing this would be to catch this exception and retry the `parent.child(path.getName())` request.",
          "issue_comments": [
            {
              "comment_username": "paulmillar",
              "comment_create_time": "2019-01-04T10:26:54Z",
              "comment_edit_time": "2019-01-04T10:26:54Z",
              "comment_text": "Note that the same problem exists in `MkColHandler`:\r\n```\r\n\t\tResource existingChild = existingCol.child(newName);\r\n\t\tif (existingChild != null) {\r\n\t\t        // ...\r\n\t\t\tresponseHandler.respondMethodNotAllowed(existingChild, response, request);\r\n\t\t\treturn;\r\n\t\t}\r\n\t\tCollectionResource made = creator.createResource(existingCol, newName, request);\r\n```\r\nMultiple concurrent MKCOL requests targeting the same missing collection could all see a `null` value for `existingChild`, so multiple threads will attempt to call `CollectionResourceCreator#createResource`, which (by default), calls `MakeCollectionableResource#createCollection`.\r\n\r\nInterestingly, the code here returns status code 405 Method Not Allowed (which I see now is actually the correct code).\r\n\r\nTherefore, I believe the correct solution here is to add a new throwable exception in the `MakeCollectionableResource#createCollection` method's signature to indicate the resource already exists, and update `MkColHandler` and `PutHandler` to react correctly to that exception."
            },
            {
              "comment_username": "bradmac",
              "comment_create_time": "2019-01-06T20:50:03Z",
              "comment_edit_time": "2019-01-06T20:50:03Z",
              "comment_text": "> Assuming BadRequestException is the expected reaction, one way of fixing this would be to catch this exception and retry the parent.child(path.getName()) request.\r\n\r\nI think that would be exceeding the scope of milton's functionality. Milton isnt responsible for transaction handling/isolation etc. \r\n\r\nConcurrency is intended to be handled in webdav through locking. Most operating system clients use an elaborate sequence of actions to ensure concurrency, eg create a zero byte file with a temp name, lock it, upload to it, then move it to the desired name.\r\n\r\nIn addition there is a feature defined in the webdav spec referred to as the 'lock null' process where a resource can be locked *before it exists*. Milton fully supports lock-null, but i understand Mac Finder is the only OS client which supports it."
            },
            {
              "comment_username": "bradmac",
              "comment_create_time": "2019-01-06T20:53:33Z",
              "comment_edit_time": "2019-01-06T20:53:33Z",
              "comment_text": "I think if you have an implementation with concurrency problems the best way to handle it is\r\n- ensure you support lock-null (if Finder is a supported client)\r\n- hold a list of pending resource creation operations, and use that to validate resource names when a create request is received. This in effect intentionally breaks transaction isolation for this case."
            },
            {
              "comment_username": "paulmillar",
              "comment_create_time": "2019-01-07T00:03:02Z",
              "comment_edit_time": "2019-01-07T00:03:02Z",
              "comment_text": "Thanks for the reply, but I believe we are talking somewhat at cross-purposes.\r\n\r\nI appreciate that WebDAV support locking; however here I am interested in how Milton supports multiple clients, each issuing a single PUT request.\r\n\r\nTo give you a concrete example: I discovered this problem when attempting to use JMeter to benchmark performance characteristics.  My simple test-case has JMeter with a threadgroup (=> simulating multiple clients), where each is uploading a unique file: thread-1 uploads to `/public/jmeter/thread-1.dat`, thread-2 uploads to `/public/jmeter/thread-2.dat`, etc.\r\n\r\nEach client issues a PUT request at (more or less) the same time, as the test-case starts.  This means that the jetty server will process these PUT requests will concurrently, using different threads.\r\n\r\nThe directory `/public/jmeter` does not exist before the test starts.  Multiple jetty/milton threads notice `parent.child(path.getName())` returning `null` (for a `/public` parent and `jmeter` as the name).  Therefore, these multiple threads all call `createCollection`.  Naturally, only the first thread will succeed, and all other threads will fail.\r\n\r\nThanks for the hint about lock-null -- I'll check that dCache supports this. \r\n\r\nHowever, we have clients that do not lock all resources in the path (`/`, `/public` `/public/jmeter`, `/public/jmeter/thread-1.dat`) before issuing a PUT request, so I believe the lock-null support would likely have limited effect.\r\n\r\nI also don't believe your second solution really solves the problem for a few reasons:\r\n  1. holding the list of pending creations doesn't fix the race condition, it just reduces the window (makes it less likely)\r\n  2. a typical production deployment has multiple servers.  This list of pending creations would need to be synchronised across all such servers.  Achieving agreement reliably across multiple servers is expensive in terms of performance (see algorithms like PAXOS, RAFT; and software like ZooKeeper).\r\n\r\nMy proposed solution is relatively simple and backwards compatible: update the MakeCollectionableResource interface to allow the method to throw an exception if the resource already exists."
            },
            {
              "comment_username": "bradmac",
              "comment_create_time": "2019-01-07T00:29:24Z",
              "comment_edit_time": "2019-01-07T00:29:24Z",
              "comment_text": "I think i do understand what you mean. But the example above is invalid. For your JMeter script  to be correct it should use locking as dav clients are required to do.\r\n\r\nMilton's semantics are correct, in that if a resource exists we respond with a 405 as above. Milton tries to take on the burden of complying with protocol requirements rather then imposing that on implmentation developers. So we want to retain the current functionality where we check to see if a resource exists before calling the createCollection method.\r\n\r\nI dont believe that persisting state across clusters is impractical. There are many good tools for doing this. In fact you need to do this with locks (including lock-null), and you should do it for nonces, for milton to work correctly in a clustered environment. \r\n\r\nSo to summarise\r\n- i think its reasonable for a server to assume clients will correctly implement locking\r\n- and you can optimise your solution by maintaining a list of in-progress, but uncommited, operations. "
            },
            {
              "comment_username": "bradmac",
              "comment_create_time": "2019-01-07T00:32:06Z",
              "comment_edit_time": "2019-01-07T00:32:06Z",
              "comment_text": "Also, you can configure the milton stack to use your own MkColHandler, which could remove the check to see whether the resource exists. Milton is designed to be pluggable for this reason."
            }
          ]
        },
        {
          "issue_url": "https://github.com/miltonio/milton2/issues/112",
          "issue_title": "HTTP Basic auth encoding (again)",
          "issue_number": 112,
          "issue_text": "This issue is based on #19, which targets the character encoding used during http authentication.\\r\\n\\r\\nI expect the following change to be useless, because `enc` is the codepage independent base64 string. No need to require any special character set.\\r\\nhttps://github.com/miltonio/milton2/blob/2feff46a1589fad01046e745fd09c546f56926a1/milton-api/src/main/java/io/milton/http/Auth.java#L225\\r\\n\\r\\nThe actual question is the character encoding of the base64 decoded byte sequence stored in `bytes`. When storing this byte sequence as String (UTF-16), the platform's default charset matters:\\r\\nhttps://github.com/miltonio/milton2/blob/2feff46a1589fad01046e745fd09c546f56926a1/milton-api/src/main/java/io/milton/http/Auth.java#L226\\r\\n\\r\\nI suggest implementing some simple encoding detection at this point and constructing the string platform independent similar to:\\r\\n\\r\\n    if(isUTF8(bytes)) {\\r\\n        s = new String(bytes, Charset.forName(\"UTF-8\"));\\r\\n    }\\r\\n    else {\\r\\n        final String latin = new String(bytes, Charset.forName(\"ISO-8859-1\"));\\r\\n        final byte[] lbytes = latin.getBytes(Charset.forName(\"UTF-8\"));\\r\\n        s = new String(lbytes);\\r\\n    }\\r\\n\\r\\nActually, the list of expected charsets should be configurable.",
          "issue_comments": [
            {
              "comment_username": "bradmac",
              "comment_create_time": "2018-08-08T20:49:19Z",
              "comment_edit_time": "2018-08-08T20:49:19Z",
              "comment_text": ":+1: "
            }
          ]
        },
        {
          "issue_url": "https://github.com/miltonio/milton2/issues/110",
          "issue_title": "Add Java 9 Module System Support",
          "issue_number": 110,
          "issue_text": "Currently using milton in a project, that uses the Java 9 module system with module-path instead of class-path, is not possible due to the split package problem:\r\n- `milton-server-ce` depends on `commons-beanutils:1.8.2` which is [not compatible with the module system](https://stackoverflow.com/questions/14402745/duplicate-classes-in-commons-collections-and-commons-beanutils/14403149) due to split-package problem\r\n- `milton-server-ce` depends on `milton-api`. Both contain packages with the same name, e.g. `io.milton.http`.\r\n\r\nPlease add support for the Java 9 module system by adding module information and define clear exports for packages.",
          "issue_comments": []
        },
        {
          "issue_url": "https://github.com/miltonio/milton2/issues/104",
          "issue_title": "Null pointer exception",
          "issue_number": 104,
          "issue_text": "https://github.com/miltonio/milton2/blob/9bacade53510c38374da3cbeb9a2534c534675e6/milton-server-ent/src/main/java/io/milton/http/caldav/DefaultCalendarSearchService.java#L133\r\n\r\n\r\nWhen the event is a new EventResourceImpl() the event.getStart().before(start) or event.getEnd.after(end) will throw a null pointer exception.",
          "issue_comments": []
        },
        {
          "issue_url": "https://github.com/miltonio/milton2/issues/102",
          "issue_title": "Milton does not use UrlAdapter in every Handler - problem eliminating context path",
          "issue_number": 102,
          "issue_text": "We use Milton in a separate webapp in our project. Since we have several webapps deployed in one application server, every webapp has to use its own context path. To eliminate the context path from Milton requests i implemented your UrlAdapter interface. This works fine for Get or Delete request but not for MkCol or Move for example.\r\n\r\nI patched milton-server-ce myself to get this to work but i would prefer to see this resolved in an official release. Here is what i did:\r\n\r\nChanged MkColHandler to get ResourceHandlerHelper injected and changed MkColHandler.process() to:\r\n`String finalurl = HttpManager.decodeUrl(resourceHandlerHelper.getUrlAdapter().getUrl(request));`\r\n\r\nExtended UrlAdapter by:\r\n`String processUrl(String url);` \r\nand delegated to this method in UrlAdapterImpl.getUrl() by spliting the original method.\r\n\r\nChanged MoveHandler.processExistingResource() to:\r\n`String destUrl = resourceHandlerHelper.getUrlAdapter().processUrl(dest.url);`\r\n`Resource rDest = manager.getResourceFactory().getResource(dest.host, destUrl);`",
          "issue_comments": [
            {
              "comment_username": "bradmac",
              "comment_create_time": "2017-12-10T19:49:58Z",
              "comment_edit_time": "2017-12-10T19:49:58Z",
              "comment_text": ":+1: \r\n\r\nThanks, I'll update accordingly."
            },
            {
              "comment_username": "jfrommann",
              "comment_create_time": "2017-12-18T14:21:07Z",
              "comment_edit_time": "2017-12-18T14:21:07Z",
              "comment_text": "Hi Brad, it turned out that i forgot the PutHandler. The changes here are similar to the MkColHandler."
            },
            {
              "comment_username": "bradmac",
              "comment_create_time": "2017-12-18T19:51:52Z",
              "comment_edit_time": "2017-12-18T19:51:52Z",
              "comment_text": "Great, thank you. I should get to that in the next couple of days."
            }
          ]
        },
        {
          "issue_url": "https://github.com/miltonio/milton2/issues/89",
          "issue_title": "Could not open collection",
          "issue_number": 89,
          "issue_text": "I'm trying to include milton community version in my existing app j2ee build with jersey. The Ressource Controllers seem's to bee correctly loaded with the annotations @ResourceController. But when I use cadaver with the url: \r\ncadaver http://localhost:8080/torii/webdav/\r\n\r\nI get the error:\r\n```\r\nCould not open collection:\r\n404 Not Found\r\n```\r\n\r\nWhen I activate the debugger, I saw the error happened on the ResourceHandlerHelper.java in the process function (line 68) where the call \r\n`Resource r=manager.getResourceFactory().getResource(host, url);` (line 89) return me null and so you'll return a `http 404`.\r\n",
          "issue_comments": [
            {
              "comment_username": "bradmac",
              "comment_create_time": "2017-11-30T09:01:29Z",
              "comment_edit_time": "2017-11-30T09:01:29Z",
              "comment_text": "Hey, looks like you havent integrated the demo app correctly, i would say the rootpath hasnt been configured into the milton stack.\r\n\r\nBest thing is to run the projects from maven until you're familiar with the structure."
            }
          ]
        },
        {
          "issue_url": "https://github.com/miltonio/milton2/issues/81",
          "issue_title": "Zero bytes file size on Windows",
          "issue_number": 81,
          "issue_text": "I use milton in my Android app via `SimpletonServer`. If I use Windows Explorer as client and copy large files to the server I see the progress bar progressing very fast to 99% and then waiting until the copy process is finished. It seems, that the server don't get the real file size (see screenshot).\n\n![bildschirmfoto 2016-10-04 um 15 58 37](https://cloud.githubusercontent.com/assets/1738628/19077133/a4f4d7da-8a4b-11e6-80f9-4c401ec02a4b.png)\n\nHere is my code:\n\n```\n        HttpManagerBuilder b = new HttpManagerBuilder();\n        b.setEnableFormAuth(false);\n        b.setEnableDigestAuth(false);\n        b.setEnableOptionsAuth(false);\n\n        FileSystemResourceFactory resourceFactory = new FileSystemResourceFactory(new File(preferences.getString(BASE_FOLDER)),\n                manager, getString(R.string.app_name));\n        resourceFactory.setAllowDirectoryBrowsing(true);\n        b.setResourceFactory(resourceFactory);\n        HttpManager httpManager = b.buildHttpManager();\n\n        if (server != null) {\n            server.stop();\n        }\n        server = new SimpletonServer(httpManager, b.getOuterWebdavResponseHandler(), 100, 10);\n\n        server.setHttpPort(Integer.parseInt(preferences.getString(PORT_WEBDAV)));\n\n        server.start();\n```\n\nHere are the logs:\n\n```\nW/System.err: 43868 [Stage-dispatchStage-2] INFO io.milton.http.HttpManager - HEAD :: 192.168.1.15:8080///Pocketshare/camtasia.dmg start\nW/System.err: 43879 [Stage-dispatchStage-2] INFO io.milton.http.HttpManager - HEAD :: 192.168.1.15:8080///Pocketshare/camtasia.dmg finished 10ms, Status:HTTP/1.1 200 OK, Length:null\nW/System.err: 65167 [Stage-dispatchStage-1] INFO io.milton.http.HttpManager - PUT :: 192.168.1.15:8080///Pocketshare/camtasia.dmg start\nW/System.err: 65174 [Stage-dispatchStage-1] WARN io.milton.http.fs.FsResource - getCurrentLock called, but no lock manager: file: /storage/emulated/0/Download/camtasia.dmg\nW/System.err: 107400 [Stage-dispatchStage-1] INFO io.milton.http.HttpManager - PUT :: 192.168.1.15:8080///Pocketshare/camtasia.dmg finished 42232ms, Status:HTTP/1.1 204, Length:null\nW/System.err: 107485 [Stage-dispatchStage-3] INFO io.milton.http.HttpManager - PROPPATCH :: 192.168.1.15:8080///Pocketshare/camtasia.dmg start\nW/System.err: 107506 [Stage-dispatchStage-3] WARN io.milton.http.fs.FsResource - getCurrentLock called, but no lock manager: file: /storage/emulated/0/Download/camtasia.dmg\nW/System.err: 107520 [Stage-dispatchStage-3] WARN io.milton.http.webdav.PropertySourcePatchSetter - property not found: {urn:schemas-microsoft-com:}Win32CreationTime on resource: class io.milton.http.fs.FsFileResource\nW/System.err: 107521 [Stage-dispatchStage-3] WARN io.milton.http.webdav.PropertySourcePatchSetter - property not found: {urn:schemas-microsoft-com:}Win32LastAccessTime on resource: class io.milton.http.fs.FsFileResource\nW/System.err: 107521 [Stage-dispatchStage-3] WARN io.milton.http.webdav.PropertySourcePatchSetter - property not found: {urn:schemas-microsoft-com:}Win32LastModifiedTime on resource: class io.milton.http.fs.FsFileResource\nW/System.err: 107521 [Stage-dispatchStage-3] WARN io.milton.http.webdav.PropertySourcePatchSetter - property not found: {urn:schemas-microsoft-com:}Win32FileAttributes on resource: class io.milton.http.fs.FsFileResource\nW/System.err: 107532 [Stage-dispatchStage-3] INFO io.milton.http.HttpManager - PROPPATCH :: 192.168.1.15:8080///Pocketshare/camtasia.dmg finished 47ms, Status:HTTP/1.1 207 Multi-status, Length:null\n\n```\n",
          "issue_comments": [
            {
              "comment_username": "bradmac",
              "comment_create_time": "2017-11-30T09:05:47Z",
              "comment_edit_time": "2017-11-30T09:05:47Z",
              "comment_text": "Hey, sorry i missed this issue. Do you still need assistance?\r\n\r\nBut note that the FileSystemResourceFactory  isnt really intended for production use, its just an aid to getting started"
            },
            {
              "comment_username": "artkoenig",
              "comment_create_time": "2017-11-30T09:23:49Z",
              "comment_edit_time": "2017-11-30T09:23:49Z",
              "comment_text": "Not anymore, thank you."
            }
          ]
        },
        {
          "issue_url": "https://github.com/miltonio/milton2/issues/64",
          "issue_title": "Milton Client not parsing WebDav response correctly - missing logic for set responses ",
          "issue_number": 64,
          "issue_text": "For example when issuing a PROPFIND for property supported-report-set\n\n``` xml\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<D:propfind xmlns:D=\"DAV:\"><D:prop><D:supported-report-set /></D:prop></D:propfind>\n```\n\nand receiving back:\n\n``` xml\n<d:multistatus xmlns:cal=\"urn:ietf:params:xml:ns:caldav\" xmlns:cs=\"http://calendarserver.org/ns/\" xmlns:card=\"urn:ietf:params:xml:ns:carddav\" xmlns:d=\"DAV:\"><d:response><d:href>/spica-contacts/carddav/principals/StsConnectorMockUser/</d:href><d:propstat><d:prop><d:supported-report-set><d:supported-report><d:report><d:principal-search-property-set/></d:report></d:supported-report><d:supported-report><d:report><card:addressbook-query/></d:report></d:supported-report><d:supported-report><d:report><d:principal-property-search/></d:report></d:supported-report><d:supported-report><d:report><card:addressbook-multiget/></d:report></d:supported-report><d:supported-report><d:report><d:expand-property/></d:report></d:supported-report></d:supported-report-set></d:prop><d:status>HTTP/1.1 200 OK</d:status></d:propstat></d:response><d:response><d:href>/spica-contacts/carddav/principals/StsConnectorMockUser/addressBooks/</d:href><d:propstat><d:prop><d:supported-report-set><d:supported-report><d:report><d:principal-search-property-set/></d:report></d:supported-report><d:supported-report><d:report><card:addressbook-query/></d:report></d:supported-report><d:supported-report><d:report><d:principal-property-search/></d:report></d:supported-report><d:supported-report><d:report><card:addressbook-multiget/></d:report></d:supported-report><d:supported-report><d:report><d:expand-property/></d:report></d:supported-report></d:supported-report-set></d:prop><d:status>HTTP/1.1 200 OK</d:status></d:propstat></d:response></d:multistatus>\n```\n\nMilton client only deals with parent element : <d:supported-report-set>. No child of the parent is whatsoever discover. Corresponding response object will have a List<PropFindResponse> with one PropResponse containing exact one property with value \"\".\n\nSee PropFindResponse > getFoundProps() and PropFindResponse() expects collections only for d:prop and not for direct children of d:prop.\n",
          "issue_comments": []
        },
        {
          "issue_url": "https://github.com/miltonio/milton2/issues/55",
          "issue_title": "ConcurrentLinkedHashMap --> Caffeine",
          "issue_number": 55,
          "issue_text": "`milton-server-ce` currently uses [ConcurrentLinkedHashMap](https://code.google.com/p/concurrentlinkedhashmap) v1.3.2 for caching. The latest release, v1.4.2, is significantly faster and remains Java 6 compatible.\n\n[Caffeine](https://github.com/ben-manes/caffeine) is the successor project and designed for Java 8. It [doubles the performance](https://github.com/ben-manes/caffeine/wiki/Benchmarks) and offers the all of the features that we built for Guava's cache.\n\nWhen transitioning to Java 8, please consider migrating to Caffeine.\n",
          "issue_comments": []
        }
      ]
    }
  },
  {
    "github_url": "https://github.com/theguly/DecryptOpManager",
    "github_info": {
      "name": "theguly/DecryptOpManager",
      "language": "Java"
    },
    "github_branches": {
      "branch_datas": [
        {
          "branch_version": "master",
          "branch_url": "https://github.com/theguly/DecryptOpManager/tree/master",
          "branch_download_url": "https://github.com/theguly/DecryptOpManager/archive/master.zip"
        }
      ]
    },
    "github_pull_requests": { "pull_datas": [] },
    "github_issues": { "issue_datas": [] }
  }
]
